{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A notebook for test and debug some python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "* Create a conda env [X]\n",
    "*     Im having a error [X] (resolvi usando pip no ambiente)\n",
    "*   ERRO TRANSFORM TORCHVISION\n",
    "* Selecionar e colocar arquiteturas [X]\n",
    "* unet[X]\n",
    "* ddpm\n",
    "* cyclogan \n",
    "* vae\n",
    "* vit [X]\n",
    "* Colocar funcoes de perda da tabela\n",
    "* Colocar todas as metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the utils libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "#from torchvision import transforms \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: kornia in /home/pdi/anaconda3/envs/CLEDiff/lib/python3.11/site-packages (0.7.1)\n",
      "Requirement already satisfied: lpips in /home/pdi/anaconda3/envs/CLEDiff/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: packaging in /home/pdi/anaconda3/envs/CLEDiff/lib/python3.11/site-packages (from kornia) (23.1)\n",
      "Requirement already satisfied: torch>=1.9.1 in /home/pdi/anaconda3/envs/CLEDiff/lib/python3.11/site-packages (from kornia) (2.2.1)\n",
      "Requirement already satisfied: filelock in /home/pdi/anaconda3/envs/CLEDiff/lib/python3.11/site-packages (from torch>=1.9.1->kornia) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/pdi/anaconda3/envs/CLEDiff/lib/python3.11/site-packages (from torch>=1.9.1->kornia) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/pdi/anaconda3/envs/CLEDiff/lib/python3.11/site-packages (from torch>=1.9.1->kornia) (1.12)\n",
      "Requirement already satisfied: networkx in /home/pdi/anaconda3/envs/CLEDiff/lib/python3.11/site-packages (from torch>=1.9.1->kornia) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/pdi/anaconda3/envs/CLEDiff/lib/python3.11/site-packages (from torch>=1.9.1->kornia) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/pdi/anaconda3/envs/CLEDiff/lib/python3.11/site-packages (from torch>=1.9.1->kornia) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/pdi/anaconda3/envs/CLEDiff/lib/python3.11/site-packages (from jinja2->torch>=1.9.1->kornia) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/pdi/anaconda3/envs/CLEDiff/lib/python3.11/site-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo kornia lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7.1\n"
     ]
    }
   ],
   "source": [
    "import kornia\n",
    "print(kornia.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchinfo as tinf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtinf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'nn.Module'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'INPUT_SIZE_TYPE | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'INPUT_DATA_TYPE | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcol_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Iterable[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcol_width\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdepth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'torch.device | str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtypes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'list[torch.dtype] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrow_settings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Iterable[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'ModelStatistics'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Summarize the given PyTorch model. Summarized information includes:\n",
      "    1) Layer names,\n",
      "    2) input/output shapes,\n",
      "    3) kernel shape,\n",
      "    4) # of parameters,\n",
      "    5) # of operations (Mult-Adds),\n",
      "    6) whether layer is trainable\n",
      "\n",
      "NOTE: If neither input_data or input_size are provided, no forward pass through the\n",
      "network is performed, and the provided model information is limited to layer names.\n",
      "\n",
      "Args:\n",
      "    model (nn.Module):\n",
      "            PyTorch model to summarize. The model should be fully in either train()\n",
      "            or eval() mode. If layers are not all in the same mode, running summary\n",
      "            may have side effects on batchnorm or dropout statistics. If you\n",
      "            encounter an issue with this, please open a GitHub issue.\n",
      "\n",
      "    input_size (Sequence of Sizes):\n",
      "            Shape of input data as a List/Tuple/torch.Size\n",
      "            (dtypes must match model input, default is FloatTensors).\n",
      "            You should include batch size in the tuple.\n",
      "            Default: None\n",
      "\n",
      "    input_data (Sequence of Tensors):\n",
      "            Arguments for the model's forward pass (dtypes inferred).\n",
      "            If the forward() function takes several parameters, pass in a list of\n",
      "            args or a dict of kwargs (if your forward() function takes in a dict\n",
      "            as its only argument, wrap it in a list).\n",
      "            Default: None\n",
      "\n",
      "    batch_dim (int):\n",
      "            Batch_dimension of input data. If batch_dim is None, assume\n",
      "            input_data / input_size contains the batch dimension, which is used\n",
      "            in all calculations. Else, expand all tensors to contain the batch_dim.\n",
      "            Specifying batch_dim can be an runtime optimization, since if batch_dim\n",
      "            is specified, torchinfo uses a batch size of 1 for the forward pass.\n",
      "            Default: None\n",
      "\n",
      "    cache_forward_pass (bool):\n",
      "            If True, cache the run of the forward() function using the model\n",
      "            class name as the key. If the forward pass is an expensive operation,\n",
      "            this can make it easier to modify the formatting of your model\n",
      "            summary, e.g. changing the depth or enabled column types, especially\n",
      "            in Jupyter Notebooks.\n",
      "            WARNING: Modifying the model architecture or input data/input size when\n",
      "            this feature is enabled does not invalidate the cache or re-run the\n",
      "            forward pass, and can cause incorrect summaries as a result.\n",
      "            Default: False\n",
      "\n",
      "    col_names (Iterable[str]):\n",
      "            Specify which columns to show in the output. Currently supported: (\n",
      "                \"input_size\",\n",
      "                \"output_size\",\n",
      "                \"num_params\",\n",
      "                \"params_percent\",\n",
      "                \"kernel_size\",\n",
      "                \"mult_adds\",\n",
      "                \"trainable\",\n",
      "            )\n",
      "            Default: (\"output_size\", \"num_params\")\n",
      "            If input_data / input_size are not provided, only \"num_params\" is used.\n",
      "\n",
      "    col_width (int):\n",
      "            Width of each column.\n",
      "            Default: 25\n",
      "\n",
      "    depth (int):\n",
      "            Depth of nested layers to display (e.g. Sequentials).\n",
      "            Nested layers below this depth will not be displayed in the summary.\n",
      "            Default: 3\n",
      "\n",
      "    device (torch.Device):\n",
      "            Uses this torch device for model and input_data.\n",
      "            If not specified, uses the dtype of input_data if given, or the\n",
      "            parameters of the model. Otherwise, uses the result of\n",
      "            torch.cuda.is_available().\n",
      "            Default: None\n",
      "\n",
      "    dtypes (List[torch.dtype]):\n",
      "            If you use input_size, torchinfo assumes your input uses FloatTensors.\n",
      "            If your model use a different data type, specify that dtype.\n",
      "            For multiple inputs, specify the size of both inputs, and\n",
      "            also specify the types of each parameter here.\n",
      "            Default: None\n",
      "\n",
      "    mode (str)\n",
      "            Either \"train\" or \"eval\", which determines whether we call\n",
      "            model.train() or model.eval() before calling summary().\n",
      "            Default: \"eval\".\n",
      "\n",
      "    row_settings (Iterable[str]):\n",
      "            Specify which features to show in a row. Currently supported: (\n",
      "                \"ascii_only\",\n",
      "                \"depth\",\n",
      "                \"var_names\",\n",
      "            )\n",
      "            Default: (\"depth\",)\n",
      "\n",
      "    verbose (int):\n",
      "            0 (quiet): No output\n",
      "            1 (default): Print model summary\n",
      "            2 (verbose): Show weight and bias layers in full detail\n",
      "            Default: 1\n",
      "            If using a Juypter Notebook or Google Colab, the default is 0.\n",
      "\n",
      "    **kwargs:\n",
      "            Other arguments used in `model.forward` function. Passing *args is no\n",
      "            longer supported.\n",
      "\n",
      "Return:\n",
      "    ModelStatistics object\n",
      "            See torchinfo/model_statistics.py for more information.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/CLEDiff/lib/python3.11/site-packages/torchinfo/torchinfo.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "# tinf.summary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtinf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'nn.Module'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'INPUT_SIZE_TYPE | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'INPUT_DATA_TYPE | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcol_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Iterable[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcol_width\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdepth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'torch.device | str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtypes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'list[torch.dtype] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrow_settings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Iterable[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'ModelStatistics'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Summarize the given PyTorch model. Summarized information includes:\n",
      "    1) Layer names,\n",
      "    2) input/output shapes,\n",
      "    3) kernel shape,\n",
      "    4) # of parameters,\n",
      "    5) # of operations (Mult-Adds),\n",
      "    6) whether layer is trainable\n",
      "\n",
      "NOTE: If neither input_data or input_size are provided, no forward pass through the\n",
      "network is performed, and the provided model information is limited to layer names.\n",
      "\n",
      "Args:\n",
      "    model (nn.Module):\n",
      "            PyTorch model to summarize. The model should be fully in either train()\n",
      "            or eval() mode. If layers are not all in the same mode, running summary\n",
      "            may have side effects on batchnorm or dropout statistics. If you\n",
      "            encounter an issue with this, please open a GitHub issue.\n",
      "\n",
      "    input_size (Sequence of Sizes):\n",
      "            Shape of input data as a List/Tuple/torch.Size\n",
      "            (dtypes must match model input, default is FloatTensors).\n",
      "            You should include batch size in the tuple.\n",
      "            Default: None\n",
      "\n",
      "    input_data (Sequence of Tensors):\n",
      "            Arguments for the model's forward pass (dtypes inferred).\n",
      "            If the forward() function takes several parameters, pass in a list of\n",
      "            args or a dict of kwargs (if your forward() function takes in a dict\n",
      "            as its only argument, wrap it in a list).\n",
      "            Default: None\n",
      "\n",
      "    batch_dim (int):\n",
      "            Batch_dimension of input data. If batch_dim is None, assume\n",
      "            input_data / input_size contains the batch dimension, which is used\n",
      "            in all calculations. Else, expand all tensors to contain the batch_dim.\n",
      "            Specifying batch_dim can be an runtime optimization, since if batch_dim\n",
      "            is specified, torchinfo uses a batch size of 1 for the forward pass.\n",
      "            Default: None\n",
      "\n",
      "    cache_forward_pass (bool):\n",
      "            If True, cache the run of the forward() function using the model\n",
      "            class name as the key. If the forward pass is an expensive operation,\n",
      "            this can make it easier to modify the formatting of your model\n",
      "            summary, e.g. changing the depth or enabled column types, especially\n",
      "            in Jupyter Notebooks.\n",
      "            WARNING: Modifying the model architecture or input data/input size when\n",
      "            this feature is enabled does not invalidate the cache or re-run the\n",
      "            forward pass, and can cause incorrect summaries as a result.\n",
      "            Default: False\n",
      "\n",
      "    col_names (Iterable[str]):\n",
      "            Specify which columns to show in the output. Currently supported: (\n",
      "                \"input_size\",\n",
      "                \"output_size\",\n",
      "                \"num_params\",\n",
      "                \"params_percent\",\n",
      "                \"kernel_size\",\n",
      "                \"mult_adds\",\n",
      "                \"trainable\",\n",
      "            )\n",
      "            Default: (\"output_size\", \"num_params\")\n",
      "            If input_data / input_size are not provided, only \"num_params\" is used.\n",
      "\n",
      "    col_width (int):\n",
      "            Width of each column.\n",
      "            Default: 25\n",
      "\n",
      "    depth (int):\n",
      "            Depth of nested layers to display (e.g. Sequentials).\n",
      "            Nested layers below this depth will not be displayed in the summary.\n",
      "            Default: 3\n",
      "\n",
      "    device (torch.Device):\n",
      "            Uses this torch device for model and input_data.\n",
      "            If not specified, uses the dtype of input_data if given, or the\n",
      "            parameters of the model. Otherwise, uses the result of\n",
      "            torch.cuda.is_available().\n",
      "            Default: None\n",
      "\n",
      "    dtypes (List[torch.dtype]):\n",
      "            If you use input_size, torchinfo assumes your input uses FloatTensors.\n",
      "            If your model use a different data type, specify that dtype.\n",
      "            For multiple inputs, specify the size of both inputs, and\n",
      "            also specify the types of each parameter here.\n",
      "            Default: None\n",
      "\n",
      "    mode (str)\n",
      "            Either \"train\" or \"eval\", which determines whether we call\n",
      "            model.train() or model.eval() before calling summary().\n",
      "            Default: \"eval\".\n",
      "\n",
      "    row_settings (Iterable[str]):\n",
      "            Specify which features to show in a row. Currently supported: (\n",
      "                \"ascii_only\",\n",
      "                \"depth\",\n",
      "                \"var_names\",\n",
      "            )\n",
      "            Default: (\"depth\",)\n",
      "\n",
      "    verbose (int):\n",
      "            0 (quiet): No output\n",
      "            1 (default): Print model summary\n",
      "            2 (verbose): Show weight and bias layers in full detail\n",
      "            Default: 1\n",
      "            If using a Juypter Notebook or Google Colab, the default is 0.\n",
      "\n",
      "    **kwargs:\n",
      "            Other arguments used in `model.forward` function. Passing *args is no\n",
      "            longer supported.\n",
      "\n",
      "Return:\n",
      "    ModelStatistics object\n",
      "            See torchinfo/model_statistics.py for more information.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/Losses/lib/python3.12/site-packages/torchinfo/torchinfo.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "# tinf.summary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archtectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloco de duas convoluções 3x3 seguidas de ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementação da arquitetura U-Net adaptada para geração de imagens.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Encoder\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.down5 = DoubleConv(512, 1024)\n",
    "\n",
    "        # Max-pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.conv_up1 = DoubleConv(1024, 512)\n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv_up2 = DoubleConv(512, 256)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv_up3 = DoubleConv(256, 128)\n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv_up4 = DoubleConv(128, 64)\n",
    "\n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(self.pool(d1))\n",
    "        d3 = self.down3(self.pool(d2))\n",
    "        d4 = self.down4(self.pool(d3))\n",
    "        d5 = self.down5(self.pool(d4))\n",
    "\n",
    "        # Decoder\n",
    "        u1 = self.up1(d5)\n",
    "        u1 = torch.cat((u1, d4), dim=1)\n",
    "        u1 = self.conv_up1(u1)\n",
    "        u2 = self.up2(u1)\n",
    "        u2 = torch.cat((u2, d3), dim=1)\n",
    "        u2 = self.conv_up2(u2)\n",
    "        u3 = self.up3(u2)\n",
    "        u3 = torch.cat((u3, d2), dim=1)\n",
    "        u3 = self.conv_up3(u3)\n",
    "        u4 = self.up4(u3)\n",
    "        u4 = torch.cat((u4, d1), dim=1)\n",
    "        u4 = self.conv_up4(u4)\n",
    "\n",
    "        return self.final_conv(u4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Definindo o dispositivo (GPU se disponível)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Inicializando a U-Net\n",
    "    in_channels = 3  # Por exemplo, RGB\n",
    "    out_channels = 3  # Saída de imagem RGB\n",
    "    model = UNet(in_channels, out_channels).to(device)\n",
    "\n",
    "    # Exemplo de entrada\n",
    "    x = torch.randn(1, in_channels, 256, 256).to(device)  # Batch size 1, 256x256 imagem\n",
    "    output = model(x)\n",
    "\n",
    "    print(x.shape, output.shape)  # Deve ser (1, out_channels, 256, 256)\n",
    "     # Plotando a arquitetura da rede\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "UNet                                     [1, 3, 256, 256]          --\n",
      "├─DoubleConv: 1-1                        [1, 64, 256, 256]         --\n",
      "│    └─Sequential: 2-1                   [1, 64, 256, 256]         --\n",
      "│    │    └─Conv2d: 3-1                  [1, 64, 256, 256]         1,792\n",
      "│    │    └─BatchNorm2d: 3-2             [1, 64, 256, 256]         128\n",
      "│    │    └─ReLU: 3-3                    [1, 64, 256, 256]         --\n",
      "│    │    └─Conv2d: 3-4                  [1, 64, 256, 256]         36,928\n",
      "│    │    └─BatchNorm2d: 3-5             [1, 64, 256, 256]         128\n",
      "│    │    └─ReLU: 3-6                    [1, 64, 256, 256]         --\n",
      "├─MaxPool2d: 1-2                         [1, 64, 128, 128]         --\n",
      "├─DoubleConv: 1-3                        [1, 128, 128, 128]        --\n",
      "│    └─Sequential: 2-2                   [1, 128, 128, 128]        --\n",
      "│    │    └─Conv2d: 3-7                  [1, 128, 128, 128]        73,856\n",
      "│    │    └─BatchNorm2d: 3-8             [1, 128, 128, 128]        256\n",
      "│    │    └─ReLU: 3-9                    [1, 128, 128, 128]        --\n",
      "│    │    └─Conv2d: 3-10                 [1, 128, 128, 128]        147,584\n",
      "│    │    └─BatchNorm2d: 3-11            [1, 128, 128, 128]        256\n",
      "│    │    └─ReLU: 3-12                   [1, 128, 128, 128]        --\n",
      "├─MaxPool2d: 1-4                         [1, 128, 64, 64]          --\n",
      "├─DoubleConv: 1-5                        [1, 256, 64, 64]          --\n",
      "│    └─Sequential: 2-3                   [1, 256, 64, 64]          --\n",
      "│    │    └─Conv2d: 3-13                 [1, 256, 64, 64]          295,168\n",
      "│    │    └─BatchNorm2d: 3-14            [1, 256, 64, 64]          512\n",
      "│    │    └─ReLU: 3-15                   [1, 256, 64, 64]          --\n",
      "│    │    └─Conv2d: 3-16                 [1, 256, 64, 64]          590,080\n",
      "│    │    └─BatchNorm2d: 3-17            [1, 256, 64, 64]          512\n",
      "│    │    └─ReLU: 3-18                   [1, 256, 64, 64]          --\n",
      "├─MaxPool2d: 1-6                         [1, 256, 32, 32]          --\n",
      "├─DoubleConv: 1-7                        [1, 512, 32, 32]          --\n",
      "│    └─Sequential: 2-4                   [1, 512, 32, 32]          --\n",
      "│    │    └─Conv2d: 3-19                 [1, 512, 32, 32]          1,180,160\n",
      "│    │    └─BatchNorm2d: 3-20            [1, 512, 32, 32]          1,024\n",
      "│    │    └─ReLU: 3-21                   [1, 512, 32, 32]          --\n",
      "│    │    └─Conv2d: 3-22                 [1, 512, 32, 32]          2,359,808\n",
      "│    │    └─BatchNorm2d: 3-23            [1, 512, 32, 32]          1,024\n",
      "│    │    └─ReLU: 3-24                   [1, 512, 32, 32]          --\n",
      "├─MaxPool2d: 1-8                         [1, 512, 16, 16]          --\n",
      "├─DoubleConv: 1-9                        [1, 1024, 16, 16]         --\n",
      "│    └─Sequential: 2-5                   [1, 1024, 16, 16]         --\n",
      "│    │    └─Conv2d: 3-25                 [1, 1024, 16, 16]         4,719,616\n",
      "│    │    └─BatchNorm2d: 3-26            [1, 1024, 16, 16]         2,048\n",
      "│    │    └─ReLU: 3-27                   [1, 1024, 16, 16]         --\n",
      "│    │    └─Conv2d: 3-28                 [1, 1024, 16, 16]         9,438,208\n",
      "│    │    └─BatchNorm2d: 3-29            [1, 1024, 16, 16]         2,048\n",
      "│    │    └─ReLU: 3-30                   [1, 1024, 16, 16]         --\n",
      "├─ConvTranspose2d: 1-10                  [1, 512, 32, 32]          2,097,664\n",
      "├─DoubleConv: 1-11                       [1, 512, 32, 32]          --\n",
      "│    └─Sequential: 2-6                   [1, 512, 32, 32]          --\n",
      "│    │    └─Conv2d: 3-31                 [1, 512, 32, 32]          4,719,104\n",
      "│    │    └─BatchNorm2d: 3-32            [1, 512, 32, 32]          1,024\n",
      "│    │    └─ReLU: 3-33                   [1, 512, 32, 32]          --\n",
      "│    │    └─Conv2d: 3-34                 [1, 512, 32, 32]          2,359,808\n",
      "│    │    └─BatchNorm2d: 3-35            [1, 512, 32, 32]          1,024\n",
      "│    │    └─ReLU: 3-36                   [1, 512, 32, 32]          --\n",
      "├─ConvTranspose2d: 1-12                  [1, 256, 64, 64]          524,544\n",
      "├─DoubleConv: 1-13                       [1, 256, 64, 64]          --\n",
      "│    └─Sequential: 2-7                   [1, 256, 64, 64]          --\n",
      "│    │    └─Conv2d: 3-37                 [1, 256, 64, 64]          1,179,904\n",
      "│    │    └─BatchNorm2d: 3-38            [1, 256, 64, 64]          512\n",
      "│    │    └─ReLU: 3-39                   [1, 256, 64, 64]          --\n",
      "│    │    └─Conv2d: 3-40                 [1, 256, 64, 64]          590,080\n",
      "│    │    └─BatchNorm2d: 3-41            [1, 256, 64, 64]          512\n",
      "│    │    └─ReLU: 3-42                   [1, 256, 64, 64]          --\n",
      "├─ConvTranspose2d: 1-14                  [1, 128, 128, 128]        131,200\n",
      "├─DoubleConv: 1-15                       [1, 128, 128, 128]        --\n",
      "│    └─Sequential: 2-8                   [1, 128, 128, 128]        --\n",
      "│    │    └─Conv2d: 3-43                 [1, 128, 128, 128]        295,040\n",
      "│    │    └─BatchNorm2d: 3-44            [1, 128, 128, 128]        256\n",
      "│    │    └─ReLU: 3-45                   [1, 128, 128, 128]        --\n",
      "│    │    └─Conv2d: 3-46                 [1, 128, 128, 128]        147,584\n",
      "│    │    └─BatchNorm2d: 3-47            [1, 128, 128, 128]        256\n",
      "│    │    └─ReLU: 3-48                   [1, 128, 128, 128]        --\n",
      "├─ConvTranspose2d: 1-16                  [1, 64, 256, 256]         32,832\n",
      "├─DoubleConv: 1-17                       [1, 64, 256, 256]         --\n",
      "│    └─Sequential: 2-9                   [1, 64, 256, 256]         --\n",
      "│    │    └─Conv2d: 3-49                 [1, 64, 256, 256]         73,792\n",
      "│    │    └─BatchNorm2d: 3-50            [1, 64, 256, 256]         128\n",
      "│    │    └─ReLU: 3-51                   [1, 64, 256, 256]         --\n",
      "│    │    └─Conv2d: 3-52                 [1, 64, 256, 256]         36,928\n",
      "│    │    └─BatchNorm2d: 3-53            [1, 64, 256, 256]         128\n",
      "│    │    └─ReLU: 3-54                   [1, 64, 256, 256]         --\n",
      "├─Conv2d: 1-18                           [1, 3, 256, 256]          195\n",
      "==========================================================================================\n",
      "Total params: 31,043,651\n",
      "Trainable params: 31,043,651\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 54.66\n",
      "==========================================================================================\n",
      "Input size (MB): 0.79\n",
      "Forward/backward pass size (MB): 576.19\n",
      "Params size (MB): 124.17\n",
      "Estimated Total Size (MB): 701.15\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_stats = tinf.summary(model, (1, 3, 256, 256), verbose=0)\n",
    "summary_str = str(model_stats)\n",
    "# summary_str contains the string representation of the summary!\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitetura da U-Net plotada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#         y = model(torch.randn(1, in_channels, 256, 256).to(device))\n",
    "#         tv.make_dot(y, params=dict(list(model.named_parameters()))).render(\"unet_architecture\", format=\"png\")\n",
    "        \n",
    "#         #plot.format = 'png'\n",
    "        \n",
    "#         print(\"Arquitetura da U-Net plotada com sucesso.\")\n",
    "# except ImportError:\n",
    "#         print(\"Para plotar a arquitetura, instale o pacote torchviz.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "summary() got an unexpected keyword argument 'depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbranching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: summary() got an unexpected keyword argument 'depth'"
     ]
    }
   ],
   "source": [
    "# summary(model, x,depth=1, dtypes=[torch.long],branching=False,verbose=1, col_width=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloco de duas convoluções 3x3 seguidas de ReLU.\n",
    "    Permite a parametrização do uso de BatchNorm.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_batch_norm=True):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        layers.extend([\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ])\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        self.double_conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementação da arquitetura U-Net parametrizável para geração de imagens.\n",
    "    Permite a parametrização do número de blocos de convolução e o uso de BatchNorm.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, base_filters=64, num_layers=5, use_batch_norm=True):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.base_filters = base_filters\n",
    "        self.num_layers = num_layers\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        filters = base_filters\n",
    "        \n",
    "        for i in range(num_layers-1):\n",
    "            self.encoder_layers.append(DoubleConv(in_channels, filters, use_batch_norm))\n",
    "            in_channels = filters\n",
    "            filters *= 2\n",
    "        \n",
    "        # Max-pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(filters // 2, filters, use_batch_norm)\n",
    "\n",
    "        # Decoder\n",
    "        self.up_layers = nn.ModuleList()\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        for i in range(num_layers-1, 0, -1):#range(num_layers-1, 0, -1)\n",
    "            filters //= 2\n",
    "            self.up_layers.append(nn.ConvTranspose2d(filters * 2, filters, kernel_size=2, stride=2))\n",
    "            self.up_convs.append(DoubleConv(filters * 2, filters , use_batch_norm))\n",
    "            \n",
    "\n",
    "        # Final convolution\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(base_filters, out_channels, kernel_size=1)\n",
    "    def see_model(self):\n",
    "        print(f\"\"\"\n",
    "        encoder: {self.encoder_layers}\n",
    "        pool: {self.pool}\n",
    "        botleneck: {self.bottleneck}\n",
    "        up laeyers: {self.up_layers}\n",
    "        up_convs: {self.up_convs}\n",
    "        final conv: {self.final_conv}\"\"\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        encoder_results = []\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "            encoder_results.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Decoder\n",
    "        for i, (up, conv) in enumerate(zip(self.up_layers, self.up_convs)):\n",
    "            x = up(x)\n",
    "            x = torch.cat((x, encoder_results[-(i+1)]), dim=1)\n",
    "            x = conv(x)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "def standardize_image(image):\n",
    "    \"\"\"\n",
    "    Função para padronizar as imagens de entrada.\n",
    "    \"\"\"\n",
    "    mean = image.mean()\n",
    "    std = image.std()\n",
    "    return (image - mean) / std\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Definindo o dispositivo (GPU se disponível)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Inicializando a U-Net parametrizável\n",
    "    in_channels = 3  # Por exemplo, RGB\n",
    "    out_channels = 3  # Saída de imagem RGB\n",
    "    base_filters = 64\n",
    "    num_layers = 4\n",
    "    use_batch_norm = True\n",
    "    model = UNet(in_channels, out_channels, base_filters, num_layers, use_batch_norm).to(device)\n",
    "    #model.see_model()\n",
    "\n",
    "    # Exibindo a arquitetura da rede\n",
    "    #summary(model, input_size=(in_channels, 256, 256))\n",
    "\n",
    "    # Exemplo de entrada\n",
    "    x = torch.randn(1, in_channels, 256, 256).to(device)  # Batch size 1, 256x256 imagem\n",
    "    # x = standardize_image(x)\n",
    "    output = model(x)\n",
    "\n",
    "    print(output.shape)  # Deve ser (1, out_channels, 256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Plotando a arquitetura da rede\n",
    "# try:\n",
    "\n",
    "#     y = model(torch.randn(1, in_channels, 256, 256).to(device))\n",
    "#     tv.make_dot(y, params=dict(list(model.named_parameters()))).render(\"unet_architecture_2\", format=\"png\")\n",
    "#     print(\"Arquitetura da U-Net plotada com sucesso.\")\n",
    "# except ImportError:\n",
    "#     print(\"Para plotar a arquitetura, instale o pacote torchviz.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input_size is not a recognized type. Please ensure input_size is valid.\nFor multiple inputs to the network, ensure input_size is a list of tuple sizes. If you are having trouble here, please submit a GitHub issue.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tinf\u001b[38;5;241m.\u001b[39msummary(model, x,depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtypes\u001b[38;5;241m=\u001b[39m[torch\u001b[38;5;241m.\u001b[39mlong],branching\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, col_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/CLEDiff/lib/python3.11/site-packages/torchinfo/torchinfo.py:220\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[0;32m--> 220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[1;32m    223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m forward_pass(\n\u001b[1;32m    224\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n",
      "File \u001b[0;32m~/anaconda3/envs/CLEDiff/lib/python3.11/site-packages/torchinfo/torchinfo.py:255\u001b[0m, in \u001b[0;36mprocess_input\u001b[0;34m(input_data, input_size, batch_dim, device, dtypes)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         dtypes \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfloat] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_size)\n\u001b[0;32m--> 255\u001b[0m     correct_input_size \u001b[38;5;241m=\u001b[39m get_correct_input_sizes(input_size)\n\u001b[1;32m    256\u001b[0m     x \u001b[38;5;241m=\u001b[39m get_input_tensor(correct_input_size, batch_dim, dtypes, device)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, correct_input_size\n",
      "File \u001b[0;32m~/anaconda3/envs/CLEDiff/lib/python3.11/site-packages/torchinfo/torchinfo.py:551\u001b[0m, in \u001b[0;36mget_correct_input_sizes\u001b[0;34m(input_size)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03mConvert input_size to the correct form, which is a list of tuples.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03mAlso handles multiple inputs to the network.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput_size is not a recognized type. Please ensure input_size is valid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    553\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor multiple inputs to the network, ensure input_size is a list of tuple \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    554\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msizes. If you are having trouble here, please submit a GitHub issue.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    555\u001b[0m     )\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m input_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m flatten(input_size)):\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput_data is invalid, or negative size found in input_data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Input_size is not a recognized type. Please ensure input_size is valid.\nFor multiple inputs to the network, ensure input_size is a list of tuple sizes. If you are having trouble here, please submit a GitHub issue."
     ]
    }
   ],
   "source": [
    "tinf.summary(model, x,depth=1, dtypes=[torch.long],branching=False,verbose=1, col_width=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpa memoria CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória alocada: 0 bytes\n",
      "Memória reservada: 0 bytes\n",
      "Memória alocada: 0 bytes\n",
      "Memória reservada: 0 bytes\n",
      "Memoria limpa\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "# Force a coleta de lixo para liberar a memória imediatamente\n",
    "gc.collect()\n",
    "\n",
    "# Libere o cache de memória do PyTorch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "print(f\"Memoria limpa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CycleGAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CycleGAN.ipynb\tdocs\t\t LICENSE  pix2pix.ipynb     scripts   util\n",
      "data\t\tenvironment.yml  models   README.md\t    test.py\n",
      "datasets\timgs\t\t options  requirements.txt  train.py\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# if not os.path.isdir(\"pytorch-CycleGAN-and-pix2pix\"):\n",
    "#     !git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git\n",
    "# os.chdir(\"pytorch-CycleGAN-and-pix2pix\")\n",
    "# !pip install -r requirements.txt \n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Falta aprender a resgatar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\t    pytorch-CycleGAN-and-pix2pix  unet_architecture_2.png\n",
      "Losses.yml  README.md\t\t\t  unet_architecture.png\n",
      "main.py     src\t\t\t\t  vit_architecture.png\n",
      "metrics     unet_architecture\t\t  work.ipynb\n",
      "models\t    unet_architecture_2\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image Shape: torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Encoder parametrizável com BatchNorm\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dims, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Inicializa uma lista para armazenar as camadas da rede\n",
    "        modules = []\n",
    "        \n",
    "        # Adiciona camadas de convolução e batch normalization conforme especificado em hidden_dims\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(input_channels, h_dim, kernel_size=4, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "            input_channels = h_dim\n",
    "        \n",
    "        # Define as camadas finais para calcular mu e logvar\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1] * 16 * 16, latent_dim)  # Supondo entrada de 256x256\n",
    "        self.fc_logvar = nn.Linear(hidden_dims[-1] * 16 * 16, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "# Decoder parametrizável com BatchNorm\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dims, output_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Inicializa uma lista para armazenar as camadas da rede\n",
    "        modules = []\n",
    "        \n",
    "        # Primeira camada completamente conectada para expandir a representação latente\n",
    "        self.fc = nn.Linear(latent_dim, hidden_dims[-1] * 16 * 16)\n",
    "        \n",
    "        # Adiciona camadas de convolução transposta e batch normalization conforme especificado em hidden_dims\n",
    "        hidden_dims.reverse()\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=4,\n",
    "                                       stride=2,\n",
    "                                       padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Camada final para reconstrução da imagem\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                   output_channels,\n",
    "                                   kernel_size=4,\n",
    "                                   stride=2,\n",
    "                                   padding=1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = torch.relu(self.fc(z))\n",
    "        x = x.view(x.size(0), -1, 16, 16)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Definição do VAE parametrizável\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dims, latent_dim, output_channels):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_channels, hidden_dims, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dims, output_channels)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "# Configuração de parâmetros\n",
    "latent_dim = 16\n",
    "hidden_dims = [32, 64, 128, 256]\n",
    "input_channels = 3\n",
    "output_channels = 3\n",
    "\n",
    "# Inicialização do modelo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE(input_channels=input_channels, hidden_dims=hidden_dims, latent_dim=latent_dim, output_channels=output_channels).to(device)\n",
    "\n",
    "# Teste com tensor randômico\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Gera um tensor randômico com formato (1, 3, 256, 256)\n",
    "    random_tensor = torch.randn(1, 3, 256, 256).to(device)\n",
    "    \n",
    "    # Passa o tensor randômico pelo encoder para obter mu e logvar\n",
    "    mu, logvar = model.encoder(random_tensor)\n",
    "    \n",
    "    # Reamostragem para obter o vetor latente z\n",
    "    z = model.reparameterize(mu, logvar)\n",
    "    \n",
    "    # Passa o vetor latente pelo decoder para gerar a imagem reconstruída\n",
    "    generated_image, _, _ = model(random_tensor)\n",
    "    \n",
    "    print(\"Generated Image Shape:\", generated_image.shape)  # Esperado: (1, 3, 256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "VAE                                           [1, 3, 256, 256]          --\n",
       "├─Encoder: 1-1                                [1, 16]                   --\n",
       "│    └─Sequential: 2-1                        [1, 256, 16, 16]          --\n",
       "│    │    └─Sequential: 3-1                   [1, 32, 128, 128]         --\n",
       "│    │    │    └─Conv2d: 4-1                  [1, 32, 128, 128]         1,568\n",
       "│    │    │    └─BatchNorm2d: 4-2             [1, 32, 128, 128]         64\n",
       "│    │    │    └─ReLU: 4-3                    [1, 32, 128, 128]         --\n",
       "│    │    └─Sequential: 3-2                   [1, 64, 64, 64]           --\n",
       "│    │    │    └─Conv2d: 4-4                  [1, 64, 64, 64]           32,832\n",
       "│    │    │    └─BatchNorm2d: 4-5             [1, 64, 64, 64]           128\n",
       "│    │    │    └─ReLU: 4-6                    [1, 64, 64, 64]           --\n",
       "│    │    └─Sequential: 3-3                   [1, 128, 32, 32]          --\n",
       "│    │    │    └─Conv2d: 4-7                  [1, 128, 32, 32]          131,200\n",
       "│    │    │    └─BatchNorm2d: 4-8             [1, 128, 32, 32]          256\n",
       "│    │    │    └─ReLU: 4-9                    [1, 128, 32, 32]          --\n",
       "│    │    └─Sequential: 3-4                   [1, 256, 16, 16]          --\n",
       "│    │    │    └─Conv2d: 4-10                 [1, 256, 16, 16]          524,544\n",
       "│    │    │    └─BatchNorm2d: 4-11            [1, 256, 16, 16]          512\n",
       "│    │    │    └─ReLU: 4-12                   [1, 256, 16, 16]          --\n",
       "│    └─Linear: 2-2                            [1, 16]                   1,048,592\n",
       "│    └─Linear: 2-3                            [1, 16]                   1,048,592\n",
       "├─Decoder: 1-2                                [1, 3, 256, 256]          --\n",
       "│    └─Linear: 2-4                            [1, 65536]                1,114,112\n",
       "│    └─Sequential: 2-5                        [1, 3, 256, 256]          --\n",
       "│    │    └─Sequential: 3-5                   [1, 128, 32, 32]          --\n",
       "│    │    │    └─ConvTranspose2d: 4-13        [1, 128, 32, 32]          524,416\n",
       "│    │    │    └─BatchNorm2d: 4-14            [1, 128, 32, 32]          256\n",
       "│    │    │    └─ReLU: 4-15                   [1, 128, 32, 32]          --\n",
       "│    │    └─Sequential: 3-6                   [1, 64, 64, 64]           --\n",
       "│    │    │    └─ConvTranspose2d: 4-16        [1, 64, 64, 64]           131,136\n",
       "│    │    │    └─BatchNorm2d: 4-17            [1, 64, 64, 64]           128\n",
       "│    │    │    └─ReLU: 4-18                   [1, 64, 64, 64]           --\n",
       "│    │    └─Sequential: 3-7                   [1, 32, 128, 128]         --\n",
       "│    │    │    └─ConvTranspose2d: 4-19        [1, 32, 128, 128]         32,800\n",
       "│    │    │    └─BatchNorm2d: 4-20            [1, 32, 128, 128]         64\n",
       "│    │    │    └─ReLU: 4-21                   [1, 32, 128, 128]         --\n",
       "│    │    └─Sequential: 3-8                   [1, 3, 256, 256]          --\n",
       "│    │    │    └─ConvTranspose2d: 4-22        [1, 3, 256, 256]          1,539\n",
       "│    │    │    └─Sigmoid: 4-23                [1, 3, 256, 256]          --\n",
       "===============================================================================================\n",
       "Total params: 4,592,739\n",
       "Trainable params: 4,592,739\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.14\n",
       "===============================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 32.51\n",
       "Params size (MB): 18.37\n",
       "Estimated Total Size (MB): 51.66\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinf.summary(model, (1,3,256,256),depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = x + self.dropout(mlp_output)\n",
    "        x = self.norm2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEnhancerTransformer(nn.Module):\n",
    "    def __init__(self, image_size=256, patch_size=16, num_channels=3, embed_dim=512, num_heads=8, mlp_dim=1024, num_layers=6, dropout=0.1):\n",
    "        super(ImageEnhancerTransformer, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_dim = num_channels * patch_size * patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embeddings = nn.Linear(self.patch_dim, embed_dim)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, self.patch_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self._to_patches(x)\n",
    "        x = self.patch_embeddings(x) + self.position_embeddings\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.fc(x)\n",
    "        x = self._from_patches(x, batch_size)\n",
    "        return x\n",
    "\n",
    "    def _to_patches(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        x = x.view(batch_size, channels, self.image_size // self.patch_size, self.patch_size,\n",
    "                   self.image_size // self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1).contiguous()\n",
    "        x = x.view(batch_size, self.num_patches, -1)\n",
    "        return x\n",
    "\n",
    "    def _from_patches(self, x, batch_size):\n",
    "        x = x.view(batch_size, self.image_size // self.patch_size, self.image_size // self.patch_size,\n",
    "                   self.patch_size, self.patch_size, -1)\n",
    "        x = x.permute(0, 5, 1, 3, 2, 4).contiguous()\n",
    "        x = x.view(batch_size, -1, self.image_size, self.image_size)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma do tensor de entrada: torch.Size([1, 3, 256, 256])\n",
      "Forma do tensor de saída: torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ImageEnhancerTransformer                 [1, 3, 256, 256]          131,072\n",
       "├─Linear: 1-1                            [1, 256, 512]             393,728\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─TransformerBlock: 2-1             [1, 256, 512]             --\n",
       "│    │    └─MultiheadAttention: 3-1      [1, 256, 512]             1,050,624\n",
       "│    │    └─Dropout: 3-2                 [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-3               [1, 256, 512]             1,024\n",
       "│    │    └─Sequential: 3-4              [1, 256, 512]             1,050,112\n",
       "│    │    └─Dropout: 3-5                 [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-6               [1, 256, 512]             1,024\n",
       "│    └─TransformerBlock: 2-2             [1, 256, 512]             --\n",
       "│    │    └─MultiheadAttention: 3-7      [1, 256, 512]             1,050,624\n",
       "│    │    └─Dropout: 3-8                 [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-9               [1, 256, 512]             1,024\n",
       "│    │    └─Sequential: 3-10             [1, 256, 512]             1,050,112\n",
       "│    │    └─Dropout: 3-11                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-12              [1, 256, 512]             1,024\n",
       "│    └─TransformerBlock: 2-3             [1, 256, 512]             --\n",
       "│    │    └─MultiheadAttention: 3-13     [1, 256, 512]             1,050,624\n",
       "│    │    └─Dropout: 3-14                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-15              [1, 256, 512]             1,024\n",
       "│    │    └─Sequential: 3-16             [1, 256, 512]             1,050,112\n",
       "│    │    └─Dropout: 3-17                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-18              [1, 256, 512]             1,024\n",
       "│    └─TransformerBlock: 2-4             [1, 256, 512]             --\n",
       "│    │    └─MultiheadAttention: 3-19     [1, 256, 512]             1,050,624\n",
       "│    │    └─Dropout: 3-20                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-21              [1, 256, 512]             1,024\n",
       "│    │    └─Sequential: 3-22             [1, 256, 512]             1,050,112\n",
       "│    │    └─Dropout: 3-23                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-24              [1, 256, 512]             1,024\n",
       "│    └─TransformerBlock: 2-5             [1, 256, 512]             --\n",
       "│    │    └─MultiheadAttention: 3-25     [1, 256, 512]             1,050,624\n",
       "│    │    └─Dropout: 3-26                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-27              [1, 256, 512]             1,024\n",
       "│    │    └─Sequential: 3-28             [1, 256, 512]             1,050,112\n",
       "│    │    └─Dropout: 3-29                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-30              [1, 256, 512]             1,024\n",
       "│    └─TransformerBlock: 2-6             [1, 256, 512]             --\n",
       "│    │    └─MultiheadAttention: 3-31     [1, 256, 512]             1,050,624\n",
       "│    │    └─Dropout: 3-32                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-33              [1, 256, 512]             1,024\n",
       "│    │    └─Sequential: 3-34             [1, 256, 512]             1,050,112\n",
       "│    │    └─Dropout: 3-35                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-36              [1, 256, 512]             1,024\n",
       "├─LayerNorm: 1-3                         [1, 256, 512]             1,024\n",
       "├─Linear: 1-4                            [1, 256, 768]             393,984\n",
       "==========================================================================================\n",
       "Total params: 13,536,512\n",
       "Trainable params: 13,536,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 7.10\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 35.13\n",
       "Params size (MB): 28.41\n",
       "Estimated Total Size (MB): 64.32\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100  # número de exemplos fictício\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Gera uma imagem aleatória como entrada e uma versão \"melhorada\" como alvo\n",
    "        input_image = Image.fromarray((np.random.rand(256, 256, 3) * 255).astype(np.uint8))\n",
    "        target_image = Image.fromarray((np.random.rand(256, 256, 3) * 255).astype(np.uint8))\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            target_image = self.transform(target_image)\n",
    "\n",
    "        return input_image, target_image\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "model = ImageEnhancerTransformer()\n",
    "# Gera um tensor aleatório como entrada\n",
    "input_tensor = torch.rand(1, 3, 256, 256)  # Batch size de 1, 3 canais, 256x256 de resolução\n",
    "\n",
    "# Passa o tensor pelo modelo\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "# Mostra as formas dos tensores de entrada e saída\n",
    "print(\"Forma do tensor de entrada:\", input_tensor.shape)\n",
    "print(\"Forma do tensor de saída:\", output_tensor.shape)\n",
    "\n",
    "tinf.summary(model, (1, 3, 256, 256),depth=3)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # Loop de treinamento\n",
    "# for epoch in range(10):  # número de épocas fictício\n",
    "#     for inputs, targets in dataloader:\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitetura da U-Net plotada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# # Plotando a arquitetura da rede\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# try:\n",
    "\n",
    "#     y = model(torch.randn(1, 3, 256, 256).to(device))\n",
    "#     tv.make_dot(y, params=dict(list(model.named_parameters()))).render(\"vit_architecture\", format=\"png\")\n",
    "#     print(\"Arquitetura da U-Net plotada com sucesso.\")\n",
    "# except ImportError:\n",
    "#     print(\"Para plotar a arquitetura, instale o pacote torchviz.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória alocada: 0 bytes\n",
      "Memória reservada: 0 bytes\n",
      "Memória alocada: 0 bytes\n",
      "Memória reservada: 0 bytes\n",
      "Memoria limpa\n"
     ]
    }
   ],
   "source": [
    "### Limpa memoria CUDA\n",
    "import gc\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "# Force a coleta de lixo para liberar a memória imediatamente\n",
    "gc.collect()\n",
    "\n",
    "# Libere o cache de memória do PyTorch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "print(f\"Memoria limpa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, use_lsgan=True):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.use_lsgan = use_lsgan\n",
    "        if use_lsgan:\n",
    "            self.criterion = nn.MSELoss()\n",
    "        else:\n",
    "            self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, output, target_is_real):\n",
    "        target = torch.ones_like(output) if target_is_real else torch.zeros_like(output)\n",
    "        loss = self.criterion(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Teste da classe GANLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color loss\t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorLoss, self).__init__()\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.criterion(input, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SSIM PSNR MS-SSIM\t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49616897106170654 -7.792870998382568 66.98175811767578 0.07644809782505035\n"
     ]
    }
   ],
   "source": [
    "from kornia.losses import ssim_loss as ssim, psnr_loss as psnr, MS_SSIMLoss as ms_ssim, charbonnier_loss as charbonnier\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "input1 = torch.rand(1, 3, 255, 255)\n",
    "input2 = torch.rand(1, 3, 255, 255)\n",
    "# print(np.max(input1.numpy()), np.max(input2.numpy()))\n",
    "\n",
    "loss = ssim(input1, input2, 11)\n",
    "\n",
    "loss2 = psnr(input1,input2, np.max(input1.numpy())) # 10 * log(4/((1.2-1)**2)) / log(10)\n",
    "loss3 = ms_ssim()\n",
    "\n",
    "loss4 = charbonnier(input1, input2,reduction='mean')\n",
    "\n",
    "# class SSIMLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SSIMLoss, self).__init__()\n",
    "\n",
    "#     def forward(self, img1, img2):\n",
    "#         return ssim(img1, img2)\n",
    "# class PSNRLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(PSNRLoss, self).__init__()\n",
    "\n",
    "#     def forward(self, img1, img2):\n",
    "#         return (img1, img2)\n",
    "print(loss.item(), loss2.item(), loss3(input1, input2).item(), loss4.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE\t \t\t \t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAELoss, self).__init__()\n",
    "        self.criterion = nn.L1Loss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.criterion(input, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAELoss, self).__init__()\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.criterion(input, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uranker\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUSIQ\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acumlative Superiority\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "light_loss : 0.0 // Time: 0.000449\n",
      "color_loss : 0.0 // Time: 0.020019\n",
      "L1Loss : 0.0 // Time: 0.000178\n",
      "L_exp : 0.0011275997385382652 // Time: 0.000307\n",
      "RGBLoss : 7.77901823312277e-06 // Time: 0.000181\n"
     ]
    }
   ],
   "source": [
    "# def color_loss(output, gt,mask=None):\n",
    "#     img_ref = F.normalize(output, p = 2, dim = 1)\n",
    "#     ref_p = F.normalize(gt, p = 2, dim = 1)\n",
    "#     if mask!=None:\n",
    "#         img_ref=mask*img_ref\n",
    "#         ref_p*=mask\n",
    "#     loss_cos = 1 - torch.mean(F.cosine_similarity(img_ref, ref_p, dim=1))\n",
    "#     # loss_cos = self.mse(img_ref, ref_p)\n",
    "#     return loss_cos\n",
    "\n",
    "# def light_loss(output,gt,mask=None):\n",
    "#     #output = torch.mean(output, 1, keepdim=True)\n",
    "#     #gt=torch.mean(gt,1,keepdim=True)\n",
    "#     output =output[:, 0:1, :, :] * 0.299 + output[:, 1:2, :, :] * 0.587 + output[:, 2:3, :, :] * 0.114\n",
    "#     gt = gt[:, 0:1, :, :] * 0.299 + gt[:, 1:2, :, :] * 0.587 + gt[:, 2:3, :, :] * 0.114\n",
    "#     if mask != None:\n",
    "#         output*=mask\n",
    "#         gt*=mask\n",
    "#     loss=F.l1_loss(output,gt)\n",
    "#     return loss\n",
    "\n",
    "###############################################\n",
    "\n",
    "class color_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(color_loss, self).__init__()\n",
    "    \n",
    "    def forward(self, output, gt,mask=None):\n",
    "        img_ref = F.normalize(output, p = 2, dim = 1)\n",
    "        ref_p = F.normalize(gt, p = 2, dim = 1)\n",
    "        if mask!=None:\n",
    "            img_ref=mask*img_ref\n",
    "            ref_p*=mask\n",
    "        loss_cos = 1 - torch.mean(F.cosine_similarity(img_ref, ref_p, dim=1))\n",
    "        # loss_cos = self.mse(img_ref, ref_p)\n",
    "        return loss_cos\n",
    "\n",
    "class light_loss(nn.Module):##pesquisar significado das modificacoes\n",
    "    def __init__(self):\n",
    "        super(light_loss, self).__init__()\n",
    "\n",
    "    def forward(self,output,gt,mask=None):\n",
    "        #output = torch.mean(output, 1, keepdim=True)\n",
    "        #gt=torch.mean(gt,1,keepdim=True)\n",
    "        output =output[:, 0:1, :, :] * 0.299 + output[:, 1:2, :, :] * 0.587 + output[:, 2:3, :, :] * 0.114\n",
    "        gt = gt[:, 0:1, :, :] * 0.299 + gt[:, 1:2, :, :] * 0.587 + gt[:, 2:3, :, :] * 0.114\n",
    "        if mask != None:\n",
    "            output*=mask\n",
    "            gt*=mask\n",
    "        loss=F.l1_loss(output,gt)\n",
    "        return loss\n",
    "\n",
    "##############################################3\n",
    "#não pegar\n",
    "class RGBloss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RGBloss, self).__init__()\n",
    "\n",
    "    def forward(self, x ):\n",
    "\n",
    "        b,c,h,w = x.shape\n",
    "\n",
    "        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n",
    "        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n",
    "        Drg = torch.pow(mr-mg,2)\n",
    "        Drb = torch.pow(mr-mb,2)\n",
    "        Dgb = torch.pow(mb-mg,2)\n",
    "        k = torch.pow(torch.pow(Drg,2) + torch.pow(Drb,2) + torch.pow(Dgb,2),0.5)\n",
    "        return k\n",
    "#não pegar\n",
    "class L_exp(nn.Module):\n",
    "    \"\"\"\n",
    "    patch_size: tamanho do patch para calcular a média\n",
    "    mean_val: valor médio esperado\n",
    "    \"\"\"\n",
    "    def __init__(self,patch_size,mean_val):\n",
    "        super(L_exp, self).__init__()\n",
    "        # print(1)\n",
    "        self.pool = nn.AvgPool2d(patch_size)\n",
    "        self.mean_val = mean_val\n",
    "\n",
    "    def forward(self, x ):\n",
    "        device=x.device\n",
    "        b,c,h,w = x.shape\n",
    "        x = torch.mean(x,1,keepdim=True)\n",
    "        mean = self.pool(x)\n",
    "\n",
    "        d = torch.mean(torch.pow(mean- torch.FloatTensor([self.mean_val] ).to(device),2))\n",
    "        return d\n",
    "   \n",
    "\n",
    "class L1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L1Loss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Calcula a diferença absoluta entre o input e o target\n",
    "        abs_diff = torch.abs(input - target)\n",
    "        # Calcula a média da diferença absoluta\n",
    "        loss = torch.mean(abs_diff)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "\n",
    "from numpy import mean, round, transpose\n",
    "from time import time\n",
    "\n",
    "#print(\"gt_image: \",gt_image.shape,\"t_image: \", t_image.shape, \"/n\")\n",
    "t_image = torch.rand(1, 3, 255, 255)\n",
    "gt_image = t_image#torch.rand(1, 3, 255, 255)   \n",
    "mean_val = mean(t_image.detach().numpy())\n",
    "losses = {\n",
    "\"light_loss\": light_loss(),#\n",
    "\"color_loss\": color_loss(),#\n",
    "\"L1Loss\": L1Loss(),\n",
    "\"L_exp\": L_exp(5,mean_val),\n",
    "\"RGBLoss\": RGBloss(),\n",
    "}\n",
    "\n",
    "for name, loss in losses.items():\n",
    "    start_time = time()\n",
    "    if name == \"L1Loss\" or name == \"light_loss\" or name == \"mse_loss\" or name == \"color_loss\":\n",
    "        loss = loss(t_image, gt_image)\n",
    "    else:\n",
    "        #print(name)\n",
    "        loss = loss(t_image)\n",
    "    end_time = time()\n",
    "    print(name, \":\", loss.item(), \"// Time:\", round(end_time-start_time,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dark Channel Loss: 0.0 time: 0.033286 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DarkChannelLoss(nn.Module):\n",
    "    def __init__(self, patch_size=15):\n",
    "        super(DarkChannelLoss, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Compute the Dark Channel Loss between the input and target images.\n",
    "        \n",
    "        Args:\n",
    "            input (Tensor): The predicted image with shape (N, C, H, W).\n",
    "            target (Tensor): The ground truth image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The Dark Channel Loss value.\n",
    "        \"\"\"\n",
    "        def dark_channel(image, patch_size):\n",
    "            # Compute the dark channel of an image\n",
    "            min_channel = torch.min(image, dim=1, keepdim=True)[0]\n",
    "            kernel = torch.ones((1, 1, patch_size, patch_size), device=image.device)\n",
    "            dark_channel = F.conv2d(min_channel, kernel, stride=1, padding=patch_size//2)\n",
    "            return dark_channel\n",
    "        \n",
    "        # Compute dark channels\n",
    "        dark_input = dark_channel(input, self.patch_size)\n",
    "        dark_target = dark_channel(target, self.patch_size)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = F.mse_loss(dark_input, dark_target)\n",
    "        return loss\n",
    "\n",
    "# Test the DarkChannelLoss\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a random tensor for input and target\n",
    "    input_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    target_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    \n",
    "    # Initialize the DarkChannelLoss\n",
    "    dcl_loss = DarkChannelLoss(patch_size=15)\n",
    "    start_time = time()\n",
    "    # Compute loss\n",
    "    loss_value = dcl_loss(input_tensor, input_tensor)\n",
    "    end_time = time()\n",
    "    print(f\"Dark Channel Loss: {loss_value.item()} time: {end_time-start_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Initialize the LCHChannelLoss\u001b[39;00m\n\u001b[1;32m     87\u001b[0m lch_loss \u001b[38;5;241m=\u001b[39m LCHChannelLoss(patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m s\u001b[38;5;66;03m#tart_time = time()\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     90\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m lch_loss(input_tensor, input_tensor)\n",
      "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class LCHChannelLoss(nn.Module):\n",
    "    def __init__(self, patch_size=15):\n",
    "        super(LCHChannelLoss, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def rgb_to_lch(self, rgb):\n",
    "        \"\"\"\n",
    "        Convert RGB to LCH color space.\n",
    "        \n",
    "        Args:\n",
    "            rgb (Tensor): The RGB image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The LCH image with shape (N, 3, H, W).\n",
    "        \"\"\"\n",
    "        # Convert RGB to XYZ\n",
    "        r = rgb[:, 0, :, :]\n",
    "        g = rgb[:, 1, :, :]\n",
    "        b = rgb[:, 2, :, :]\n",
    "        x = 0.4124564 * r + 0.3575761 * g + 0.1804375 * b\n",
    "        y = 0.2126729 * r + 0.7151522 * g + 0.0721750 * b\n",
    "        z = 0.0193339 * r + 0.1191920 * g + 0.9503041 * b\n",
    "        \n",
    "        # Normalize XYZ\n",
    "        x = x / 0.95047\n",
    "        z = z / 1.08883\n",
    "        \n",
    "        # Convert XYZ to LCH\n",
    "        l = 116 * y.pow(1/3) - 16\n",
    "        c = torch.sqrt((x - y.pow(1/3)) ** 2 + (y - z) ** 2)\n",
    "        h = torch.atan2(x - y.pow(1/3), y - z)\n",
    "        \n",
    "        return torch.stack([l, c, h], dim=1)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Compute the LCH Channel Loss between the input and target images.\n",
    "        \n",
    "        Args:\n",
    "            input (Tensor): The predicted image with shape (N, C, H, W).\n",
    "            target (Tensor): The ground truth image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The LCH Channel Loss value.\n",
    "        \"\"\"\n",
    "        def lch_channel(image, patch_size):\n",
    "            # Compute the LCH channels of an image\n",
    "            lch = self.rgb_to_lch(image)\n",
    "            l = lch[:, 0, :, :]\n",
    "            c = lch[:, 1, :, :]\n",
    "            h = lch[:, 2, :, :]\n",
    "            \n",
    "            # Apply convolution to compute dark channel (LCH-based)\n",
    "            kernel = torch.ones((1, 1, patch_size, patch_size), device=image.device)\n",
    "            l_channel = F.conv2d(l.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            c_channel = F.conv2d(c.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            h_channel = F.conv2d(h.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            \n",
    "            return l_channel, c_channel, h_channel\n",
    "        \n",
    "        # Compute LCH channels\n",
    "        lch_input = lch_channel(input, self.patch_size)\n",
    "        lch_target = lch_channel(target, self.patch_size)\n",
    "        \n",
    "        # Compute the loss\n",
    "        l_loss = F.mse_loss(lch_input[0], lch_target[0])\n",
    "        c_loss = F.mse_loss(lch_input[1], lch_target[1])\n",
    "        h_loss = F.mse_loss(lch_input[2], lch_target[2])\n",
    "        \n",
    "        # Total loss\n",
    "        loss = l_loss + c_loss + h_loss\n",
    "        return loss\n",
    "\n",
    "# Test the LCHChannelLoss\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a random tensor for input and target\n",
    "    input_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    target_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    \n",
    "    # Initialize the LCHChannelLoss\n",
    "    lch_loss = LCHChannelLoss(patch_size=15)\n",
    "    #start_time = time()\n",
    "    # Compute loss\n",
    "    loss_value = lch_loss(input_tensor, input_tensor)\n",
    "    #end_time = time()\n",
    "    print(f\"LCH Channel Loss: {loss_value.item()} time: {end_time-start_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lab Channel Loss: 0.0 time: 0.099752 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class LabChannelLoss(nn.Module):\n",
    "    def __init__(self, patch_size=15):\n",
    "        super(LabChannelLoss, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def rgb_to_lab(self, rgb):\n",
    "        \"\"\"\n",
    "        Convert RGB to LAB color space.\n",
    "        \n",
    "        Args:\n",
    "            rgb (Tensor): The RGB image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The LAB image with shape (N, 3, H, W).\n",
    "        \"\"\"\n",
    "        # Convert RGB to XYZ\n",
    "        r = rgb[:, 0, :, :]\n",
    "        g = rgb[:, 1, :, :]\n",
    "        b = rgb[:, 2, :, :]\n",
    "        \n",
    "        # Linear RGB to XYZ conversion\n",
    "        r = r / 255.0\n",
    "        g = g / 255.0\n",
    "        b = b / 255.0\n",
    "        \n",
    "        # Apply transformation matrix\n",
    "        x = 0.4124564 * r + 0.3575761 * g + 0.1804375 * b\n",
    "        y = 0.2126729 * r + 0.7151522 * g + 0.0721750 * b\n",
    "        z = 0.0193339 * r + 0.1191920 * g + 0.9503041 * b\n",
    "        \n",
    "        # Normalize XYZ\n",
    "        x = x / 0.95047\n",
    "        z = z / 1.08883\n",
    "        \n",
    "        # Convert XYZ to LAB\n",
    "        def f(t):\n",
    "            return torch.where(t > 0.008856, t.pow(1/3), 7.787 * t + 16/116)\n",
    "        \n",
    "        l = 116 * f(y) - 16\n",
    "        a = 500 * (f(x) - f(y))\n",
    "        b = 200 * (f(y) - f(z))\n",
    "        \n",
    "        return torch.stack([l, a, b], dim=1)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Compute the LAB Channel Loss between the input and target images.\n",
    "        \n",
    "        Args:\n",
    "            input (Tensor): The predicted image with shape (N, C, H, W).\n",
    "            target (Tensor): The ground truth image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The LAB Channel Loss value.\n",
    "        \"\"\"\n",
    "        def lab_channel(image, patch_size):\n",
    "            # Compute the LAB channels of an image\n",
    "            lab = self.rgb_to_lab(image)\n",
    "            l = lab[:, 0, :, :]\n",
    "            a = lab[:, 1, :, :]\n",
    "            b = lab[:, 2, :, :]\n",
    "            \n",
    "            # Apply convolution to compute channel loss (LAB-based)\n",
    "            kernel = torch.ones((1, 1, patch_size, patch_size), device=image.device)\n",
    "            l_channel = F.conv2d(l.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            a_channel = F.conv2d(a.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            b_channel = F.conv2d(b.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            \n",
    "            return l_channel, a_channel, b_channel\n",
    "        \n",
    "        # Compute LAB channels\n",
    "        lab_input = lab_channel(input, self.patch_size)\n",
    "        lab_target = lab_channel(target, self.patch_size)\n",
    "        \n",
    "        # Compute the loss\n",
    "        l_loss = F.mse_loss(lab_input[0], lab_target[0])\n",
    "        a_loss = F.mse_loss(lab_input[1], lab_target[1])\n",
    "        b_loss = F.mse_loss(lab_input[2], lab_target[2])\n",
    "        \n",
    "        # Total loss\n",
    "        loss = l_loss + a_loss + b_loss\n",
    "        return loss\n",
    "\n",
    "# Test the LabChannelLoss\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a random tensor for input and target\n",
    "    input_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    target_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    \n",
    "    # Initialize the LabChannelLoss\n",
    "    lab_loss = LabChannelLoss(patch_size=15)\n",
    "    start_time = time()\n",
    "    # Compute loss\n",
    "    loss_value = lab_loss(input_tensor, input_tensor)\n",
    "    end_time = time()\n",
    "    print(f\"Lab Channel Loss: {round(loss_value.item())} time: {round(end_time-start_time,6)} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YUV Channel Loss: 0.0, time: 0.071526 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class YUVChannelLoss(nn.Module):\n",
    "    def __init__(self, patch_size=15):\n",
    "        super(YUVChannelLoss, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def rgb_to_yuv(self, rgb):\n",
    "        \"\"\"\n",
    "        Convert RGB to YUV color space.\n",
    "        \n",
    "        Args:\n",
    "            rgb (Tensor): The RGB image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The YUV image with shape (N, 3, H, W).\n",
    "        \"\"\"\n",
    "        # Convert RGB to YUV\n",
    "        r = rgb[:, 0, :, :]\n",
    "        g = rgb[:, 1, :, :]\n",
    "        b = rgb[:, 2, :, :]\n",
    "        \n",
    "        y = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "        u = -0.14713 * r - 0.28886 * g + 0.436 * b\n",
    "        v = 0.615 * r - 0.51499 * g - 0.10001 * b\n",
    "        \n",
    "        return torch.stack([y, u, v], dim=1)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Compute the YUV Channel Loss between the input and target images.\n",
    "        \n",
    "        Args:\n",
    "            input (Tensor): The predicted image with shape (N, C, H, W).\n",
    "            target (Tensor): The ground truth image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The YUV Channel Loss value.\n",
    "        \"\"\"\n",
    "        def yuv_channel(image, patch_size):\n",
    "            # Compute the YUV channels of an image\n",
    "            yuv = self.rgb_to_yuv(image)\n",
    "            y = yuv[:, 0, :, :]\n",
    "            u = yuv[:, 1, :, :]\n",
    "            v = yuv[:, 2, :, :]\n",
    "            \n",
    "            # Apply convolution to compute channel loss (YUV-based)\n",
    "            kernel = torch.ones((1, 1, patch_size, patch_size), device=image.device)\n",
    "            y_channel = F.conv2d(y.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            u_channel = F.conv2d(u.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            v_channel = F.conv2d(v.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            \n",
    "            return y_channel, u_channel, v_channel\n",
    "        \n",
    "        # Compute YUV channels\n",
    "        yuv_input = yuv_channel(input, self.patch_size)\n",
    "        yuv_target = yuv_channel(target, self.patch_size)\n",
    "        \n",
    "        # Compute the loss\n",
    "        y_loss = F.mse_loss(yuv_input[0], yuv_target[0])\n",
    "        u_loss = F.mse_loss(yuv_input[1], yuv_target[1])\n",
    "        v_loss = F.mse_loss(yuv_input[2], yuv_target[2])\n",
    "        \n",
    "        # Total loss\n",
    "        loss = y_loss + u_loss + v_loss\n",
    "        return loss\n",
    "\n",
    "# Test the YUVChannelLoss\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a random tensor for input and target\n",
    "    input_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    target_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    \n",
    "    # Initialize the YUVChannelLoss\n",
    "    yuv_loss = YUVChannelLoss(patch_size=15)\n",
    "    start_time = time()\n",
    "    # Compute loss\n",
    "    loss_value = yuv_loss(input_tensor, input_tensor)\n",
    "    end_time = time()\n",
    "    \n",
    "    print(f\"YUV Channel Loss: {loss_value.item()}, time: {end_time-start_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSV Channel Loss: 0.0 time: 0.073903 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HSVChannelLoss(nn.Module):\n",
    "    def __init__(self, patch_size=15):\n",
    "        super(HSVChannelLoss, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def rgb_to_hsv(self, rgb):\n",
    "        \"\"\"\n",
    "        Convert RGB to HSV color space.\n",
    "        \n",
    "        Args:\n",
    "            rgb (Tensor): The RGB image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The HSV image with shape (N, 3, H, W).\n",
    "        \"\"\"\n",
    "        r = rgb[:, 0, :, :]\n",
    "        g = rgb[:, 1, :, :]\n",
    "        b = rgb[:, 2, :, :]\n",
    "\n",
    "        max_val, _ = torch.max(torch.stack([r, g, b], dim=1), dim=1)\n",
    "        min_val, _ = torch.min(torch.stack([r, g, b], dim=1), dim=1)\n",
    "\n",
    "        delta = max_val - min_val\n",
    "        s = torch.where(max_val == 0, torch.tensor(0.0, device=rgb.device), delta / max_val)\n",
    "\n",
    "        h = torch.where(\n",
    "            delta == 0,\n",
    "            torch.tensor(0.0, device=rgb.device),\n",
    "            torch.where(\n",
    "                max_val == r,\n",
    "                (g - b) / delta % 6,\n",
    "                torch.where(\n",
    "                    max_val == g,\n",
    "                    (b - r) / delta + 2,\n",
    "                    (r - g) / delta + 4\n",
    "                )\n",
    "            ) / 6\n",
    "        )\n",
    "        h = (h + 1) % 1\n",
    "\n",
    "        return torch.stack([h, s, max_val], dim=1)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Compute the HSV Channel Loss between the input and target images.\n",
    "        \n",
    "        Args:\n",
    "            input (Tensor): The predicted image with shape (N, C, H, W).\n",
    "            target (Tensor): The ground truth image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The HSV Channel Loss value.\n",
    "        \"\"\"\n",
    "        def hsv_channel(image, patch_size):\n",
    "            # Compute the HSV channels of an image\n",
    "            hsv = self.rgb_to_hsv(image)\n",
    "            h = hsv[:, 0, :, :]\n",
    "            s = hsv[:, 1, :, :]\n",
    "            v = hsv[:, 2, :, :]\n",
    "\n",
    "            # Apply convolution to compute channel loss (HSV-based)\n",
    "            kernel = torch.ones((1, 1, patch_size, patch_size), device=image.device)\n",
    "            h_channel = F.conv2d(h.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            s_channel = F.conv2d(s.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            v_channel = F.conv2d(v.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "\n",
    "            return h_channel, s_channel, v_channel\n",
    "\n",
    "        # Compute HSV channels\n",
    "        hsv_input = hsv_channel(input, self.patch_size)\n",
    "        hsv_target = hsv_channel(target, self.patch_size)\n",
    "\n",
    "        # Compute the loss\n",
    "        h_loss = F.mse_loss(hsv_input[0], hsv_target[0])\n",
    "        s_loss = F.mse_loss(hsv_input[1], hsv_target[1])\n",
    "        v_loss = F.mse_loss(hsv_input[2], hsv_target[2])\n",
    "\n",
    "        # Total loss\n",
    "        loss = h_loss + s_loss + v_loss\n",
    "        return loss\n",
    "\n",
    "# Test the HSVChannelLoss\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a random tensor for input and target\n",
    "    input_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    target_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    \n",
    "    # Initialize the HSVChannelLoss\n",
    "    hsv_loss = HSVChannelLoss(patch_size=15)\n",
    "    start_time = time()\n",
    "    # Compute loss\n",
    "    loss_value = hsv_loss(input_tensor, input_tensor)\n",
    "    end_time = time()\n",
    "    print(f\"HSV Channel Loss: {loss_value.item()} time: {end_time-start_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YCbCr Channel Loss: 0.0 time: 0.065857 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class YCbCrChannelLoss(nn.Module):\n",
    "    def __init__(self, patch_size=15):\n",
    "        super(YCbCrChannelLoss, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def rgb_to_ycbcr(self, rgb):\n",
    "        \"\"\"\n",
    "        Convert RGB to YCbCr color space.\n",
    "        \n",
    "        Args:\n",
    "            rgb (Tensor): The RGB image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The YCbCr image with shape (N, 3, H, W).\n",
    "        \"\"\"\n",
    "        r = rgb[:, 0, :, :]\n",
    "        g = rgb[:, 1, :, :]\n",
    "        b = rgb[:, 2, :, :]\n",
    "\n",
    "        # YCbCr conversion coefficients\n",
    "        y = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "        cb = -0.169 * r - 0.331 * g + 0.500 * b + 128\n",
    "        cr = 0.500 * r - 0.460 * g - 0.040 * b + 128\n",
    "        \n",
    "        return torch.stack([y, cb, cr], dim=1)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Compute the YCbCr Channel Loss between the input and target images.\n",
    "        \n",
    "        Args:\n",
    "            input (Tensor): The predicted image with shape (N, C, H, W).\n",
    "            target (Tensor): The ground truth image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The YCbCr Channel Loss value.\n",
    "        \"\"\"\n",
    "        def ycbcr_channel(image, patch_size):\n",
    "            # Compute the YCbCr channels of an image\n",
    "            ycbcr = self.rgb_to_ycbcr(image)\n",
    "            y = ycbcr[:, 0, :, :]\n",
    "            cb = ycbcr[:, 1, :, :]\n",
    "            cr = ycbcr[:, 2, :, :]\n",
    "\n",
    "            # Apply convolution to compute channel loss (YCbCr-based)\n",
    "            kernel = torch.ones((1, 1, patch_size, patch_size), device=image.device)\n",
    "            y_channel = F.conv2d(y.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            cb_channel = F.conv2d(cb.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            cr_channel = F.conv2d(cr.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "\n",
    "            return y_channel, cb_channel, cr_channel\n",
    "\n",
    "        # Compute YCbCr channels\n",
    "        ycbcr_input = ycbcr_channel(input, self.patch_size)\n",
    "        ycbcr_target = ycbcr_channel(target, self.patch_size)\n",
    "\n",
    "        # Compute the loss\n",
    "        y_loss = F.mse_loss(ycbcr_input[0], ycbcr_target[0])\n",
    "        cb_loss = F.mse_loss(ycbcr_input[1], ycbcr_target[1])\n",
    "        cr_loss = F.mse_loss(ycbcr_input[2], ycbcr_target[2])\n",
    "\n",
    "        # Total loss\n",
    "        loss = y_loss + cb_loss + cr_loss\n",
    "        return loss\n",
    "\n",
    "# Test the YCbCrChannelLoss\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a random tensor for input and target\n",
    "    input_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    target_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    \n",
    "    # Initialize the YCbCrChannelLoss\n",
    "    ycbcr_loss = YCbCrChannelLoss(patch_size=15)\n",
    "    start_time = time()\n",
    "    # Compute loss\n",
    "    loss_value = ycbcr_loss(input_tensor, input_tensor)\n",
    "    end_time = time()\n",
    "    print(f\"YCbCr Channel Loss: {loss_value.item()} time: {end_time-start_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIELUV Channel Loss: 0.0 time: 0.077800 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CIELUVChannelLoss(nn.Module):\n",
    "    def __init__(self, patch_size=15):\n",
    "        super(CIELUVChannelLoss, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def rgb_to_cieluv(self, rgb):\n",
    "        \"\"\"\n",
    "        Convert RGB to CIELUV color space.\n",
    "        \n",
    "        Args:\n",
    "            rgb (Tensor): The RGB image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The CIELUV image with shape (N, 3, H, W).\n",
    "        \"\"\"\n",
    "        # Convert RGB to XYZ\n",
    "        r = rgb[:, 0, :, :]\n",
    "        g = rgb[:, 1, :, :]\n",
    "        b = rgb[:, 2, :, :]\n",
    "\n",
    "        x = 0.4124564 * r + 0.3575761 * g + 0.1804375 * b\n",
    "        y = 0.2126729 * r + 0.7151522 * g + 0.0721750 * b\n",
    "        z = 0.0193339 * r + 0.1191920 * g + 0.9503041 * b\n",
    "\n",
    "        # Normalize XYZ\n",
    "        x_n =  0.95047\n",
    "        y_n =  1.00000\n",
    "        z_n =  1.08883\n",
    "\n",
    "        x = x / x_n\n",
    "        z = z / z_n\n",
    "\n",
    "        def f(t):\n",
    "            return torch.where(t > 0.008856, t.pow(1/3), 7.787 * t + 16/116)\n",
    "\n",
    "        l = 116 * f(y) - 16\n",
    "        u = 13 * l * (f(x) - f(y))\n",
    "        v = 13 * l * (f(y) - f(z))\n",
    "\n",
    "        return torch.stack([l, u, v], dim=1)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Compute the CIELUV Channel Loss between the input and target images.\n",
    "        \n",
    "        Args:\n",
    "            input (Tensor): The predicted image with shape (N, C, H, W).\n",
    "            target (Tensor): The ground truth image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The CIELUV Channel Loss value.\n",
    "        \"\"\"\n",
    "        def cieluv_channel(image, patch_size):\n",
    "            # Compute the CIELUV channels of an image\n",
    "            cieluv = self.rgb_to_cieluv(image)\n",
    "            l = cieluv[:, 0, :, :]\n",
    "            u = cieluv[:, 1, :, :]\n",
    "            v = cieluv[:, 2, :, :]\n",
    "\n",
    "            # Apply convolution to compute channel loss (CIELUV-based)\n",
    "            kernel = torch.ones((1, 1, patch_size, patch_size), device=image.device)\n",
    "            l_channel = F.conv2d(l.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            u_channel = F.conv2d(u.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            v_channel = F.conv2d(v.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "\n",
    "            return l_channel, u_channel, v_channel\n",
    "\n",
    "        # Compute CIELUV channels\n",
    "        cieluv_input = cieluv_channel(input, self.patch_size)\n",
    "        cieluv_target = cieluv_channel(target, self.patch_size)\n",
    "\n",
    "        # Compute the loss\n",
    "        l_loss = F.mse_loss(cieluv_input[0], cieluv_target[0])\n",
    "        u_loss = F.mse_loss(cieluv_input[1], cieluv_target[1])\n",
    "        v_loss = F.mse_loss(cieluv_input[2], cieluv_target[2])\n",
    "\n",
    "        # Total loss\n",
    "        loss = l_loss + u_loss + v_loss\n",
    "        return loss\n",
    "\n",
    "# Test the CIELUVChannelLoss\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a random tensor for input and target\n",
    "    input_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    target_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    \n",
    "    # Initialize the CIELUVChannelLoss\n",
    "    cieluv_loss = CIELUVChannelLoss(patch_size=15)\n",
    "    start_time = time()\n",
    "    # Compute loss\n",
    "    loss_value = cieluv_loss(input_tensor, input_tensor)\n",
    "    end_time = time()\n",
    "    print(f\"CIELUV Channel Loss: {loss_value.item()} time: {end_time-start_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YUV420 Channel Loss: 0.0, time: 0.086384 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class YUV420ChannelLoss(nn.Module):\n",
    "    def __init__(self, patch_size=15):\n",
    "        super(YUV420ChannelLoss, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def rgb_to_yuv420(self, rgb):\n",
    "        \"\"\"\n",
    "        Convert RGB to YUV420 color space.\n",
    "        \n",
    "        Args:\n",
    "            rgb (Tensor): The RGB image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor, Tensor]: The Y, U, and V images with shapes (N, 1, H, W), (N, 1, H/2, W/2), (N, 1, H/2, W/2).\n",
    "        \"\"\"\n",
    "        r = rgb[:, 0, :, :]\n",
    "        g = rgb[:, 1, :, :]\n",
    "        b = rgb[:, 2, :, :]\n",
    "\n",
    "        y = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "        u = -0.169 * r - 0.331 * g + 0.500 * b + 128\n",
    "        v = 0.500 * r - 0.460 * g - 0.040 * b + 128\n",
    "\n",
    "        # Downsample U and V\n",
    "        u = F.avg_pool2d(u.unsqueeze(1), kernel_size=2, stride=2, padding=0).squeeze(1)\n",
    "        v = F.avg_pool2d(v.unsqueeze(1), kernel_size=2, stride=2, padding=0).squeeze(1)\n",
    "        \n",
    "        return y, u, v\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Compute the YUV420 Channel Loss between the input and target images.\n",
    "        \n",
    "        Args:\n",
    "            input (Tensor): The predicted image with shape (N, C, H, W).\n",
    "            target (Tensor): The ground truth image with shape (N, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The YUV420 Channel Loss value.\n",
    "        \"\"\"\n",
    "        def yuv420_channel(image, patch_size):\n",
    "            # Compute the YUV420 channels of an image\n",
    "            y, u, v = self.rgb_to_yuv420(image)\n",
    "            y_target, u_target, v_target = self.rgb_to_yuv420(target)\n",
    "\n",
    "            # Apply convolution to compute channel loss (YUV420-based)\n",
    "            kernel = torch.ones((1, 1, patch_size, patch_size), device=image.device)\n",
    "            y_channel = F.conv2d(y.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            u_channel = F.conv2d(u.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            v_channel = F.conv2d(v.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "\n",
    "            y_target_channel = F.conv2d(y_target.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            u_target_channel = F.conv2d(u_target.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "            v_target_channel = F.conv2d(v_target.unsqueeze(1), kernel, stride=1, padding=patch_size//2)\n",
    "\n",
    "            return y_channel, u_channel, v_channel, y_target_channel, u_target_channel, v_target_channel\n",
    "\n",
    "        # Compute YUV420 channels\n",
    "        yuv420_input = yuv420_channel(input, self.patch_size)\n",
    "        yuv420_target = yuv420_channel(target, self.patch_size)\n",
    "\n",
    "        # Compute the loss\n",
    "        y_loss = F.mse_loss(yuv420_input[0], yuv420_target[0])\n",
    "        u_loss = F.mse_loss(yuv420_input[1], yuv420_target[1])\n",
    "        v_loss = F.mse_loss(yuv420_input[2], yuv420_target[2])\n",
    "\n",
    "        # Total loss\n",
    "        loss = y_loss + u_loss + v_loss\n",
    "        return loss\n",
    "\n",
    "# Test the YUV420ChannelLoss\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a random tensor for input and target\n",
    "    input_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    target_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    \n",
    "    # Initialize the YUV420ChannelLoss\n",
    "    yuv420_loss = YUV420ChannelLoss(patch_size=15)\n",
    "    start_time = time()\n",
    "    # Compute loss\n",
    "    loss_value = yuv420_loss(input_tensor, input_tensor)\n",
    "    end_time = time()\n",
    "    print(f\"YUV420 Channel Loss: {loss_value.item()}, time: {end_time-start_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram Color Loss: 0.0, time: 0.000908 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HistogramColorLoss(nn.Module):\n",
    "    def __init__(self, bins=256):\n",
    "        super(HistogramColorLoss, self).__init__()\n",
    "        self.bins = bins\n",
    "\n",
    "    def compute_histogram(self, image, bins):\n",
    "        \"\"\"\n",
    "        Compute the histogram of an image.\n",
    "\n",
    "        Args:\n",
    "            image (Tensor): The image with shape (N, C, H, W).\n",
    "            bins (int): Number of bins for the histogram.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The histogram with shape (N, C, bins).\n",
    "        \"\"\"\n",
    "        N, C, H, W = image.shape\n",
    "        histograms = []\n",
    "        for c in range(C):\n",
    "            channel = image[:, c, :, :].view(N, -1)  # Flatten the channel\n",
    "            histogram = torch.histc(channel, bins=bins, min=0, max=1)  # Compute histogram\n",
    "            histograms.append(histogram)\n",
    "        \n",
    "        return torch.stack(histograms, dim=1)  # Shape (N, C, bins)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Compute the histogram color loss between the input and target images.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): The predicted image with shape (N, C, H, W).\n",
    "            target (Tensor): The ground truth image with shape (N, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The Histogram Color Loss value.\n",
    "        \"\"\"\n",
    "        # Normalize images to [0, 1] range\n",
    "        input = (input - input.min()) / (input.max() - input.min())\n",
    "        target = (target - target.min()) / (target.max() - target.min())\n",
    "        \n",
    "        # Compute histograms\n",
    "        hist_input = self.compute_histogram(input, self.bins)\n",
    "        hist_target = self.compute_histogram(target, self.bins)\n",
    "        \n",
    "        # Calculate the histogram loss\n",
    "        loss = F.mse_loss(hist_input, hist_target)\n",
    "        return loss\n",
    "\n",
    "# Test the HistogramColorLoss\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a random tensor for input and target\n",
    "    input_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    target_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    \n",
    "    # Initialize the HistogramColorLoss\n",
    "    histogram_loss = HistogramColorLoss(bins=256)\n",
    "    start_time = time()\n",
    "    # Compute loss\n",
    "    loss_value = histogram_loss(input_tensor, input_tensor)\n",
    "    end_time = time()\n",
    "    print(f\"Histogram Color Loss: {loss_value.item()}, time: {end_time-start_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptual VGG Squeezenet Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pdi_4/anaconda3/envs/CLEDiff/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/pdi_4/anaconda3/envs/CLEDiff/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/pdi_4/anaconda3/envs/CLEDiff/lib/python3.11/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [squeeze], v[0.1], spatial [off]\n",
      "Loading model from: /home/pdi_4/anaconda3/envs/CLEDiff/lib/python3.11/site-packages/lpips/weights/v0.1/squeeze.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pdi_4/anaconda3/envs/CLEDiff/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/pdi_4/anaconda3/envs/CLEDiff/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/pdi_4/anaconda3/envs/CLEDiff/lib/python3.11/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "import lpips\n",
    "\n",
    "\n",
    "loss_vgg11 = lpips.LPIPS(net='vgg')#a vgg vem do lpips entao\n",
    "loss_squeeze = lpips.LPIPS(net='squeeze')\n",
    "loss_alex = lpips.LPIPS(net='alex')\n",
    "#from torchvision.models import vgg11, vgg11_bn, vgg16, vgg16_bn, vgg19_bn, vgg19, vgg13, vgg13_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "input1 = torch.randn(1,3,256,256)\n",
    "print(loss_vgg11(input1,input1).item(),loss_squeeze(input1,input1).item(),loss_alex(input1,input1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG perceptuais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, vgg_model='vgg16', layer_indices=None):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        # Load the VGG model\n",
    "        if vgg_model == 'vgg11':\n",
    "            self.vgg = models.vgg11(weights=models.VGG11_Weights.IMAGENET1K_V1).features\n",
    "        elif vgg_model == 'vgg11_bn':\n",
    "            self.vgg = models.vgg11_bn(weights=models.VGG11_BN_Weights.IMAGENET1K_V1).features\n",
    "        elif vgg_model == 'vgg13':\n",
    "            self.vgg = models.vgg13(weights=models.VGG13_Weights.IMAGENET1K_V1).features\n",
    "        elif vgg_model == 'vgg13_bn':\n",
    "            self.vgg = models.vgg13_bn(weights=models.VGG13_BN_Weights.IMAGENET1K_V1).features\n",
    "        elif vgg_model == 'vgg16':\n",
    "            self.vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n",
    "        elif vgg_model == 'vgg16_bn':\n",
    "            self.vgg = models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1).features\n",
    "        elif vgg_model == 'vgg19':\n",
    "            self.vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n",
    "        elif vgg_model == 'vgg19_bn':\n",
    "            self.vgg = models.vgg19_bn(weights=models.VGG19_BN_Weights.IMAGENET1K_V1).features\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported VGG model type\")\n",
    "\n",
    "        self.vgg.eval()  # Set to evaluation mode\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False  # Freeze the parameters\n",
    "\n",
    "        # Specify the layers to extract features from\n",
    "        if layer_indices is None:\n",
    "            self.layer_indices = [3, 8, 15, 22]  # Default layers for VGG16\n",
    "        else:\n",
    "            self.layer_indices = layer_indices\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Normalize the inputs\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n",
    "        x = (x - mean) / std\n",
    "        y = (y - mean) / std\n",
    "\n",
    "        # Extract features\n",
    "        x_features = self.extract_features(x)\n",
    "        y_features = self.extract_features(y)\n",
    "\n",
    "        # Calculate perceptual loss\n",
    "        loss = 0.0\n",
    "        for xf, yf in zip(x_features, y_features):\n",
    "            loss += nn.functional.l1_loss(xf, yf)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        features = []\n",
    "        for i, layer in enumerate(self.vgg):\n",
    "            x = layer(x)\n",
    "            if i in self.layer_indices:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "# Example usage:\n",
    "# Initialize the loss function\n",
    "#loss_function = VGGPerceptualLoss(vgg_model='vgg16', layer_indices=[3, 8, 15, 22])\n",
    "\n",
    "# Assume x and y are your input and target images respectively\n",
    "# x = ...\n",
    "# y = ...\n",
    "\n",
    "# Calculate the loss\n",
    "# loss = loss_function(x, y)\n",
    "loss_function = VGGPerceptualLoss(vgg_model='vgg13_bn', layer_indices=[3, 8, 15, 22])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 256, 256)\n",
    "y =x\n",
    "print(loss_function(x, y).item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Loss: 0.0\n",
      "Time taken: 0.003594 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class GradientLossOpenCV(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GradientLossOpenCV, self).__init__()\n",
    "    \n",
    "    def compute_gradient(self, image):\n",
    "        \"\"\"\n",
    "        Compute the gradient of an image using OpenCV Sobel filters.\n",
    "\n",
    "        Args:\n",
    "            image (Tensor): The image with shape (N, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient magnitude with shape (N, C, H, W).\n",
    "        \"\"\"\n",
    "        # Convert PyTorch tensor to NumPy array\n",
    "        image_np = image.detach().cpu().numpy()\n",
    "        N, C, H, W = image_np.shape\n",
    "        \n",
    "        gradient_magnitude = np.zeros((N, C, H, W), dtype=np.float32)\n",
    "        \n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                img = image_np[n, c, :, :]\n",
    "                grad_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "                grad_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "                grad_mag = np.sqrt(grad_x ** 2 + grad_y ** 2)\n",
    "                gradient_magnitude[n, c, :, :] = grad_mag\n",
    "        \n",
    "        # Convert NumPy array back to PyTorch tensor\n",
    "        gradient_magnitude_tensor = torch.tensor(gradient_magnitude).to(image.device)\n",
    "        \n",
    "        return gradient_magnitude_tensor\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Compute the gradient loss between the input and target images.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): The predicted image with shape (N, C, H, W).\n",
    "            target (Tensor): The ground truth image with shape (N, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The Gradient Loss value.\n",
    "        \"\"\"\n",
    "        # Compute gradients\n",
    "        gradient_input = self.compute_gradient(input)\n",
    "        gradient_target = self.compute_gradient(target)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = F.mse_loss(gradient_input, gradient_target)\n",
    "        return loss\n",
    "\n",
    "# Test the GradientLossOpenCV with timing\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a random tensor for input and target\n",
    "    input_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    target_tensor = torch.rand((1, 3, 256, 256))  # Example shape (N, C, H, W)\n",
    "    \n",
    "    # Initialize the GradientLossOpenCV\n",
    "    gradient_loss = GradientLossOpenCV()\n",
    "    \n",
    "    # Measure the time taken to compute the loss\n",
    "    start_time = time.time()\n",
    "    loss_value = gradient_loss(input_tensor, input_tensor)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Gradient Loss: {loss_value.item()}\")\n",
    "    print(f\"Time taken: {end_time - start_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Losses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
