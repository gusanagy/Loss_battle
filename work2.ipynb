{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A notebook for test and debug some python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "* Create a conda env [X]\n",
    "*     Im having a error [X] (resolvi usando pip no ambiente)\n",
    "*   ERRO TRANSFORM TORCHVISION\n",
    "* Selecionar e colocar arquiteturas [X]\n",
    "* unet[X]\n",
    "* ddpm\n",
    "* cyclogan \n",
    "* vae\n",
    "* vit [X]\n",
    "* Colocar funcoes de perda da tabela\n",
    "* Colocar todas as metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the utils libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#from torchvision import transforms \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: torchviz in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: torchinfo in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: graphviz in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (0.20.3)\n",
      "Requirement already satisfied: kornia[x] in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (0.7.3)\n",
      "Requirement already satisfied: torch in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from torchviz) (2.3.1)\n",
      "Requirement already satisfied: kornia-rs>=0.1.0 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from kornia[x]) (0.1.5)\n",
      "Requirement already satisfied: packaging in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from kornia[x]) (24.1)\n",
      "Requirement already satisfied: accelerate in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from kornia[x]) (0.33.0)\n",
      "Requirement already satisfied: onnxruntime-gpu>=1.16 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from kornia[x]) (1.18.1)\n",
      "Requirement already satisfied: coloredlogs in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from onnxruntime-gpu>=1.16->kornia[x]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from onnxruntime-gpu>=1.16->kornia[x]) (24.3.25)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21.6 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from onnxruntime-gpu>=1.16->kornia[x]) (1.26.4)\n",
      "Requirement already satisfied: protobuf in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from onnxruntime-gpu>=1.16->kornia[x]) (5.27.2)\n",
      "Requirement already satisfied: sympy in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from onnxruntime-gpu>=1.16->kornia[x]) (1.13.0)\n",
      "Requirement already satisfied: filelock in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from torch->torchviz) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from torch->torchviz) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from torch->torchviz) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from torch->torchviz) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from torch->torchviz) (2024.6.1)\n",
      "Requirement already satisfied: psutil in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from accelerate->kornia[x]) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from accelerate->kornia[x]) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from accelerate->kornia[x]) (0.24.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from accelerate->kornia[x]) (0.4.3)\n",
      "Requirement already satisfied: requests in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate->kornia[x]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate->kornia[x]) (4.66.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from coloredlogs->onnxruntime-gpu>=1.16->kornia[x]) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from jinja2->torch->torchviz) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from sympy->onnxruntime-gpu>=1.16->kornia[x]) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate->kornia[x]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate->kornia[x]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate->kornia[x]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pdi_4/anaconda3/envs/Losses/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate->kornia[x]) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary torchviz torchinfo kornia[x] graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7.3\n"
     ]
    }
   ],
   "source": [
    "import kornia\n",
    "print(kornia.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torchviz as tv\n",
    "import torchinfo as tinf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/Losses/lib/python3.12/site-packages/torchsummary/torchsummary.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "summary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtinf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'nn.Module'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'INPUT_SIZE_TYPE | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'INPUT_DATA_TYPE | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcol_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Iterable[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcol_width\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdepth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'torch.device | str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtypes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'list[torch.dtype] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrow_settings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Iterable[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'ModelStatistics'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Summarize the given PyTorch model. Summarized information includes:\n",
      "    1) Layer names,\n",
      "    2) input/output shapes,\n",
      "    3) kernel shape,\n",
      "    4) # of parameters,\n",
      "    5) # of operations (Mult-Adds),\n",
      "    6) whether layer is trainable\n",
      "\n",
      "NOTE: If neither input_data or input_size are provided, no forward pass through the\n",
      "network is performed, and the provided model information is limited to layer names.\n",
      "\n",
      "Args:\n",
      "    model (nn.Module):\n",
      "            PyTorch model to summarize. The model should be fully in either train()\n",
      "            or eval() mode. If layers are not all in the same mode, running summary\n",
      "            may have side effects on batchnorm or dropout statistics. If you\n",
      "            encounter an issue with this, please open a GitHub issue.\n",
      "\n",
      "    input_size (Sequence of Sizes):\n",
      "            Shape of input data as a List/Tuple/torch.Size\n",
      "            (dtypes must match model input, default is FloatTensors).\n",
      "            You should include batch size in the tuple.\n",
      "            Default: None\n",
      "\n",
      "    input_data (Sequence of Tensors):\n",
      "            Arguments for the model's forward pass (dtypes inferred).\n",
      "            If the forward() function takes several parameters, pass in a list of\n",
      "            args or a dict of kwargs (if your forward() function takes in a dict\n",
      "            as its only argument, wrap it in a list).\n",
      "            Default: None\n",
      "\n",
      "    batch_dim (int):\n",
      "            Batch_dimension of input data. If batch_dim is None, assume\n",
      "            input_data / input_size contains the batch dimension, which is used\n",
      "            in all calculations. Else, expand all tensors to contain the batch_dim.\n",
      "            Specifying batch_dim can be an runtime optimization, since if batch_dim\n",
      "            is specified, torchinfo uses a batch size of 1 for the forward pass.\n",
      "            Default: None\n",
      "\n",
      "    cache_forward_pass (bool):\n",
      "            If True, cache the run of the forward() function using the model\n",
      "            class name as the key. If the forward pass is an expensive operation,\n",
      "            this can make it easier to modify the formatting of your model\n",
      "            summary, e.g. changing the depth or enabled column types, especially\n",
      "            in Jupyter Notebooks.\n",
      "            WARNING: Modifying the model architecture or input data/input size when\n",
      "            this feature is enabled does not invalidate the cache or re-run the\n",
      "            forward pass, and can cause incorrect summaries as a result.\n",
      "            Default: False\n",
      "\n",
      "    col_names (Iterable[str]):\n",
      "            Specify which columns to show in the output. Currently supported: (\n",
      "                \"input_size\",\n",
      "                \"output_size\",\n",
      "                \"num_params\",\n",
      "                \"params_percent\",\n",
      "                \"kernel_size\",\n",
      "                \"mult_adds\",\n",
      "                \"trainable\",\n",
      "            )\n",
      "            Default: (\"output_size\", \"num_params\")\n",
      "            If input_data / input_size are not provided, only \"num_params\" is used.\n",
      "\n",
      "    col_width (int):\n",
      "            Width of each column.\n",
      "            Default: 25\n",
      "\n",
      "    depth (int):\n",
      "            Depth of nested layers to display (e.g. Sequentials).\n",
      "            Nested layers below this depth will not be displayed in the summary.\n",
      "            Default: 3\n",
      "\n",
      "    device (torch.Device):\n",
      "            Uses this torch device for model and input_data.\n",
      "            If not specified, uses the dtype of input_data if given, or the\n",
      "            parameters of the model. Otherwise, uses the result of\n",
      "            torch.cuda.is_available().\n",
      "            Default: None\n",
      "\n",
      "    dtypes (List[torch.dtype]):\n",
      "            If you use input_size, torchinfo assumes your input uses FloatTensors.\n",
      "            If your model use a different data type, specify that dtype.\n",
      "            For multiple inputs, specify the size of both inputs, and\n",
      "            also specify the types of each parameter here.\n",
      "            Default: None\n",
      "\n",
      "    mode (str)\n",
      "            Either \"train\" or \"eval\", which determines whether we call\n",
      "            model.train() or model.eval() before calling summary().\n",
      "            Default: \"eval\".\n",
      "\n",
      "    row_settings (Iterable[str]):\n",
      "            Specify which features to show in a row. Currently supported: (\n",
      "                \"ascii_only\",\n",
      "                \"depth\",\n",
      "                \"var_names\",\n",
      "            )\n",
      "            Default: (\"depth\",)\n",
      "\n",
      "    verbose (int):\n",
      "            0 (quiet): No output\n",
      "            1 (default): Print model summary\n",
      "            2 (verbose): Show weight and bias layers in full detail\n",
      "            Default: 1\n",
      "            If using a Juypter Notebook or Google Colab, the default is 0.\n",
      "\n",
      "    **kwargs:\n",
      "            Other arguments used in `model.forward` function. Passing *args is no\n",
      "            longer supported.\n",
      "\n",
      "Return:\n",
      "    ModelStatistics object\n",
      "            See torchinfo/model_statistics.py for more information.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/Losses/lib/python3.12/site-packages/torchinfo/torchinfo.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "tinf.summary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archtectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloco de duas convoluções 3x3 seguidas de ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementação da arquitetura U-Net adaptada para geração de imagens.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Encoder\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.down5 = DoubleConv(512, 1024)\n",
    "\n",
    "        # Max-pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.conv_up1 = DoubleConv(1024, 512)\n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv_up2 = DoubleConv(512, 256)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv_up3 = DoubleConv(256, 128)\n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv_up4 = DoubleConv(128, 64)\n",
    "\n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(self.pool(d1))\n",
    "        d3 = self.down3(self.pool(d2))\n",
    "        d4 = self.down4(self.pool(d3))\n",
    "        d5 = self.down5(self.pool(d4))\n",
    "\n",
    "        # Decoder\n",
    "        u1 = self.up1(d5)\n",
    "        u1 = torch.cat((u1, d4), dim=1)\n",
    "        u1 = self.conv_up1(u1)\n",
    "        u2 = self.up2(u1)\n",
    "        u2 = torch.cat((u2, d3), dim=1)\n",
    "        u2 = self.conv_up2(u2)\n",
    "        u3 = self.up3(u2)\n",
    "        u3 = torch.cat((u3, d2), dim=1)\n",
    "        u3 = self.conv_up3(u3)\n",
    "        u4 = self.up4(u3)\n",
    "        u4 = torch.cat((u4, d1), dim=1)\n",
    "        u4 = self.conv_up4(u4)\n",
    "\n",
    "        return self.final_conv(u4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Definindo o dispositivo (GPU se disponível)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Inicializando a U-Net\n",
    "    in_channels = 3  # Por exemplo, RGB\n",
    "    out_channels = 3  # Saída de imagem RGB\n",
    "    model = UNet(in_channels, out_channels).to(device)\n",
    "\n",
    "    # Exemplo de entrada\n",
    "    x = torch.randn(1, in_channels, 256, 256).to(device)  # Batch size 1, 256x256 imagem\n",
    "    output = model(x)\n",
    "\n",
    "    print(x.shape, output.shape)  # Deve ser (1, out_channels, 256, 256)\n",
    "     # Plotando a arquitetura da rede\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "UNet                                     [1, 3, 256, 256]          --\n",
      "├─DoubleConv: 1-1                        [1, 64, 256, 256]         --\n",
      "│    └─Sequential: 2-1                   [1, 64, 256, 256]         --\n",
      "│    │    └─Conv2d: 3-1                  [1, 64, 256, 256]         1,792\n",
      "│    │    └─BatchNorm2d: 3-2             [1, 64, 256, 256]         128\n",
      "│    │    └─ReLU: 3-3                    [1, 64, 256, 256]         --\n",
      "│    │    └─Conv2d: 3-4                  [1, 64, 256, 256]         36,928\n",
      "│    │    └─BatchNorm2d: 3-5             [1, 64, 256, 256]         128\n",
      "│    │    └─ReLU: 3-6                    [1, 64, 256, 256]         --\n",
      "├─MaxPool2d: 1-2                         [1, 64, 128, 128]         --\n",
      "├─DoubleConv: 1-3                        [1, 128, 128, 128]        --\n",
      "│    └─Sequential: 2-2                   [1, 128, 128, 128]        --\n",
      "│    │    └─Conv2d: 3-7                  [1, 128, 128, 128]        73,856\n",
      "│    │    └─BatchNorm2d: 3-8             [1, 128, 128, 128]        256\n",
      "│    │    └─ReLU: 3-9                    [1, 128, 128, 128]        --\n",
      "│    │    └─Conv2d: 3-10                 [1, 128, 128, 128]        147,584\n",
      "│    │    └─BatchNorm2d: 3-11            [1, 128, 128, 128]        256\n",
      "│    │    └─ReLU: 3-12                   [1, 128, 128, 128]        --\n",
      "├─MaxPool2d: 1-4                         [1, 128, 64, 64]          --\n",
      "├─DoubleConv: 1-5                        [1, 256, 64, 64]          --\n",
      "│    └─Sequential: 2-3                   [1, 256, 64, 64]          --\n",
      "│    │    └─Conv2d: 3-13                 [1, 256, 64, 64]          295,168\n",
      "│    │    └─BatchNorm2d: 3-14            [1, 256, 64, 64]          512\n",
      "│    │    └─ReLU: 3-15                   [1, 256, 64, 64]          --\n",
      "│    │    └─Conv2d: 3-16                 [1, 256, 64, 64]          590,080\n",
      "│    │    └─BatchNorm2d: 3-17            [1, 256, 64, 64]          512\n",
      "│    │    └─ReLU: 3-18                   [1, 256, 64, 64]          --\n",
      "├─MaxPool2d: 1-6                         [1, 256, 32, 32]          --\n",
      "├─DoubleConv: 1-7                        [1, 512, 32, 32]          --\n",
      "│    └─Sequential: 2-4                   [1, 512, 32, 32]          --\n",
      "│    │    └─Conv2d: 3-19                 [1, 512, 32, 32]          1,180,160\n",
      "│    │    └─BatchNorm2d: 3-20            [1, 512, 32, 32]          1,024\n",
      "│    │    └─ReLU: 3-21                   [1, 512, 32, 32]          --\n",
      "│    │    └─Conv2d: 3-22                 [1, 512, 32, 32]          2,359,808\n",
      "│    │    └─BatchNorm2d: 3-23            [1, 512, 32, 32]          1,024\n",
      "│    │    └─ReLU: 3-24                   [1, 512, 32, 32]          --\n",
      "├─MaxPool2d: 1-8                         [1, 512, 16, 16]          --\n",
      "├─DoubleConv: 1-9                        [1, 1024, 16, 16]         --\n",
      "│    └─Sequential: 2-5                   [1, 1024, 16, 16]         --\n",
      "│    │    └─Conv2d: 3-25                 [1, 1024, 16, 16]         4,719,616\n",
      "│    │    └─BatchNorm2d: 3-26            [1, 1024, 16, 16]         2,048\n",
      "│    │    └─ReLU: 3-27                   [1, 1024, 16, 16]         --\n",
      "│    │    └─Conv2d: 3-28                 [1, 1024, 16, 16]         9,438,208\n",
      "│    │    └─BatchNorm2d: 3-29            [1, 1024, 16, 16]         2,048\n",
      "│    │    └─ReLU: 3-30                   [1, 1024, 16, 16]         --\n",
      "├─ConvTranspose2d: 1-10                  [1, 512, 32, 32]          2,097,664\n",
      "├─DoubleConv: 1-11                       [1, 512, 32, 32]          --\n",
      "│    └─Sequential: 2-6                   [1, 512, 32, 32]          --\n",
      "│    │    └─Conv2d: 3-31                 [1, 512, 32, 32]          4,719,104\n",
      "│    │    └─BatchNorm2d: 3-32            [1, 512, 32, 32]          1,024\n",
      "│    │    └─ReLU: 3-33                   [1, 512, 32, 32]          --\n",
      "│    │    └─Conv2d: 3-34                 [1, 512, 32, 32]          2,359,808\n",
      "│    │    └─BatchNorm2d: 3-35            [1, 512, 32, 32]          1,024\n",
      "│    │    └─ReLU: 3-36                   [1, 512, 32, 32]          --\n",
      "├─ConvTranspose2d: 1-12                  [1, 256, 64, 64]          524,544\n",
      "├─DoubleConv: 1-13                       [1, 256, 64, 64]          --\n",
      "│    └─Sequential: 2-7                   [1, 256, 64, 64]          --\n",
      "│    │    └─Conv2d: 3-37                 [1, 256, 64, 64]          1,179,904\n",
      "│    │    └─BatchNorm2d: 3-38            [1, 256, 64, 64]          512\n",
      "│    │    └─ReLU: 3-39                   [1, 256, 64, 64]          --\n",
      "│    │    └─Conv2d: 3-40                 [1, 256, 64, 64]          590,080\n",
      "│    │    └─BatchNorm2d: 3-41            [1, 256, 64, 64]          512\n",
      "│    │    └─ReLU: 3-42                   [1, 256, 64, 64]          --\n",
      "├─ConvTranspose2d: 1-14                  [1, 128, 128, 128]        131,200\n",
      "├─DoubleConv: 1-15                       [1, 128, 128, 128]        --\n",
      "│    └─Sequential: 2-8                   [1, 128, 128, 128]        --\n",
      "│    │    └─Conv2d: 3-43                 [1, 128, 128, 128]        295,040\n",
      "│    │    └─BatchNorm2d: 3-44            [1, 128, 128, 128]        256\n",
      "│    │    └─ReLU: 3-45                   [1, 128, 128, 128]        --\n",
      "│    │    └─Conv2d: 3-46                 [1, 128, 128, 128]        147,584\n",
      "│    │    └─BatchNorm2d: 3-47            [1, 128, 128, 128]        256\n",
      "│    │    └─ReLU: 3-48                   [1, 128, 128, 128]        --\n",
      "├─ConvTranspose2d: 1-16                  [1, 64, 256, 256]         32,832\n",
      "├─DoubleConv: 1-17                       [1, 64, 256, 256]         --\n",
      "│    └─Sequential: 2-9                   [1, 64, 256, 256]         --\n",
      "│    │    └─Conv2d: 3-49                 [1, 64, 256, 256]         73,792\n",
      "│    │    └─BatchNorm2d: 3-50            [1, 64, 256, 256]         128\n",
      "│    │    └─ReLU: 3-51                   [1, 64, 256, 256]         --\n",
      "│    │    └─Conv2d: 3-52                 [1, 64, 256, 256]         36,928\n",
      "│    │    └─BatchNorm2d: 3-53            [1, 64, 256, 256]         128\n",
      "│    │    └─ReLU: 3-54                   [1, 64, 256, 256]         --\n",
      "├─Conv2d: 1-18                           [1, 3, 256, 256]          195\n",
      "==========================================================================================\n",
      "Total params: 31,043,651\n",
      "Trainable params: 31,043,651\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 54.66\n",
      "==========================================================================================\n",
      "Input size (MB): 0.79\n",
      "Forward/backward pass size (MB): 576.19\n",
      "Params size (MB): 124.17\n",
      "Estimated Total Size (MB): 701.15\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_stats = tinf.summary(model, (1, 3, 256, 256), verbose=0)\n",
    "summary_str = str(model_stats)\n",
    "# summary_str contains the string representation of the summary!\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitetura da U-Net plotada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        y = model(torch.randn(1, in_channels, 256, 256).to(device))\n",
    "        tv.make_dot(y, params=dict(list(model.named_parameters()))).render(\"unet_architecture\", format=\"png\")\n",
    "        \n",
    "        #plot.format = 'png'\n",
    "        \n",
    "        print(\"Arquitetura da U-Net plotada com sucesso.\")\n",
    "except ImportError:\n",
    "        print(\"Para plotar a arquitetura, instale o pacote torchviz.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "summary() got an unexpected keyword argument 'depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbranching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: summary() got an unexpected keyword argument 'depth'"
     ]
    }
   ],
   "source": [
    "summary(model, x,depth=1, dtypes=[torch.long],branching=False,verbose=1, col_width=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/graphviz/backend/execute.py:78\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m         proc \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/subprocess.py:1955\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: PosixPath('dot')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 133\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     y \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, in_channels, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mtv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munet_architecture_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArquitetura da U-Net plotada com sucesso.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/graphviz/rendering.py:122\u001b[0m, in \u001b[0;36mRender.render\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[1;32m    118\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(filename, directory\u001b[38;5;241m=\u001b[39mdirectory, skip_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    120\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(filepath)\n\u001b[0;32m--> 122\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    125\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelete \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, filepath)\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/graphviz/backend/rendering.py:326\u001b[0m, in \u001b[0;36mrender\u001b[0;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[1;32m    322\u001b[0m cmd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork around pytype false alarm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 326\u001b[0m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mfspath(outfile)\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/graphviz/backend/execute.py:81\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloco de duas convoluções 3x3 seguidas de ReLU.\n",
    "    Permite a parametrização do uso de BatchNorm.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_batch_norm=True):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        layers.extend([\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ])\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        self.double_conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementação da arquitetura U-Net parametrizável para geração de imagens.\n",
    "    Permite a parametrização do número de blocos de convolução e o uso de BatchNorm.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, base_filters=64, num_layers=5, use_batch_norm=True):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.base_filters = base_filters\n",
    "        self.num_layers = num_layers\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        filters = base_filters\n",
    "        \n",
    "        for i in range(num_layers-1):\n",
    "            self.encoder_layers.append(DoubleConv(in_channels, filters, use_batch_norm))\n",
    "            in_channels = filters\n",
    "            filters *= 2\n",
    "        \n",
    "        # Max-pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(filters // 2, filters, use_batch_norm)\n",
    "\n",
    "        # Decoder\n",
    "        self.up_layers = nn.ModuleList()\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        for i in range(num_layers-1, 0, -1):#range(num_layers-1, 0, -1)\n",
    "            filters //= 2\n",
    "            self.up_layers.append(nn.ConvTranspose2d(filters * 2, filters, kernel_size=2, stride=2))\n",
    "            self.up_convs.append(DoubleConv(filters * 2, filters , use_batch_norm))\n",
    "            \n",
    "\n",
    "        # Final convolution\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(base_filters, out_channels, kernel_size=1)\n",
    "    def see_model(self):\n",
    "        print(f\"\"\"\n",
    "        encoder: {self.encoder_layers}\n",
    "        pool: {self.pool}\n",
    "        botleneck: {self.bottleneck}\n",
    "        up laeyers: {self.up_layers}\n",
    "        up_convs: {self.up_convs}\n",
    "        final conv: {self.final_conv}\"\"\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        encoder_results = []\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "            encoder_results.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Decoder\n",
    "        for i, (up, conv) in enumerate(zip(self.up_layers, self.up_convs)):\n",
    "            x = up(x)\n",
    "            x = torch.cat((x, encoder_results[-(i+1)]), dim=1)\n",
    "            x = conv(x)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "def standardize_image(image):\n",
    "    \"\"\"\n",
    "    Função para padronizar as imagens de entrada.\n",
    "    \"\"\"\n",
    "    mean = image.mean()\n",
    "    std = image.std()\n",
    "    return (image - mean) / std\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Definindo o dispositivo (GPU se disponível)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Inicializando a U-Net parametrizável\n",
    "    in_channels = 3  # Por exemplo, RGB\n",
    "    out_channels = 3  # Saída de imagem RGB\n",
    "    base_filters = 64\n",
    "    num_layers = 4\n",
    "    use_batch_norm = True\n",
    "    model = UNet(in_channels, out_channels, base_filters, num_layers, use_batch_norm).to(device)\n",
    "    #model.see_model()\n",
    "\n",
    "    # Exibindo a arquitetura da rede\n",
    "    #summary(model, input_size=(in_channels, 256, 256))\n",
    "\n",
    "    # Exemplo de entrada\n",
    "    x = torch.randn(1, in_channels, 256, 256).to(device)  # Batch size 1, 256x256 imagem\n",
    "    # x = standardize_image(x)\n",
    "    output = model(x)\n",
    "\n",
    "    print(output.shape)  # Deve ser (1, out_channels, 256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotando a arquitetura da rede\n",
    "try:\n",
    "\n",
    "    y = model(torch.randn(1, in_channels, 256, 256).to(device))\n",
    "    tv.make_dot(y, params=dict(list(model.named_parameters()))).render(\"unet_architecture_2\", format=\"png\")\n",
    "    print(\"Arquitetura da U-Net plotada com sucesso.\")\n",
    "except ImportError:\n",
    "    print(\"Para plotar a arquitetura, instale o pacote torchviz.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Layer (type:depth-idx)                   Output Shape     Param #\n",
      "========================================================================\n",
      "DoubleConv: 2-1                          [-1, 64, 256, 256] --\n",
      "MaxPool2d: 1-1                           [-1, 64, 128, 128] --\n",
      "DoubleConv: 2-2                          [-1, 128, 128, 128] --\n",
      "MaxPool2d: 1-2                           [-1, 128, 64, 64] --\n",
      "DoubleConv: 2-3                          [-1, 256, 64, 64] --\n",
      "MaxPool2d: 1-3                           [-1, 256, 32, 32] --\n",
      "DoubleConv: 1-4                          [-1, 512, 32, 32] --\n",
      "Sequential: 2-4                          [-1, 512, 32, 32] --\n",
      "ConvTranspose2d: 2-5                     [-1, 256, 64, 64] 524,544\n",
      "DoubleConv: 2-6                          [-1, 256, 64, 64] --\n",
      "ConvTranspose2d: 2-7                     [-1, 128, 128, 128] 131,200\n",
      "DoubleConv: 2-8                          [-1, 128, 128, 128] --\n",
      "ConvTranspose2d: 2-9                     [-1, 64, 256, 256] 32,832\n",
      "DoubleConv: 2-10                         [-1, 64, 256, 256] --\n",
      "Conv2d: 1-5                              [-1, 3, 256, 256] 195\n",
      "========================================================================\n",
      "Total params: 3,542,211\n",
      "Trainable params: 3,542,211\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 6.47\n",
      "========================================================================\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 57.50\n",
      "Params size (MB): 13.51\n",
      "Estimated Total Size (MB): 71.76\n",
      "========================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================\n",
       "Layer (type:depth-idx)                   Output Shape     Param #\n",
       "========================================================================\n",
       "DoubleConv: 2-1                          [-1, 64, 256, 256] --\n",
       "MaxPool2d: 1-1                           [-1, 64, 128, 128] --\n",
       "DoubleConv: 2-2                          [-1, 128, 128, 128] --\n",
       "MaxPool2d: 1-2                           [-1, 128, 64, 64] --\n",
       "DoubleConv: 2-3                          [-1, 256, 64, 64] --\n",
       "MaxPool2d: 1-3                           [-1, 256, 32, 32] --\n",
       "DoubleConv: 1-4                          [-1, 512, 32, 32] --\n",
       "Sequential: 2-4                          [-1, 512, 32, 32] --\n",
       "ConvTranspose2d: 2-5                     [-1, 256, 64, 64] 524,544\n",
       "DoubleConv: 2-6                          [-1, 256, 64, 64] --\n",
       "ConvTranspose2d: 2-7                     [-1, 128, 128, 128] 131,200\n",
       "DoubleConv: 2-8                          [-1, 128, 128, 128] --\n",
       "ConvTranspose2d: 2-9                     [-1, 64, 256, 256] 32,832\n",
       "DoubleConv: 2-10                         [-1, 64, 256, 256] --\n",
       "Conv2d: 1-5                              [-1, 3, 256, 256] 195\n",
       "========================================================================\n",
       "Total params: 3,542,211\n",
       "Trainable params: 3,542,211\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.47\n",
       "========================================================================\n",
       "Input size (MB): 0.75\n",
       "Forward/backward pass size (MB): 57.50\n",
       "Params size (MB): 13.51\n",
       "Estimated Total Size (MB): 71.76\n",
       "========================================================================"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, x,depth=1, dtypes=[torch.long],branching=False,verbose=1, col_width=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpa memoria CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória alocada: 0 bytes\n",
      "Memória reservada: 0 bytes\n",
      "Memória alocada: 0 bytes\n",
      "Memória reservada: 0 bytes\n",
      "Memoria limpa\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "# Force a coleta de lixo para liberar a memória imediatamente\n",
    "gc.collect()\n",
    "\n",
    "# Libere o cache de memória do PyTorch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "print(f\"Memoria limpa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CycleGAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CycleGAN.ipynb\tdocs\t\t LICENSE  pix2pix.ipynb     scripts   util\n",
      "data\t\tenvironment.yml  models   README.md\t    test.py\n",
      "datasets\timgs\t\t options  requirements.txt  train.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.isdir(\"pytorch-CycleGAN-and-pix2pix\"):\n",
    "    !git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git\n",
    "os.chdir(\"pytorch-CycleGAN-and-pix2pix\")\n",
    "!pip install -r requirements.txt \n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Falta aprender a resgatar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\t    pytorch-CycleGAN-and-pix2pix  unet_architecture_2.png\n",
      "Losses.yml  README.md\t\t\t  unet_architecture.png\n",
      "main.py     src\t\t\t\t  vit_architecture.png\n",
      "metrics     unet_architecture\t\t  work.ipynb\n",
      "models\t    unet_architecture_2\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"..\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Image Shape: torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Encoder parametrizável com BatchNorm\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dims, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Inicializa uma lista para armazenar as camadas da rede\n",
    "        modules = []\n",
    "        \n",
    "        # Adiciona camadas de convolução e batch normalization conforme especificado em hidden_dims\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(input_channels, h_dim, kernel_size=4, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "            input_channels = h_dim\n",
    "        \n",
    "        # Define as camadas finais para calcular mu e logvar\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1] * 16 * 16, latent_dim)  # Supondo entrada de 256x256\n",
    "        self.fc_logvar = nn.Linear(hidden_dims[-1] * 16 * 16, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "# Decoder parametrizável com BatchNorm\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dims, output_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Inicializa uma lista para armazenar as camadas da rede\n",
    "        modules = []\n",
    "        \n",
    "        # Primeira camada completamente conectada para expandir a representação latente\n",
    "        self.fc = nn.Linear(latent_dim, hidden_dims[-1] * 16 * 16)\n",
    "        \n",
    "        # Adiciona camadas de convolução transposta e batch normalization conforme especificado em hidden_dims\n",
    "        hidden_dims.reverse()\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=4,\n",
    "                                       stride=2,\n",
    "                                       padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Camada final para reconstrução da imagem\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                   output_channels,\n",
    "                                   kernel_size=4,\n",
    "                                   stride=2,\n",
    "                                   padding=1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = torch.relu(self.fc(z))\n",
    "        x = x.view(x.size(0), -1, 16, 16)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Definição do VAE parametrizável\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dims, latent_dim, output_channels):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_channels, hidden_dims, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dims, output_channels)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "# Configuração de parâmetros\n",
    "latent_dim = 16\n",
    "hidden_dims = [32, 64, 128, 256]\n",
    "input_channels = 3\n",
    "output_channels = 3\n",
    "\n",
    "# Inicialização do modelo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE(input_channels=input_channels, hidden_dims=hidden_dims, latent_dim=latent_dim, output_channels=output_channels).to(device)\n",
    "\n",
    "# Teste com tensor randômico\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Gera um tensor randômico com formato (1, 3, 256, 256)\n",
    "    random_tensor = torch.randn(1, 3, 256, 256).to(device)\n",
    "    \n",
    "    # Passa o tensor randômico pelo encoder para obter mu e logvar\n",
    "    mu, logvar = model.encoder(random_tensor)\n",
    "    \n",
    "    # Reamostragem para obter o vetor latente z\n",
    "    z = model.reparameterize(mu, logvar)\n",
    "    \n",
    "    # Passa o vetor latente pelo decoder para gerar a imagem reconstruída\n",
    "    generated_image, _, _ = model(random_tensor)\n",
    "    \n",
    "    print(\"Generated Image Shape:\", generated_image.shape)  # Esperado: (1, 3, 256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "VAE                                           [1, 3, 256, 256]          --\n",
       "├─Encoder: 1-1                                [1, 16]                   --\n",
       "│    └─Sequential: 2-1                        [1, 256, 16, 16]          --\n",
       "│    │    └─Sequential: 3-1                   [1, 32, 128, 128]         --\n",
       "│    │    │    └─Conv2d: 4-1                  [1, 32, 128, 128]         1,568\n",
       "│    │    │    └─BatchNorm2d: 4-2             [1, 32, 128, 128]         64\n",
       "│    │    │    └─ReLU: 4-3                    [1, 32, 128, 128]         --\n",
       "│    │    └─Sequential: 3-2                   [1, 64, 64, 64]           --\n",
       "│    │    │    └─Conv2d: 4-4                  [1, 64, 64, 64]           32,832\n",
       "│    │    │    └─BatchNorm2d: 4-5             [1, 64, 64, 64]           128\n",
       "│    │    │    └─ReLU: 4-6                    [1, 64, 64, 64]           --\n",
       "│    │    └─Sequential: 3-3                   [1, 128, 32, 32]          --\n",
       "│    │    │    └─Conv2d: 4-7                  [1, 128, 32, 32]          131,200\n",
       "│    │    │    └─BatchNorm2d: 4-8             [1, 128, 32, 32]          256\n",
       "│    │    │    └─ReLU: 4-9                    [1, 128, 32, 32]          --\n",
       "│    │    └─Sequential: 3-4                   [1, 256, 16, 16]          --\n",
       "│    │    │    └─Conv2d: 4-10                 [1, 256, 16, 16]          524,544\n",
       "│    │    │    └─BatchNorm2d: 4-11            [1, 256, 16, 16]          512\n",
       "│    │    │    └─ReLU: 4-12                   [1, 256, 16, 16]          --\n",
       "│    └─Linear: 2-2                            [1, 16]                   1,048,592\n",
       "│    └─Linear: 2-3                            [1, 16]                   1,048,592\n",
       "├─Decoder: 1-2                                [1, 3, 256, 256]          --\n",
       "│    └─Linear: 2-4                            [1, 65536]                1,114,112\n",
       "│    └─Sequential: 2-5                        [1, 3, 256, 256]          --\n",
       "│    │    └─Sequential: 3-5                   [1, 128, 32, 32]          --\n",
       "│    │    │    └─ConvTranspose2d: 4-13        [1, 128, 32, 32]          524,416\n",
       "│    │    │    └─BatchNorm2d: 4-14            [1, 128, 32, 32]          256\n",
       "│    │    │    └─ReLU: 4-15                   [1, 128, 32, 32]          --\n",
       "│    │    └─Sequential: 3-6                   [1, 64, 64, 64]           --\n",
       "│    │    │    └─ConvTranspose2d: 4-16        [1, 64, 64, 64]           131,136\n",
       "│    │    │    └─BatchNorm2d: 4-17            [1, 64, 64, 64]           128\n",
       "│    │    │    └─ReLU: 4-18                   [1, 64, 64, 64]           --\n",
       "│    │    └─Sequential: 3-7                   [1, 32, 128, 128]         --\n",
       "│    │    │    └─ConvTranspose2d: 4-19        [1, 32, 128, 128]         32,800\n",
       "│    │    │    └─BatchNorm2d: 4-20            [1, 32, 128, 128]         64\n",
       "│    │    │    └─ReLU: 4-21                   [1, 32, 128, 128]         --\n",
       "│    │    └─Sequential: 3-8                   [1, 3, 256, 256]          --\n",
       "│    │    │    └─ConvTranspose2d: 4-22        [1, 3, 256, 256]          1,539\n",
       "│    │    │    └─Sigmoid: 4-23                [1, 3, 256, 256]          --\n",
       "===============================================================================================\n",
       "Total params: 4,592,739\n",
       "Trainable params: 4,592,739\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.14\n",
       "===============================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 32.51\n",
       "Params size (MB): 18.37\n",
       "Estimated Total Size (MB): 51.66\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinf.summary(model, (1,3,256,256),depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = x + self.dropout(mlp_output)\n",
    "        x = self.norm2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEnhancerTransformer(nn.Module):\n",
    "    def __init__(self, image_size=256, patch_size=16, num_channels=3, embed_dim=512, num_heads=8, mlp_dim=1024, num_layers=6, dropout=0.1):\n",
    "        super(ImageEnhancerTransformer, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_dim = num_channels * patch_size * patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embeddings = nn.Linear(self.patch_dim, embed_dim)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, self.patch_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self._to_patches(x)\n",
    "        x = self.patch_embeddings(x) + self.position_embeddings\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.fc(x)\n",
    "        x = self._from_patches(x, batch_size)\n",
    "        return x\n",
    "\n",
    "    def _to_patches(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        x = x.view(batch_size, channels, self.image_size // self.patch_size, self.patch_size,\n",
    "                   self.image_size // self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1).contiguous()\n",
    "        x = x.view(batch_size, self.num_patches, -1)\n",
    "        return x\n",
    "\n",
    "    def _from_patches(self, x, batch_size):\n",
    "        x = x.view(batch_size, self.image_size // self.patch_size, self.image_size // self.patch_size,\n",
    "                   self.patch_size, self.patch_size, -1)\n",
    "        x = x.permute(0, 5, 1, 3, 2, 4).contiguous()\n",
    "        x = x.view(batch_size, -1, self.image_size, self.image_size)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma do tensor de entrada: torch.Size([1, 3, 256, 256])\n",
      "Forma do tensor de saída: torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ImageEnhancerTransformer                 [1, 3, 256, 256]          131,072\n",
       "├─Linear: 1-1                            [1, 256, 512]             393,728\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─TransformerBlock: 2-1             [1, 256, 512]             --\n",
       "│    │    └─MultiheadAttention: 3-1      [1, 256, 512]             1,050,624\n",
       "│    │    └─Dropout: 3-2                 [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-3               [1, 256, 512]             1,024\n",
       "│    │    └─Sequential: 3-4              [1, 256, 512]             1,050,112\n",
       "│    │    └─Dropout: 3-5                 [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-6               [1, 256, 512]             1,024\n",
       "│    └─TransformerBlock: 2-2             [1, 256, 512]             --\n",
       "│    │    └─MultiheadAttention: 3-7      [1, 256, 512]             1,050,624\n",
       "│    │    └─Dropout: 3-8                 [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-9               [1, 256, 512]             1,024\n",
       "│    │    └─Sequential: 3-10             [1, 256, 512]             1,050,112\n",
       "│    │    └─Dropout: 3-11                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-12              [1, 256, 512]             1,024\n",
       "│    └─TransformerBlock: 2-3             [1, 256, 512]             --\n",
       "│    │    └─MultiheadAttention: 3-13     [1, 256, 512]             1,050,624\n",
       "│    │    └─Dropout: 3-14                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-15              [1, 256, 512]             1,024\n",
       "│    │    └─Sequential: 3-16             [1, 256, 512]             1,050,112\n",
       "│    │    └─Dropout: 3-17                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-18              [1, 256, 512]             1,024\n",
       "│    └─TransformerBlock: 2-4             [1, 256, 512]             --\n",
       "│    │    └─MultiheadAttention: 3-19     [1, 256, 512]             1,050,624\n",
       "│    │    └─Dropout: 3-20                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-21              [1, 256, 512]             1,024\n",
       "│    │    └─Sequential: 3-22             [1, 256, 512]             1,050,112\n",
       "│    │    └─Dropout: 3-23                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-24              [1, 256, 512]             1,024\n",
       "│    └─TransformerBlock: 2-5             [1, 256, 512]             --\n",
       "│    │    └─MultiheadAttention: 3-25     [1, 256, 512]             1,050,624\n",
       "│    │    └─Dropout: 3-26                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-27              [1, 256, 512]             1,024\n",
       "│    │    └─Sequential: 3-28             [1, 256, 512]             1,050,112\n",
       "│    │    └─Dropout: 3-29                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-30              [1, 256, 512]             1,024\n",
       "│    └─TransformerBlock: 2-6             [1, 256, 512]             --\n",
       "│    │    └─MultiheadAttention: 3-31     [1, 256, 512]             1,050,624\n",
       "│    │    └─Dropout: 3-32                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-33              [1, 256, 512]             1,024\n",
       "│    │    └─Sequential: 3-34             [1, 256, 512]             1,050,112\n",
       "│    │    └─Dropout: 3-35                [1, 256, 512]             --\n",
       "│    │    └─LayerNorm: 3-36              [1, 256, 512]             1,024\n",
       "├─LayerNorm: 1-3                         [1, 256, 512]             1,024\n",
       "├─Linear: 1-4                            [1, 256, 768]             393,984\n",
       "==========================================================================================\n",
       "Total params: 13,536,512\n",
       "Trainable params: 13,536,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 7.10\n",
       "==========================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 35.13\n",
       "Params size (MB): 28.41\n",
       "Estimated Total Size (MB): 64.32\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100  # número de exemplos fictício\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Gera uma imagem aleatória como entrada e uma versão \"melhorada\" como alvo\n",
    "        input_image = Image.fromarray((np.random.rand(256, 256, 3) * 255).astype(np.uint8))\n",
    "        target_image = Image.fromarray((np.random.rand(256, 256, 3) * 255).astype(np.uint8))\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            target_image = self.transform(target_image)\n",
    "\n",
    "        return input_image, target_image\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "dataset = DummyDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "model = ImageEnhancerTransformer()\n",
    "# Gera um tensor aleatório como entrada\n",
    "input_tensor = torch.rand(1, 3, 256, 256)  # Batch size de 1, 3 canais, 256x256 de resolução\n",
    "\n",
    "# Passa o tensor pelo modelo\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "# Mostra as formas dos tensores de entrada e saída\n",
    "print(\"Forma do tensor de entrada:\", input_tensor.shape)\n",
    "print(\"Forma do tensor de saída:\", output_tensor.shape)\n",
    "\n",
    "tinf.summary(model, (1,3, 256, 256),depth=3)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # Loop de treinamento\n",
    "# for epoch in range(10):  # número de épocas fictício\n",
    "#     for inputs, targets in dataloader:\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitetura da U-Net plotada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Plotando a arquitetura da rede\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "\n",
    "    y = model(torch.randn(1, 3, 256, 256).to(device))\n",
    "    tv.make_dot(y, params=dict(list(model.named_parameters()))).render(\"vit_architecture\", format=\"png\")\n",
    "    print(\"Arquitetura da U-Net plotada com sucesso.\")\n",
    "except ImportError:\n",
    "    print(\"Para plotar a arquitetura, instale o pacote torchviz.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória alocada: 0 bytes\n",
      "Memória reservada: 0 bytes\n",
      "Memória alocada: 0 bytes\n",
      "Memória reservada: 0 bytes\n",
      "Memoria limpa\n"
     ]
    }
   ],
   "source": [
    "### Limpa memoria CUDA\n",
    "import gc\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "# Force a coleta de lixo para liberar a memória imediatamente\n",
    "gc.collect()\n",
    "\n",
    "# Libere o cache de memória do PyTorch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "print(f\"Memoria limpa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, use_lsgan=True):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.use_lsgan = use_lsgan\n",
    "        if use_lsgan:\n",
    "            self.criterion = nn.MSELoss()\n",
    "        else:\n",
    "            self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, output, target_is_real):\n",
    "        target = torch.ones_like(output) if target_is_real else torch.zeros_like(output)\n",
    "        loss = self.criterion(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color loss\t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorLoss, self).__init__()\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.criterion(input, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SSIM PSNR MS-SSIM\t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49616897106170654 -7.792870998382568 66.98175811767578 0.07644809782505035\n"
     ]
    }
   ],
   "source": [
    "from kornia.losses import ssim_loss as ssim, psnr_loss as psnr, MS_SSIMLoss as ms_ssim, charbonnier_loss as charbonnier\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "input1 = torch.rand(1, 3, 255, 255)\n",
    "input2 = torch.rand(1, 3, 255, 255)\n",
    "# print(np.max(input1.numpy()), np.max(input2.numpy()))\n",
    "\n",
    "loss = ssim(input1, input2, 11)\n",
    "\n",
    "loss2 = psnr(input1,input2, np.max(input1.numpy())) # 10 * log(4/((1.2-1)**2)) / log(10)\n",
    "loss3 = ms_ssim()\n",
    "\n",
    "loss4 = charbonnier(input1, input2,reduction='mean')\n",
    "\n",
    "# class SSIMLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SSIMLoss, self).__init__()\n",
    "\n",
    "#     def forward(self, img1, img2):\n",
    "#         return ssim(img1, img2)\n",
    "# class PSNRLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(PSNRLoss, self).__init__()\n",
    "\n",
    "#     def forward(self, img1, img2):\n",
    "#         return (img1, img2)\n",
    "print(loss.item(), loss2.item(), loss3(input1, input2).item(), loss4.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE\t \t\t \t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAELoss, self).__init__()\n",
    "        self.criterion = nn.L1Loss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.criterion(input, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAELoss, self).__init__()\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.criterion(input, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uranker\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUSIQ\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Loss VGG\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acumlative Superiority\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIoU\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGan Loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gan loss\t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kornia'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkornia\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ssim_loss \u001b[38;5;28;01mas\u001b[39;00m ssim, psnr_loss \u001b[38;5;28;01mas\u001b[39;00m psnr, MS_SSIMLoss \u001b[38;5;28;01mas\u001b[39;00m ms_ssim, charbonnier_loss \u001b[38;5;28;01mas\u001b[39;00m charbonnier\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kornia'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Structure Perceptual\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detail loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGan\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angular\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### margin-ranking loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity mapping\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle consistency loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCH loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Losses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
