{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import *\n",
    "import torch\n",
    "from models import *\n",
    "from src.dataload import *\n",
    "from tqdm.notebook import tqdm\n",
    "from metrics.metrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train raw: 712 Train ref: 712 \n",
      "Test raw: 178 Test ref: 178\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#dataloader UIEBUIEB\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_loader_UIEB, test_loader_UIEB \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUIEB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#dataloader TURBID\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_loader_TURBID, test_loader_TURBID \u001b[38;5;241m=\u001b[39m create_dataloader(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTURBID\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documentos/Loss_battle/src/dataload.py:178\u001b[0m, in \u001b[0;36mcreate_dataloader\u001b[0;34m(dataset_name, dataset_path, batch_size, num_workers, ddp, world_size, rank)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m#creating the DataLoaders\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ddp:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m#sampler\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     sampler \u001b[38;5;241m=\u001b[39m \u001b[43mDistributedSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_replicas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset,sampler\u001b[38;5;241m=\u001b[39msampler, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, num_workers\u001b[38;5;241m=\u001b[39mnum_workers)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/losstest/lib/python3.11/site-packages/torch/utils/data/distributed.py:68\u001b[0m, in \u001b[0;36mDistributedSampler.__init__\u001b[0;34m(self, dataset, num_replicas, rank, shuffle, seed, drop_last)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequires distributed package to be available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m     num_replicas \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_world_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[0;32m~/anaconda3/envs/losstest/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:1832\u001b[0m, in \u001b[0;36mget_world_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[1;32m   1830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1832\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_group_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/losstest/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:864\u001b[0m, in \u001b[0;36m_get_group_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get a given group's world size.\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD \u001b[38;5;129;01mor\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 864\u001b[0m     default_pg \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_pg\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m group\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m~/anaconda3/envs/losstest/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:1025\u001b[0m, in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the default process group created by init_process_group.\"\"\"\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault process group has not been initialized, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1027\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease make sure to call init_process_group.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1028\u001b[0m     )\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m not_none(GroupMember\u001b[38;5;241m.\u001b[39mWORLD)\n",
      "\u001b[0;31mValueError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "\n",
    "#dataloader UIEBUIEB\n",
    "train_loader_UIEB, test_loader_UIEB = create_dataloader(dataset_name=\"UIEB\", dataset_path=\"data\")\n",
    "\n",
    "#dataloader TURBID\n",
    "train_loader_TURBID, test_loader_TURBID = create_dataloader(dataset_name=\"TURBID\", dataset_path=\"data\")\n",
    "\n",
    "#dataloader LSUI\n",
    "train_loader_LSUI, test_loader_LSUI = create_dataloader(dataset_name=\"LSUI\", dataset_path=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def competicao():\n",
    "    ##importa dataset\n",
    "\n",
    "    ##importa loss\n",
    "\n",
    "    ##importa modelos\n",
    "\n",
    "    ##importar metricas\n",
    "\n",
    "    ##inicia estrutura com ddp e torchrun\n",
    "\n",
    "    ##treinar por 100 epocas para cada modelo//total 1500 epocas pq sao 15 loss\n",
    "\n",
    "    ##salvar checkpoints\n",
    "\n",
    "    ##roda as metricas\n",
    "\n",
    "    \n",
    "\n",
    "    modelos = [Unet_model(), Vit_model(),VAE_model()]\n",
    "    #modelos = [VAE_model()]\n",
    "\n",
    "    return modelos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 16]) torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.randn(1, 3, 256, 256).to(device)\n",
    "modelos = load_models()\n",
    "for model in modelos:\n",
    "    model = model.to(device)\n",
    "    if type(model(x)) is tuple:\n",
    "        print(model(x)[0].shape, model(x)[1].shape, model(x)[2].shape)\n",
    "    else:\n",
    "        print(model(x).shape)\n",
    "\n",
    "if type(model(x)) is tuple:\n",
    "        print(model(x)[0].shape, model(x)[1].shape, model(x)[2].shape)\n",
    "    else:\n",
    "        print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notas sobre as Loss\n",
    "A loss lch nao esta funcionando.\n",
    "\n",
    "Serao reajustadas para o intervalo de zero a 1 :\n",
    "* Histogram\n",
    "* darkchannel\n",
    "* lch nao funciona\n",
    "* hsv\n",
    "### Notas\n",
    "\n",
    "* Maiores problemas no autoencoder e em seu funcionamento. A dimensao latente de 16 nao gera nada aparentemente. \n",
    "* Talvez alterar o intervalo numero dos canais de cor ajude neste caso. O intervalo entre zero e 1 fazer 1/loss -1\n",
    "* Terei de retreinar para passar os resultados\n",
    "* Para selecionar as melhores loss function devemos utilizar outro metodo. Nao sera possivel utilizar as metricas devido a grande quantidade de funcoes que nao gera imagem como a MSE e a SSIM. Essas funcoes tem de ser investigadas. \n",
    "* Como o tempo esta curso talvez seja melhor selecionar visualmente as melhores funcoes e agrupar de acordo com as necessidades do script.\n",
    "* Ainda preciso revisar os excperimentos do sbr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 loss functions to train with\n",
      "\n",
      "    HistogramColorLoss, result(6022.24755859375)\n",
      "    angular_color_loss, result(0.9995093941688538)\n",
      "    DarkChannelLoss, result(261.767578125)\n",
      "    LCHChannelLoss, result(nan)\n",
      "    HSVChannelLoss, result(8594029568.0)\n"
     ]
    }
   ],
   "source": [
    "#modelos = load_models()\n",
    "import torch\n",
    "from loss import *\n",
    "from models import *\n",
    "from src.dataload import *\n",
    "from tqdm.notebook import tqdm\n",
    "from metrics.metrics import *\n",
    "loss_battle = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#loss_battle.extend(build_perceptual_losses(rank=device))\n",
    "loss_battle.extend(build_channel_losses(rank = device))\n",
    "#loss_battle.extend(build_structural_losses(rank = device))\n",
    "\n",
    "print(f\"{len(loss_battle)} loss functions to train with\\n\")\n",
    "#print(f\"{len(modelos)} models to train with\")\n",
    "x = torch.randn(1, 3, 256, 256).to(device)\n",
    "y = torch.randn(1, 3, 256, 256).to(device)\n",
    "for loss_fn in loss_battle:\n",
    "    print(f\"    {loss_fn.name}, result({loss_fn(x, y)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outos ajustes ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler, TensorDataset\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "    print(f\"Rank {rank} initialized.\")\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def train(rank, world_size, epochs=100):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    #dataloader UIEBUIEB\n",
    "    train_loader_UIEB, test_loader_UIEB,sampler = create_dataloader(dataset_name=\"UIEB\", dataset_path=\"data\",world_size=world_size,rank=rank,rank_test=0)\n",
    "\n",
    "    model = model.cuda(rank)##carrega o modelo\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    criterion = nn.MSELoss().cuda(rank)##carrega loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ddp_model.train()\n",
    "        sampler.set_epoch(epoch)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader_UIEB):\n",
    "            data, target = data.cuda(rank), target.cuda(rank)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = ddp_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Rank {rank}, Epoch [{epoch}/{epochs}], Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item()}\")\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--nodes\", type=int, default=1, help=\"Number of nodes\")\n",
    "    parser.add_argument(\"--gpus\", type=int, default=2, help=\"Number of GPUs per node\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    world_size = args.nodes * args.gpus\n",
    "\n",
    "    torch.multiprocessing.spawn(train, args=(world_size, args.epochs), nprocs=args.gpus, join=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%torchrun --nnodes=2 --nproc_per_node=2 --node_rank=0 --master_addr=\"10.228.252.209\" --master_port=12355 train_ddp.py --nodes=2 --gpus=2 --epochs=10\n",
    "#10.228.247.253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler, TensorDataset\n",
    "from loss import *\n",
    "import torch\n",
    "from models import *\n",
    "from src.dataload import *\n",
    "from tqdm.notebook import tqdm\n",
    "from metrics.metrics import *\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "    print(f\"Rank {rank} initialized.\")\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def train_one_model(rank, world_size, epochs, loss_fn, model_name, model):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    #dataloader UIEBUIEB\n",
    "    train_loader_UIEB, test_loader_UIEB,sampler = create_dataloader(dataset_name=\"UIEB\", dataset_path=\"data\",world_size=world_size,rank=rank,rank_test=0)\n",
    "\n",
    "    modell = model.cuda(rank)\n",
    "    ddp_model = DDP(modell, device_ids=[rank])\n",
    "\n",
    "    criterion = loss_fn().cuda(rank)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ddp_model.train()\n",
    "        sampler.set_epoch(epoch)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader_UIEB):\n",
    "            data, target = data.cuda(rank), target.cuda(rank)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = ddp_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Rank {rank}, Epoch [{epoch}/{epochs}], Batch [{batch_idx}/{len(train_loader_UIEB)}], Loss: {loss.item()}\")\n",
    "    ##Salve Dir para salvar os checkpoints\n",
    "    ckpt_savedir='output/ckpt_battle/'\n",
    "    if not os.path.exists(ckpt_savedir):\n",
    "        os.makedirs(ckpt_savedir)\n",
    "    if rank == 0:\n",
    "        # Salvar o estado do modelo original, não o DDP\n",
    "        torch.save(model.state_dict(), f\"{ckpt_savedir}{model_name}_ckpt.pth\")\n",
    "        psnr_list, ssim_list, uciqe_list, uiqm_list = [], [], [], []\n",
    "        # Avaliar o modelo\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(test_loader_UIEB):\n",
    "                data, target = data.cuda(rank), target.cuda(rank)\n",
    "                #calcula a metrica\n",
    "                targets = target.cpu().numpy()\n",
    "                predictions = model(data.cuda(rank)).cpu().numpy()\n",
    "                psnr_value, ssim_value, uciqe_, uiqm = calculate_metrics(predictions, targets)\n",
    "                psnr_list.append(psnr_value)\n",
    "                ssim_list.append(ssim_value)\n",
    "                uciqe_list.append(uciqe_)\n",
    "                uiqm_list.append(uiqm)\n",
    "        avg_ssim = sum(ssim_list) / len(ssim_list)\n",
    "        avg_psnr = sum(psnr_list) / len(psnr_list)\n",
    "        avg_uciqe = sum(uciqe_list) / len(uciqe_list)\n",
    "        avg_uiqm = sum(uiqm_list) / len(uiqm_list)\n",
    "           \n",
    "        # Salvar métricas em um arquivo\n",
    "        results_savedir='output/results_battle/'\n",
    "        if not os.path.exists(results_savedir):\n",
    "            os.makedirs(results_savedir)\n",
    "        \n",
    "        with open(f'output/{model_name}_metrics.txt', 'w') as f:\n",
    "            f.write(f\"\"\"avg_ssim:{avg_ssim}\\navg_psnr:{avg_psnr}\\navg_uciqe:{avg_uciqe}\\navg_uiqm:{avg_uiqm}\"\"\")\n",
    "            print(f\"Metrics for {model_name} saved to {results_savedir}/{model_name}_metrics.txt\")\n",
    "\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "def train(rank, world_size, epochs):\n",
    "    \n",
    "    modelos = load_models()\n",
    "    loss_battle = []\n",
    "\n",
    "    loss_battle.extend(build_perceptual_losses())\n",
    "    loss_battle.extend(build_channel_losses())\n",
    "    loss_battle.extend(build_structural_losses())\n",
    "\n",
    "    for model in modelos:\n",
    "        for loss_fn in loss_battle:\n",
    "            model_name = model.__class__.__name__ + \"_\" + loss_fn.__class__.__name__\n",
    "            train_one_model(rank, world_size, epochs, loss_fn, model_name, model)        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=100, help=\"Number of epochs per model\")\n",
    "    parser.add_argument(\"--nodes\", type=int, default=1, help=\"Number of nodes\")\n",
    "    parser.add_argument(\"--gpus\", type=int, default=2, help=\"Number of GPUs per node\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    world_size = args.nodes * args.gpus\n",
    "\n",
    "    torch.multiprocessing.spawn(train, args=(world_size, args.epochs), nprocs=args.gpus, join=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rodar no cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchrun --nnodes=2 --nproc_per_node=2 --node_rank=0 --master_addr=\"10.228.252.209\" --master_port=22 main.py --nodes=2 --gpus=2 --epochs=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local\n",
    "torchrun --nproc_per_node=2 main.py --nodes=1 --gpus=2 --epochs=100\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "losstest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
