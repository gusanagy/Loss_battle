{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import *\n",
    "import torch\n",
    "from models import *\n",
    "from src.dataload import *\n",
    "from tqdm.notebook import tqdm\n",
    "from metrics.metrics import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataloader UIEBUIEB\n",
    "train_loader_UIEB, test_loader_UIEB = create_dataloader(dataset_name=\"UIEB\", dataset_path=\"data\")\n",
    "\n",
    "#dataloader TURBID\n",
    "train_loader_TURBID, test_loader_TURBID = create_dataloader(dataset_name=\"TURBID\", dataset_path=\"data\")\n",
    "\n",
    "#dataloader LSUI\n",
    "train_loader_LSUI, test_loader_LSUI = create_dataloader(dataset_name=\"LSUI\", dataset_path=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 16]) torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.randn(1, 3, 256, 256).to(device)\n",
    "modelos = load_models()\n",
    "for model in modelos:\n",
    "    model = model.to(device)\n",
    "    if type(model(x)) is tuple:\n",
    "        print(model(x)[0].shape, model(x)[1].shape, model(x)[2].shape)\n",
    "    else:\n",
    "        print(model(x).shape)\n",
    "\n",
    "if type(model(x)) is tuple:\n",
    "        print(model(x)[0].shape, model(x)[1].shape, model(x)[2].shape)\n",
    "    else:\n",
    "        print(model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema Atual\n",
    "### Notas sobre as Loss\n",
    "A loss lch nao esta funcionando. Ainda nao sei pq nao esta gerando resultado na loss.\n",
    "\n",
    "Serao reajustadas para o intervalo de zero a 1 : (Feito)\n",
    "* Histogram\n",
    "* darkchannel\n",
    "* hsv\n",
    "\n",
    "Falta fazer\n",
    "\n",
    "* lch nao funciona (Falta fazer)\n",
    "### Notas\n",
    "\n",
    "* Maiores problemas no autoencoder e em seu funcionamento. A dimensao latente de 16 nao gera nada aparentemente. \n",
    "* Talvez alterar o intervalo numero dos canais de cor ajude neste caso. O intervalo entre zero e 1 fazer 1/loss -1.\n",
    "    * As funcoes de perda caso seja colocadas entre o intervalo de 0 a 1 ainda se comportam gerando valores negativos. Talvez esta seja uma caracteristica desse tip de funcao e do tipo de dado que esta sendo passado. Talvez passando os dados entre -1 e 1 isso possa ser ajustado. Essa transformacao serveria apenas para as funcoes relacioandas a cor que parecem sofrer mais com este problema.\n",
    "* Terei de retreinar para passar os resultados\n",
    "* Para selecionar as melhores loss function devemos utilizar outro metodo. Nao sera possivel utilizar as metricas devido a grande quantidade de funcoes que nao gera imagem como a MSE e a SSIM. Essas funcoes tem de ser investigadas. \n",
    "* Como o tempo esta curso talvez seja melhor selecionar visualmente as melhores funcoes e agrupar de acordo com as necessidades do script.\n",
    "* Ainda preciso revisar os excperimentos do sbr\n",
    "\n",
    "### 3. Entrada no Intervalo [0, 1]:\n",
    "* Se os dados de entrada da rede estão no intervalo [0,1] e a função de perda gera valores negativos, isso não é problemático por si só. O que importa é como os gradientes resultantes influenciam os pesos.\n",
    "* Entretanto, é importante que os valores da função de perda sejam coerentes com o intervalo de entrada para manter a estabilidade do treinamento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falta ajustar os pesos das funcoes de perda colorchannel principalmente para hsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 loss functions to train with\n",
      "\n",
      "    SSIMLoss, result: 0.49716806411743164     type<class 'torch.Tensor'>)\n",
      "    PSNRLoss, result: 2.9897866249084473     type<class 'torch.Tensor'>)\n",
      "    MSELoss, result: 1.9905755519866943     type<class 'torch.Tensor'>)\n",
      "    GradientLossOpenCV, result: 10.314987182617188     type<class 'torch.Tensor'>)\n"
     ]
    }
   ],
   "source": [
    "#modelos = load_models()\n",
    "import torch\n",
    "from loss import *\n",
    "from models import *\n",
    "from src.dataload import *\n",
    "from tqdm.notebook import tqdm\n",
    "from metrics.metrics import *\n",
    "loss_battle = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#loss_battle.extend(build_perceptual_losses(rank=device))\n",
    "#loss_battle.extend(build_channel_losses(rank = device))\n",
    "loss_battle.extend(build_structural_losses(rank = device))\n",
    "\n",
    "print(f\"{len(loss_battle)} loss functions to train with\\n\")\n",
    "#print(f\"{len(modelos)} models to train with\")\n",
    "\n",
    "x = torch.randn(1, 3, 256, 256).to(device)\n",
    "y = torch.randn(1, 3, 256, 256).to(device)\n",
    "for loss_fn in loss_battle:\n",
    "    print(f\"    {loss_fn.name}, result: {loss_fn(x, y)}     type{type(loss_fn(x, x))})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0532e-07])\n",
      "tensor([1.0532e-07])\n",
      "tensor([-1.0000])\n",
      "tensor([-1.0000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalize_loss_output(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        result  = (1/result)\n",
    "        print(result)\n",
    "\n",
    "        # Lidar com valores infinitos no tensor\n",
    "        result = torch.where(torch.isinf(result), torch.tensor(1.0), result)\n",
    "        result = torch.where(result == -float('inf'), torch.tensor(0.0), result)\n",
    "        print(result)\n",
    "        result = result -1\n",
    "        print(result)\n",
    "        \n",
    "        \n",
    "        # # Obter o valor mínimo e máximo do tensor\n",
    "        # min_val = torch.min(result)\n",
    "        # max_val = torch.max(result)\n",
    "        \n",
    "        # # Normalização para o intervalo [0, 1]\n",
    "        # if max_val > min_val:\n",
    "        #     normalized_result = (result - min_val) / (max_val - min_val)\n",
    "        # else:\n",
    "        #     normalized_result = torch.zeros_like(result)\n",
    "        \n",
    "        # # Garantir que o tensor esteja no intervalo [0, 1]\n",
    "        # normalized_result = torch.clamp(normalized_result, 0, 1)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "@normalize_loss_output\n",
    "def a():\n",
    "    #return torch.Tensor([np.inf])\n",
    "    return torch.Tensor([9494949])\n",
    "print(a())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outos ajustes ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler, TensorDataset\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "    print(f\"Rank {rank} initialized.\")\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def train(rank, world_size, epochs=100):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    #dataloader UIEBUIEB\n",
    "    train_loader_UIEB, test_loader_UIEB,sampler = create_dataloader(dataset_name=\"UIEB\", dataset_path=\"data\",world_size=world_size,rank=rank,rank_test=0)\n",
    "\n",
    "    model = model.cuda(rank)##carrega o modelo\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    criterion = nn.MSELoss().cuda(rank)##carrega loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ddp_model.train()\n",
    "        sampler.set_epoch(epoch)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader_UIEB):\n",
    "            data, target = data.cuda(rank), target.cuda(rank)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = ddp_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Rank {rank}, Epoch [{epoch}/{epochs}], Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item()}\")\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--nodes\", type=int, default=1, help=\"Number of nodes\")\n",
    "    parser.add_argument(\"--gpus\", type=int, default=2, help=\"Number of GPUs per node\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    world_size = args.nodes * args.gpus\n",
    "\n",
    "    torch.multiprocessing.spawn(train, args=(world_size, args.epochs), nprocs=args.gpus, join=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%torchrun --nnodes=2 --nproc_per_node=2 --node_rank=0 --master_addr=\"10.228.252.209\" --master_port=12355 train_ddp.py --nodes=2 --gpus=2 --epochs=10\n",
    "#10.228.247.253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler, TensorDataset\n",
    "from loss import *\n",
    "import torch\n",
    "from models import *\n",
    "from src.dataload import *\n",
    "from tqdm.notebook import tqdm\n",
    "from metrics.metrics import *\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "    print(f\"Rank {rank} initialized.\")\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def train_one_model(rank, world_size, epochs, loss_fn, model_name, model):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    #dataloader UIEBUIEB\n",
    "    train_loader_UIEB, test_loader_UIEB,sampler = create_dataloader(dataset_name=\"UIEB\", dataset_path=\"data\",world_size=world_size,rank=rank,rank_test=0)\n",
    "\n",
    "    modell = model.cuda(rank)\n",
    "    ddp_model = DDP(modell, device_ids=[rank])\n",
    "\n",
    "    criterion = loss_fn().cuda(rank)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ddp_model.train()\n",
    "        sampler.set_epoch(epoch)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader_UIEB):\n",
    "            data, target = data.cuda(rank), target.cuda(rank)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = ddp_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Rank {rank}, Epoch [{epoch}/{epochs}], Batch [{batch_idx}/{len(train_loader_UIEB)}], Loss: {loss.item()}\")\n",
    "    ##Salve Dir para salvar os checkpoints\n",
    "    ckpt_savedir='output/ckpt_battle/'\n",
    "    if not os.path.exists(ckpt_savedir):\n",
    "        os.makedirs(ckpt_savedir)\n",
    "    if rank == 0:\n",
    "        # Salvar o estado do modelo original, não o DDP\n",
    "        torch.save(model.state_dict(), f\"{ckpt_savedir}{model_name}_ckpt.pth\")\n",
    "        psnr_list, ssim_list, uciqe_list, uiqm_list = [], [], [], []\n",
    "        # Avaliar o modelo\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(test_loader_UIEB):\n",
    "                data, target = data.cuda(rank), target.cuda(rank)\n",
    "                #calcula a metrica\n",
    "                targets = target.cpu().numpy()\n",
    "                predictions = model(data.cuda(rank)).cpu().numpy()\n",
    "                psnr_value, ssim_value, uciqe_, uiqm = calculate_metrics(predictions, targets)\n",
    "                psnr_list.append(psnr_value)\n",
    "                ssim_list.append(ssim_value)\n",
    "                uciqe_list.append(uciqe_)\n",
    "                uiqm_list.append(uiqm)\n",
    "        avg_ssim = sum(ssim_list) / len(ssim_list)\n",
    "        avg_psnr = sum(psnr_list) / len(psnr_list)\n",
    "        avg_uciqe = sum(uciqe_list) / len(uciqe_list)\n",
    "        avg_uiqm = sum(uiqm_list) / len(uiqm_list)\n",
    "           \n",
    "        # Salvar métricas em um arquivo\n",
    "        results_savedir='output/results_battle/'\n",
    "        if not os.path.exists(results_savedir):\n",
    "            os.makedirs(results_savedir)\n",
    "        \n",
    "        with open(f'output/{model_name}_metrics.txt', 'w') as f:\n",
    "            f.write(f\"\"\"avg_ssim:{avg_ssim}\\navg_psnr:{avg_psnr}\\navg_uciqe:{avg_uciqe}\\navg_uiqm:{avg_uiqm}\"\"\")\n",
    "            print(f\"Metrics for {model_name} saved to {results_savedir}/{model_name}_metrics.txt\")\n",
    "\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "def train(rank, world_size, epochs):\n",
    "    \n",
    "    modelos = load_models()\n",
    "    loss_battle = []\n",
    "\n",
    "    loss_battle.extend(build_perceptual_losses())\n",
    "    loss_battle.extend(build_channel_losses())\n",
    "    loss_battle.extend(build_structural_losses())\n",
    "\n",
    "    for model in modelos:\n",
    "        for loss_fn in loss_battle:\n",
    "            model_name = model.__class__.__name__ + \"_\" + loss_fn.__class__.__name__\n",
    "            train_one_model(rank, world_size, epochs, loss_fn, model_name, model)        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=100, help=\"Number of epochs per model\")\n",
    "    parser.add_argument(\"--nodes\", type=int, default=1, help=\"Number of nodes\")\n",
    "    parser.add_argument(\"--gpus\", type=int, default=2, help=\"Number of GPUs per node\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    world_size = args.nodes * args.gpus\n",
    "\n",
    "    torch.multiprocessing.spawn(train, args=(world_size, args.epochs), nprocs=args.gpus, join=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swim tranformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Tuple, List\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, embed_dim: int = 96, patch_size: int = 4):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.proj(x)  # [B, C, H, W] -> [B, embed_dim, H', W']\n",
    "        x = x.flatten(2)  # [B, embed_dim, H', W'] -> [B, embed_dim, H'*W']\n",
    "        x = x.transpose(1, 2)  # [B, embed_dim, H'*W'] -> [B, H'*W', embed_dim]\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features: int, hidden_features: int = None, out_features: int = None, drop: float = 0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, num_heads, N, C // num_heads]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = self.softmax(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim: int, input_resolution: Tuple[int, int], num_heads: int, window_size: int = 7, shift_size: int = 0, mlp_ratio: float = 4., drop: float = 0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(dim, num_heads)\n",
    "        self.drop_path = nn.Identity()\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, int(dim * mlp_ratio), drop=drop)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # Cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        x_windows = self.window_partition(shifted_x)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        attn_windows = self.attn(x_windows)\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = self.window_reverse(attn_windows, H, W)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        x = shortcut + self.drop_path(x)\n",
    "\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def window_partition(self, x: Tensor) -> Tensor:\n",
    "        B, H, W, C = x.shape\n",
    "        x = x.view(B, H // self.window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rodar no cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchrun --nnodes=2 --nproc_per_node=2 --node_rank=0 --master_addr=\"10.228.252.209\" --master_port=22 main.py --nodes=2 --gpus=2 --epochs=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local\n",
    "torchrun --nproc_per_node=2 main.py --nodes=1 --gpus=2 --epochs=100\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "losstest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
