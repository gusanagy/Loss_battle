{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import *\n",
    "import torch\n",
    "from models import *\n",
    "from src.dataload import *\n",
    "from tqdm.notebook import tqdm\n",
    "from metrics.metrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataloader UIEBUIEB\n",
    "train_loader_UIEB, test_loader_UIEB = create_dataloader(dataset_name=\"UIEB\", dataset_path=\"data\")\n",
    "\n",
    "#dataloader TURBID\n",
    "train_loader_TURBID, test_loader_TURBID = create_dataloader(dataset_name=\"TURBID\", dataset_path=\"data\")\n",
    "\n",
    "#dataloader LSUI\n",
    "train_loader_LSUI, test_loader_LSUI = create_dataloader(dataset_name=\"LSUI\", dataset_path=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def competicao():\n",
    "    ##importa dataset\n",
    "\n",
    "    ##importa loss\n",
    "\n",
    "    ##importa modelos\n",
    "\n",
    "    ##importar metricas\n",
    "\n",
    "    ##inicia estrutura com ddp e torchrun\n",
    "\n",
    "    ##treinar por 100 epocas para cada modelo//total 1500 epocas pq sao 15 loss\n",
    "\n",
    "    ##salvar checkpoints\n",
    "\n",
    "    ##roda as metricas\n",
    "\n",
    "    \n",
    "\n",
    "    modelos = [Unet_model(), Vit_model(),VAE_model()]\n",
    "    #modelos = [VAE_model()]\n",
    "\n",
    "    return modelos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 16]) torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.randn(1, 3, 256, 256).to(device)\n",
    "modelos = load_models()\n",
    "for model in modelos:\n",
    "    model = model.to(device)\n",
    "    if type(model(x)) is tuple:\n",
    "        print(model(x)[0].shape, model(x)[1].shape, model(x)[2].shape)\n",
    "    else:\n",
    "        print(model(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\"\"\n",
    "# #build_perceptual_losses()\n",
    "# {build_channel_losses()}\n",
    "# {build_structural_losses()}\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler, TensorDataset\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "    print(f\"Rank {rank} initialized.\")\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def train(rank, world_size, epochs=100):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    #dataloader UIEBUIEB\n",
    "    train_loader_UIEB, test_loader_UIEB,sampler = create_dataloader(dataset_name=\"UIEB\", dataset_path=\"data\",world_size=world_size,rank=rank,rank_test=0)\n",
    "\n",
    "    model = model.cuda(rank)##carrega o modelo\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    criterion = nn.MSELoss().cuda(rank)##carrega loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ddp_model.train()\n",
    "        sampler.set_epoch(epoch)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader_UIEB):\n",
    "            data, target = data.cuda(rank), target.cuda(rank)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = ddp_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Rank {rank}, Epoch [{epoch}/{epochs}], Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item()}\")\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10, help=\"Number of epochs\")\n",
    "    parser.add_argument(\"--nodes\", type=int, default=1, help=\"Number of nodes\")\n",
    "    parser.add_argument(\"--gpus\", type=int, default=2, help=\"Number of GPUs per node\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    world_size = args.nodes * args.gpus\n",
    "\n",
    "    torch.multiprocessing.spawn(train, args=(world_size, args.epochs), nprocs=args.gpus, join=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%torchrun --nnodes=2 --nproc_per_node=2 --node_rank=0 --master_addr=\"10.228.252.209\" --master_port=12355 train_ddp.py --nodes=2 --gpus=2 --epochs=10\n",
    "#10.228.247.253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, DistributedSampler, TensorDataset\n",
    "from loss import *\n",
    "import torch\n",
    "from models import *\n",
    "from src.dataload import *\n",
    "from tqdm.notebook import tqdm\n",
    "from metrics.metrics import *\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "    print(f\"Rank {rank} initialized.\")\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def train_one_model(rank, world_size, epochs, loss_fn, model_name, model):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    #dataloader UIEBUIEB\n",
    "    train_loader_UIEB, test_loader_UIEB,sampler = create_dataloader(dataset_name=\"UIEB\", dataset_path=\"data\",world_size=world_size,rank=rank,rank_test=0)\n",
    "\n",
    "    modell = model.cuda(rank)\n",
    "    ddp_model = DDP(modell, device_ids=[rank])\n",
    "\n",
    "    criterion = loss_fn().cuda(rank)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ddp_model.train()\n",
    "        sampler.set_epoch(epoch)\n",
    "        for batch_idx, (data, target) in enumerate(train_loader_UIEB):\n",
    "            data, target = data.cuda(rank), target.cuda(rank)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = ddp_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Rank {rank}, Epoch [{epoch}/{epochs}], Batch [{batch_idx}/{len(train_loader_UIEB)}], Loss: {loss.item()}\")\n",
    "    ##Salve Dir para salvar os checkpoints\n",
    "    ckpt_savedir='output/ckpt_battle/'\n",
    "    if not os.path.exists(ckpt_savedir):\n",
    "        os.makedirs(ckpt_savedir)\n",
    "    if rank == 0:\n",
    "        # Salvar o estado do modelo original, não o DDP\n",
    "        torch.save(model.state_dict(), f\"{ckpt_savedir}{model_name}_ckpt.pth\")\n",
    "        psnr_list, ssim_list, uciqe_list, uiqm_list = [], [], [], []\n",
    "        # Avaliar o modelo\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(test_loader_UIEB):\n",
    "                data, target = data.cuda(rank), target.cuda(rank)\n",
    "                #calcula a metrica\n",
    "                targets = target.cpu().numpy()\n",
    "                predictions = model(data.cuda(rank)).cpu().numpy()\n",
    "                psnr_value, ssim_value, uciqe_, uiqm = calculate_metrics(predictions, targets)\n",
    "                psnr_list.append(psnr_value)\n",
    "                ssim_list.append(ssim_value)\n",
    "                uciqe_list.append(uciqe_)\n",
    "                uiqm_list.append(uiqm)\n",
    "        avg_ssim = sum(ssim_list) / len(ssim_list)\n",
    "        avg_psnr = sum(psnr_list) / len(psnr_list)\n",
    "        avg_uciqe = sum(uciqe_list) / len(uciqe_list)\n",
    "        avg_uiqm = sum(uiqm_list) / len(uiqm_list)\n",
    "           \n",
    "        # Salvar métricas em um arquivo\n",
    "        results_savedir='output/results_battle/'\n",
    "        if not os.path.exists(results_savedir):\n",
    "            os.makedirs(results_savedir)\n",
    "        \n",
    "        with open(f'output/{model_name}_metrics.txt', 'w') as f:\n",
    "            f.write(f\"\"\"avg_ssim:{avg_ssim}\\navg_psnr:{avg_psnr}\\navg_uciqe:{avg_uciqe}\\navg_uiqm:{avg_uiqm}\"\"\")\n",
    "            print(f\"Metrics for {model_name} saved to {results_savedir}/{model_name}_metrics.txt\")\n",
    "\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "def train(rank, world_size, epochs):\n",
    "    \n",
    "    modelos = load_models()\n",
    "    loss_battle = []\n",
    "\n",
    "    loss_battle.extend(build_perceptual_losses())\n",
    "    loss_battle.extend(build_channel_losses())\n",
    "    loss_battle.extend(build_structural_losses())\n",
    "\n",
    "    for model in modelos:\n",
    "        for loss_fn in loss_battle:\n",
    "            model_name = model.__class__.__name__ + \"_\" + loss_fn.__class__.__name__\n",
    "            train_one_model(rank, world_size, epochs, loss_fn, model_name, model)        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=100, help=\"Number of epochs per model\")\n",
    "    parser.add_argument(\"--nodes\", type=int, default=1, help=\"Number of nodes\")\n",
    "    parser.add_argument(\"--gpus\", type=int, default=2, help=\"Number of GPUs per node\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    world_size = args.nodes * args.gpus\n",
    "\n",
    "    torch.multiprocessing.spawn(train, args=(world_size, args.epochs), nprocs=args.gpus, join=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rodar no cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nnodes=2 --nproc_per_node=2 --node_rank=0 --master_addr=\"10.228.252.209\" --master_port=12355 main.py --nodes=2 --gpus=2 --epochs=100\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "losstest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
