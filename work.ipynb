{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A notebook for test and debug some python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "* Create a conda env X\n",
    "*     Im having a error X (resolvi usando pip no ambiente)\n",
    "*   ERRO TRANSFORM TORCHVISION\n",
    "* Selecionar e colocar arquiteturas x\n",
    "* unet x\n",
    "* ddpm\n",
    "* cyclogan \n",
    "* vae\n",
    "* vit\n",
    "* Colocar funcoes de perda da tabela\n",
    "* Colocar todas as metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the utils libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torchviz as tv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archtectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloco de duas convoluções 3x3 seguidas de ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementação da arquitetura U-Net adaptada para geração de imagens.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Encoder\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.down5 = DoubleConv(512, 1024)\n",
    "\n",
    "        # Max-pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.conv_up1 = DoubleConv(1024, 512)\n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv_up2 = DoubleConv(512, 256)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv_up3 = DoubleConv(256, 128)\n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv_up4 = DoubleConv(128, 64)\n",
    "\n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(self.pool(d1))\n",
    "        d3 = self.down3(self.pool(d2))\n",
    "        d4 = self.down4(self.pool(d3))\n",
    "        d5 = self.down5(self.pool(d4))\n",
    "\n",
    "        # Decoder\n",
    "        u1 = self.up1(d5)\n",
    "        u1 = torch.cat((u1, d4), dim=1)\n",
    "        u1 = self.conv_up1(u1)\n",
    "        u2 = self.up2(u1)\n",
    "        u2 = torch.cat((u2, d3), dim=1)\n",
    "        u2 = self.conv_up2(u2)\n",
    "        u3 = self.up3(u2)\n",
    "        u3 = torch.cat((u3, d2), dim=1)\n",
    "        u3 = self.conv_up3(u3)\n",
    "        u4 = self.up4(u3)\n",
    "        u4 = torch.cat((u4, d1), dim=1)\n",
    "        u4 = self.conv_up4(u4)\n",
    "\n",
    "        return self.final_conv(u4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Definindo o dispositivo (GPU se disponível)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Inicializando a U-Net\n",
    "    in_channels = 3  # Por exemplo, RGB\n",
    "    out_channels = 3  # Saída de imagem RGB\n",
    "    model = UNet(in_channels, out_channels).to(device)\n",
    "\n",
    "    # Exemplo de entrada\n",
    "    x = torch.randn(1, in_channels, 256, 256).to(device)  # Batch size 1, 256x256 imagem\n",
    "    output = model(x)\n",
    "\n",
    "    print(x.shape, output.shape)  # Deve ser (1, out_channels, 256, 256)\n",
    "     # Plotando a arquitetura da rede\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitetura da U-Net plotada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        y = model(torch.randn(1, in_channels, 256, 256).to(device))\n",
    "        tv.make_dot(y, params=dict(list(model.named_parameters()))).render(\"unet_architecture\", format=\"png\")\n",
    "        \n",
    "        #plot.format = 'png'\n",
    "        \n",
    "        print(\"Arquitetura da U-Net plotada com sucesso.\")\n",
    "except ImportError:\n",
    "        print(\"Para plotar a arquitetura, instale o pacote torchviz.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Layer (type:depth-idx)                   Output Shape     Param #\n",
      "========================================================================\n",
      "DoubleConv: 2-1                          [-1, 64, 256, 256] --\n",
      "MaxPool2d: 1-1                           [-1, 64, 128, 128] --\n",
      "DoubleConv: 2-2                          [-1, 128, 128, 128] --\n",
      "MaxPool2d: 1-2                           [-1, 128, 64, 64] --\n",
      "DoubleConv: 2-3                          [-1, 256, 64, 64] --\n",
      "MaxPool2d: 1-3                           [-1, 256, 32, 32] --\n",
      "DoubleConv: 1-4                          [-1, 512, 32, 32] --\n",
      "Sequential: 2-4                          [-1, 512, 32, 32] --\n",
      "ConvTranspose2d: 2-5                     [-1, 256, 64, 64] 524,544\n",
      "DoubleConv: 2-6                          [-1, 256, 64, 64] --\n",
      "ConvTranspose2d: 2-7                     [-1, 128, 128, 128] 131,200\n",
      "DoubleConv: 2-8                          [-1, 128, 128, 128] --\n",
      "ConvTranspose2d: 2-9                     [-1, 64, 256, 256] 32,832\n",
      "DoubleConv: 2-10                         [-1, 64, 256, 256] --\n",
      "Conv2d: 1-5                              [-1, 3, 256, 256] 195\n",
      "========================================================================\n",
      "Total params: 3,542,211\n",
      "Trainable params: 3,542,211\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 6.47\n",
      "========================================================================\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 57.50\n",
      "Params size (MB): 13.51\n",
      "Estimated Total Size (MB): 71.76\n",
      "========================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================\n",
       "Layer (type:depth-idx)                   Output Shape     Param #\n",
       "========================================================================\n",
       "DoubleConv: 2-1                          [-1, 64, 256, 256] --\n",
       "MaxPool2d: 1-1                           [-1, 64, 128, 128] --\n",
       "DoubleConv: 2-2                          [-1, 128, 128, 128] --\n",
       "MaxPool2d: 1-2                           [-1, 128, 64, 64] --\n",
       "DoubleConv: 2-3                          [-1, 256, 64, 64] --\n",
       "MaxPool2d: 1-3                           [-1, 256, 32, 32] --\n",
       "DoubleConv: 1-4                          [-1, 512, 32, 32] --\n",
       "Sequential: 2-4                          [-1, 512, 32, 32] --\n",
       "ConvTranspose2d: 2-5                     [-1, 256, 64, 64] 524,544\n",
       "DoubleConv: 2-6                          [-1, 256, 64, 64] --\n",
       "ConvTranspose2d: 2-7                     [-1, 128, 128, 128] 131,200\n",
       "DoubleConv: 2-8                          [-1, 128, 128, 128] --\n",
       "ConvTranspose2d: 2-9                     [-1, 64, 256, 256] 32,832\n",
       "DoubleConv: 2-10                         [-1, 64, 256, 256] --\n",
       "Conv2d: 1-5                              [-1, 3, 256, 256] 195\n",
       "========================================================================\n",
       "Total params: 3,542,211\n",
       "Trainable params: 3,542,211\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.47\n",
       "========================================================================\n",
       "Input size (MB): 0.75\n",
       "Forward/backward pass size (MB): 57.50\n",
       "Params size (MB): 13.51\n",
       "Estimated Total Size (MB): 71.76\n",
       "========================================================================"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, x,depth=1, dtypes=[torch.long],branching=False,verbose=1, col_width=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "Arquitetura da U-Net plotada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloco de duas convoluções 3x3 seguidas de ReLU.\n",
    "    Permite a parametrização do uso de BatchNorm.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_batch_norm=True):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        layers.extend([\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ])\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        self.double_conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementação da arquitetura U-Net parametrizável para geração de imagens.\n",
    "    Permite a parametrização do número de blocos de convolução e o uso de BatchNorm.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, base_filters=64, num_layers=5, use_batch_norm=True):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.base_filters = base_filters\n",
    "        self.num_layers = num_layers\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        filters = base_filters\n",
    "        \n",
    "        for i in range(num_layers-1):\n",
    "            self.encoder_layers.append(DoubleConv(in_channels, filters, use_batch_norm))\n",
    "            in_channels = filters\n",
    "            filters *= 2\n",
    "        \n",
    "        # Max-pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(filters // 2, filters, use_batch_norm)\n",
    "\n",
    "        # Decoder\n",
    "        self.up_layers = nn.ModuleList()\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        for i in range(num_layers-1, 0, -1):#range(num_layers-1, 0, -1)\n",
    "            filters //= 2\n",
    "            self.up_layers.append(nn.ConvTranspose2d(filters * 2, filters, kernel_size=2, stride=2))\n",
    "            self.up_convs.append(DoubleConv(filters * 2, filters , use_batch_norm))\n",
    "            \n",
    "\n",
    "        # Final convolution\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(base_filters, out_channels, kernel_size=1)\n",
    "    def see_model(self):\n",
    "        print(f\"\"\"\n",
    "        encoder: {self.encoder_layers}\n",
    "        pool: {self.pool}\n",
    "        botleneck: {self.bottleneck}\n",
    "        up laeyers: {self.up_layers}\n",
    "        up_convs: {self.up_convs}\n",
    "        final conv: {self.final_conv}\"\"\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        encoder_results = []\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "            encoder_results.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Decoder\n",
    "        for i, (up, conv) in enumerate(zip(self.up_layers, self.up_convs)):\n",
    "            x = up(x)\n",
    "            x = torch.cat((x, encoder_results[-(i+1)]), dim=1)\n",
    "            x = conv(x)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "def standardize_image(image):\n",
    "    \"\"\"\n",
    "    Função para padronizar as imagens de entrada.\n",
    "    \"\"\"\n",
    "    mean = image.mean()\n",
    "    std = image.std()\n",
    "    return (image - mean) / std\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Definindo o dispositivo (GPU se disponível)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Inicializando a U-Net parametrizável\n",
    "    in_channels = 3  # Por exemplo, RGB\n",
    "    out_channels = 3  # Saída de imagem RGB\n",
    "    base_filters = 64\n",
    "    num_layers = 4\n",
    "    use_batch_norm = True\n",
    "    model = UNet(in_channels, out_channels, base_filters, num_layers, use_batch_norm).to(device)\n",
    "    #model.see_model()\n",
    "\n",
    "    # Exibindo a arquitetura da rede\n",
    "    #summary(model, input_size=(in_channels, 256, 256))\n",
    "\n",
    "    # Exemplo de entrada\n",
    "    x = torch.randn(1, in_channels, 256, 256).to(device)  # Batch size 1, 256x256 imagem\n",
    "    # x = standardize_image(x)\n",
    "    output = model(x)\n",
    "\n",
    "    print(output.shape)  # Deve ser (1, out_channels, 256, 256)\n",
    "\n",
    "    # Plotando a arquitetura da rede\n",
    "    try:\n",
    "\n",
    "        y = model(torch.randn(1, in_channels, 256, 256).to(device))\n",
    "        tv.make_dot(y, params=dict(list(model.named_parameters()))).render(\"unet_architecture_2\", format=\"png\")\n",
    "        print(\"Arquitetura da U-Net plotada com sucesso.\")\n",
    "    except ImportError:\n",
    "        print(\"Para plotar a arquitetura, instale o pacote torchviz.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Layer (type:depth-idx)                   Output Shape     Param #\n",
      "========================================================================\n",
      "DoubleConv: 2-1                          [-1, 64, 256, 256] --\n",
      "MaxPool2d: 1-1                           [-1, 64, 128, 128] --\n",
      "DoubleConv: 2-2                          [-1, 128, 128, 128] --\n",
      "MaxPool2d: 1-2                           [-1, 128, 64, 64] --\n",
      "DoubleConv: 2-3                          [-1, 256, 64, 64] --\n",
      "MaxPool2d: 1-3                           [-1, 256, 32, 32] --\n",
      "DoubleConv: 1-4                          [-1, 512, 32, 32] --\n",
      "Sequential: 2-4                          [-1, 512, 32, 32] --\n",
      "ConvTranspose2d: 2-5                     [-1, 256, 64, 64] 524,544\n",
      "DoubleConv: 2-6                          [-1, 256, 64, 64] --\n",
      "ConvTranspose2d: 2-7                     [-1, 128, 128, 128] 131,200\n",
      "DoubleConv: 2-8                          [-1, 128, 128, 128] --\n",
      "ConvTranspose2d: 2-9                     [-1, 64, 256, 256] 32,832\n",
      "DoubleConv: 2-10                         [-1, 64, 256, 256] --\n",
      "Conv2d: 1-5                              [-1, 3, 256, 256] 195\n",
      "========================================================================\n",
      "Total params: 3,542,211\n",
      "Trainable params: 3,542,211\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 6.47\n",
      "========================================================================\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 57.50\n",
      "Params size (MB): 13.51\n",
      "Estimated Total Size (MB): 71.76\n",
      "========================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================\n",
       "Layer (type:depth-idx)                   Output Shape     Param #\n",
       "========================================================================\n",
       "DoubleConv: 2-1                          [-1, 64, 256, 256] --\n",
       "MaxPool2d: 1-1                           [-1, 64, 128, 128] --\n",
       "DoubleConv: 2-2                          [-1, 128, 128, 128] --\n",
       "MaxPool2d: 1-2                           [-1, 128, 64, 64] --\n",
       "DoubleConv: 2-3                          [-1, 256, 64, 64] --\n",
       "MaxPool2d: 1-3                           [-1, 256, 32, 32] --\n",
       "DoubleConv: 1-4                          [-1, 512, 32, 32] --\n",
       "Sequential: 2-4                          [-1, 512, 32, 32] --\n",
       "ConvTranspose2d: 2-5                     [-1, 256, 64, 64] 524,544\n",
       "DoubleConv: 2-6                          [-1, 256, 64, 64] --\n",
       "ConvTranspose2d: 2-7                     [-1, 128, 128, 128] 131,200\n",
       "DoubleConv: 2-8                          [-1, 128, 128, 128] --\n",
       "ConvTranspose2d: 2-9                     [-1, 64, 256, 256] 32,832\n",
       "DoubleConv: 2-10                         [-1, 64, 256, 256] --\n",
       "Conv2d: 1-5                              [-1, 3, 256, 256] 195\n",
       "========================================================================\n",
       "Total params: 3,542,211\n",
       "Trainable params: 3,542,211\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.47\n",
       "========================================================================\n",
       "Input size (MB): 0.75\n",
       "Forward/backward pass size (MB): 57.50\n",
       "Params size (MB): 13.51\n",
       "Estimated Total Size (MB): 71.76\n",
       "========================================================================"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, x,depth=1, dtypes=[torch.long],branching=False,verbose=1, col_width=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpa memoria CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória alocada: 0 bytes\n",
      "Memória reservada: 0 bytes\n",
      "Memória alocada: 0 bytes\n",
      "Memória reservada: 0 bytes\n",
      "Memoria limpa\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "# Force a coleta de lixo para liberar a memória imediatamente\n",
    "gc.collect()\n",
    "\n",
    "# Libere o cache de memória do PyTorch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "print(f\"Memoria limpa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CycleGAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 1, 4, 4], expected input[1, 3, 255, 255] to have 1 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m model \u001b[38;5;241m=\u001b[39m VAE(latent_dim)\n\u001b[1;32m     83\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# optimizer = optim.Adam(model.parameters(), lr=lr)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# model.train()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#         save_image(recon_batch, f'recon_{i}.png', nrow=8, normalize=True)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m#         break\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 56\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 56\u001b[0m     mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, logvar)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z), mu, logvar\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 18\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 1, 4, 4], expected input[1, 3, 255, 255] to have 1 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#from torchvision import datasets, transforms\n",
    "#from torchvision.utils import save_image\n",
    "\n",
    "# Definição do Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc_mu = nn.Linear(64 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64 * 4 * 4, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "# Definição do Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 128 * 4 * 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x = torch.relu(self.fc(z))\n",
    "        x = x.view(x.size(0), 128, 4, 4)\n",
    "        x = torch.relu(self.deconv1(x))\n",
    "        x = torch.relu(self.deconv2(x))\n",
    "        x = torch.sigmoid(self.deconv3(x))\n",
    "        return x\n",
    "\n",
    "# Definição do VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "# Função de perda\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Configuração de parâmetros e carregamento de dados\n",
    "latent_dim = 20\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5,), (0.5,))\n",
    "# ])\n",
    "\n",
    "# train_dataset = datasets.MNIST('.', train=True, download=True, transform=None)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Inicialização do modelo, otimizador e treinamento\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE(latent_dim)\n",
    "x = torch.randn(1, 3, 255, 255)\n",
    "output = model(x)\n",
    "print(output.shape)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(epochs):\n",
    "#     train_loss = 0\n",
    "#     for batch_idx, (data, _) in enumerate(train_loader):\n",
    "#         data = data.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         recon_batch, mu, logvar = model(data)\n",
    "#         loss = loss_function(recon_batch, data, mu, logvar)\n",
    "#         loss.backward()\n",
    "#         train_loss += loss.item()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     print(f'Epoch {epoch+1}, Loss: {train_loss/len(train_loader.dataset)}')\n",
    "\n",
    "# # Salvar imagens reconstruídas\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for i, (data, _) in enumerate(train_loader):\n",
    "#         data = data.to(device)\n",
    "#         recon_batch, _, _ = model(data)\n",
    "#         save_image(recon_batch, f'recon_{i}.png', nrow=8, normalize=True)\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "128\n",
      "64\n",
      "32\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x8192 and 512x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 96\u001b[0m\n\u001b[1;32m     94\u001b[0m model \u001b[38;5;241m=\u001b[39m VAE(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, hidden_dims\u001b[38;5;241m=\u001b[39mhidden_dims, latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     95\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape,output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# optimizer = optim.Adam(model.parameters(), lr=lr)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# model.train()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#     sample = model.decoder(z)\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#     save_image(sample.view(64, 1, 28, 28), 'sample.png', nrow=8, normalize=True)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 65\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 65\u001b[0m     mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, logvar)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z), mu, logvar\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 26\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_mu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_logvar(x)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mu, logvar\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Losses/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x8192 and 512x20)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from torchvision import datasets, transforms\n",
    "# from torchvision.utils import save_image\n",
    "import random\n",
    "\n",
    "# Definição do Encoder com parâmetros configuráveis\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dims, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        layers = []\n",
    "        for h_dim in hidden_dims:\n",
    "            print(h_dim)\n",
    "            layers.append(nn.Conv2d(in_channels, h_dim, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_channels = h_dim\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1] * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dims[-1] * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "# Definição do Decoder com parâmetros configuráveis\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dims, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, hidden_dims[-1] * 4 * 4)\n",
    "        hidden_dims.reverse()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_dims)):\n",
    "            print(hidden_dims[i])\n",
    "            layers.append(nn.ConvTranspose2d(hidden_dims[i], hidden_dims[i], kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "            layers.append(nn.BatchNorm2d(hidden_dims[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.ConvTranspose2d(hidden_dims[-1], out_channels, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(x.size(0), -1, 4, 4)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Definição do VAE que une o Encoder e o Decoder\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dims, latent_dim, out_channels):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_dims, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dims, out_channels)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "# Função de perda personalizada para VAE\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Configuração de parâmetros e carregamento de dados\n",
    "latent_dim = 20\n",
    "hidden_dims = [256, 128, 64, 32]\n",
    "lr = 1e-3\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "# Transformações aplicadas aos dados de entrada\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5,), (0.5,))\n",
    "# ])\n",
    "\n",
    "# Carregamento do dataset MNIST\n",
    "# train_dataset = datasets.MNIST('.', train=True, download=True, transform=transform)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Inicialização do modelo, otimizador e treinamento\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE(in_channels=3, hidden_dims=hidden_dims, latent_dim=latent_dim, out_channels=1)\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "output = model(x)\n",
    "print(x.shape,output.shape)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(epochs):\n",
    "#     train_loss = 0\n",
    "#     for batch_idx, (data, _) in enumerate(train_loader):\n",
    "#         data = data.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         recon_batch, mu, logvar = model(data)\n",
    "#         loss = loss_function(recon_batch, data, mu, logvar)\n",
    "#         loss.backward()\n",
    "#         train_loss += loss.item()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     print(f'Epoch {epoch+1}, Loss: {train_loss/len(train_loader.dataset)}')\n",
    "\n",
    "# # Teste do modelo com um vetor random\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     z = torch.randn(64, latent_dim).to(device)\n",
    "#     sample = model.decoder(z)\n",
    "#     save_image(sample.view(64, 1, 28, 28), 'sample.png', nrow=8, normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#from torchvision import transforms \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = x + self.dropout(mlp_output)\n",
    "        x = self.norm2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEnhancerTransformer(nn.Module):\n",
    "    def __init__(self, image_size=256, patch_size=16, num_channels=3, embed_dim=512, num_heads=8, mlp_dim=1024, num_layers=6, dropout=0.1):\n",
    "        super(ImageEnhancerTransformer, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_dim = num_channels * patch_size * patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embeddings = nn.Linear(self.patch_dim, embed_dim)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, self.patch_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self._to_patches(x)\n",
    "        x = self.patch_embeddings(x) + self.position_embeddings\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.fc(x)\n",
    "        x = self._from_patches(x, batch_size)\n",
    "        return x\n",
    "\n",
    "    def _to_patches(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        x = x.view(batch_size, channels, self.image_size // self.patch_size, self.patch_size,\n",
    "                   self.image_size // self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1).contiguous()\n",
    "        x = x.view(batch_size, self.num_patches, -1)\n",
    "        return x\n",
    "\n",
    "    def _from_patches(self, x, batch_size):\n",
    "        x = x.view(batch_size, self.image_size // self.patch_size, self.image_size // self.patch_size,\n",
    "                   self.patch_size, self.patch_size, -1)\n",
    "        x = x.permute(0, 5, 1, 3, 2, 4).contiguous()\n",
    "        x = x.view(batch_size, -1, self.image_size, self.image_size)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma do tensor de entrada: torch.Size([1, 3, 256, 256])\n",
      "Forma do tensor de saída: torch.Size([1, 3, 256, 256])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Linear: 1-1                            [-1, 256, 512]            393,728\n",
      "├─ModuleList: 1                          []                        --\n",
      "|    └─TransformerBlock: 2-1             [-1, 256, 512]            --\n",
      "|    |    └─MultiheadAttention: 3-1      [-1, 256, 512]            1,050,624\n",
      "|    |    └─Dropout: 3-2                 [-1, 256, 512]            --\n",
      "|    |    └─LayerNorm: 3-3               [-1, 256, 512]            1,024\n",
      "|    |    └─Sequential: 3-4              [-1, 256, 512]            1,050,112\n",
      "|    |    └─Dropout: 3-5                 [-1, 256, 512]            --\n",
      "|    |    └─LayerNorm: 3-6               [-1, 256, 512]            1,024\n",
      "|    └─TransformerBlock: 2-2             [-1, 256, 512]            --\n",
      "|    |    └─MultiheadAttention: 3-7      [-1, 256, 512]            1,050,624\n",
      "|    |    └─Dropout: 3-8                 [-1, 256, 512]            --\n",
      "|    |    └─LayerNorm: 3-9               [-1, 256, 512]            1,024\n",
      "|    |    └─Sequential: 3-10             [-1, 256, 512]            1,050,112\n",
      "|    |    └─Dropout: 3-11                [-1, 256, 512]            --\n",
      "|    |    └─LayerNorm: 3-12              [-1, 256, 512]            1,024\n",
      "|    └─TransformerBlock: 2-3             [-1, 256, 512]            --\n",
      "|    |    └─MultiheadAttention: 3-13     [-1, 256, 512]            1,050,624\n",
      "|    |    └─Dropout: 3-14                [-1, 256, 512]            --\n",
      "|    |    └─LayerNorm: 3-15              [-1, 256, 512]            1,024\n",
      "|    |    └─Sequential: 3-16             [-1, 256, 512]            1,050,112\n",
      "|    |    └─Dropout: 3-17                [-1, 256, 512]            --\n",
      "|    |    └─LayerNorm: 3-18              [-1, 256, 512]            1,024\n",
      "|    └─TransformerBlock: 2-4             [-1, 256, 512]            --\n",
      "|    |    └─MultiheadAttention: 3-19     [-1, 256, 512]            1,050,624\n",
      "|    |    └─Dropout: 3-20                [-1, 256, 512]            --\n",
      "|    |    └─LayerNorm: 3-21              [-1, 256, 512]            1,024\n",
      "|    |    └─Sequential: 3-22             [-1, 256, 512]            1,050,112\n",
      "|    |    └─Dropout: 3-23                [-1, 256, 512]            --\n",
      "|    |    └─LayerNorm: 3-24              [-1, 256, 512]            1,024\n",
      "|    └─TransformerBlock: 2-5             [-1, 256, 512]            --\n",
      "|    |    └─MultiheadAttention: 3-25     [-1, 256, 512]            1,050,624\n",
      "|    |    └─Dropout: 3-26                [-1, 256, 512]            --\n",
      "|    |    └─LayerNorm: 3-27              [-1, 256, 512]            1,024\n",
      "|    |    └─Sequential: 3-28             [-1, 256, 512]            1,050,112\n",
      "|    |    └─Dropout: 3-29                [-1, 256, 512]            --\n",
      "|    |    └─LayerNorm: 3-30              [-1, 256, 512]            1,024\n",
      "|    └─TransformerBlock: 2-6             [-1, 256, 512]            --\n",
      "|    |    └─MultiheadAttention: 3-31     [-1, 256, 512]            1,050,624\n",
      "|    |    └─Dropout: 3-32                [-1, 256, 512]            --\n",
      "|    |    └─LayerNorm: 3-33              [-1, 256, 512]            1,024\n",
      "|    |    └─Sequential: 3-34             [-1, 256, 512]            1,050,112\n",
      "|    |    └─Dropout: 3-35                [-1, 256, 512]            --\n",
      "|    |    └─LayerNorm: 3-36              [-1, 256, 512]            1,024\n",
      "├─LayerNorm: 1-2                         [-1, 256, 512]            1,024\n",
      "├─Linear: 1-3                            [-1, 256, 768]            393,984\n",
      "==========================================================================================\n",
      "Total params: 13,405,440\n",
      "Trainable params: 13,405,440\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 32.26\n",
      "==========================================================================================\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 33.50\n",
      "Params size (MB): 51.14\n",
      "Estimated Total Size (MB): 85.39\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Linear: 1-1                            [-1, 256, 512]            393,728\n",
       "├─ModuleList: 1                          []                        --\n",
       "|    └─TransformerBlock: 2-1             [-1, 256, 512]            --\n",
       "|    |    └─MultiheadAttention: 3-1      [-1, 256, 512]            1,050,624\n",
       "|    |    └─Dropout: 3-2                 [-1, 256, 512]            --\n",
       "|    |    └─LayerNorm: 3-3               [-1, 256, 512]            1,024\n",
       "|    |    └─Sequential: 3-4              [-1, 256, 512]            1,050,112\n",
       "|    |    └─Dropout: 3-5                 [-1, 256, 512]            --\n",
       "|    |    └─LayerNorm: 3-6               [-1, 256, 512]            1,024\n",
       "|    └─TransformerBlock: 2-2             [-1, 256, 512]            --\n",
       "|    |    └─MultiheadAttention: 3-7      [-1, 256, 512]            1,050,624\n",
       "|    |    └─Dropout: 3-8                 [-1, 256, 512]            --\n",
       "|    |    └─LayerNorm: 3-9               [-1, 256, 512]            1,024\n",
       "|    |    └─Sequential: 3-10             [-1, 256, 512]            1,050,112\n",
       "|    |    └─Dropout: 3-11                [-1, 256, 512]            --\n",
       "|    |    └─LayerNorm: 3-12              [-1, 256, 512]            1,024\n",
       "|    └─TransformerBlock: 2-3             [-1, 256, 512]            --\n",
       "|    |    └─MultiheadAttention: 3-13     [-1, 256, 512]            1,050,624\n",
       "|    |    └─Dropout: 3-14                [-1, 256, 512]            --\n",
       "|    |    └─LayerNorm: 3-15              [-1, 256, 512]            1,024\n",
       "|    |    └─Sequential: 3-16             [-1, 256, 512]            1,050,112\n",
       "|    |    └─Dropout: 3-17                [-1, 256, 512]            --\n",
       "|    |    └─LayerNorm: 3-18              [-1, 256, 512]            1,024\n",
       "|    └─TransformerBlock: 2-4             [-1, 256, 512]            --\n",
       "|    |    └─MultiheadAttention: 3-19     [-1, 256, 512]            1,050,624\n",
       "|    |    └─Dropout: 3-20                [-1, 256, 512]            --\n",
       "|    |    └─LayerNorm: 3-21              [-1, 256, 512]            1,024\n",
       "|    |    └─Sequential: 3-22             [-1, 256, 512]            1,050,112\n",
       "|    |    └─Dropout: 3-23                [-1, 256, 512]            --\n",
       "|    |    └─LayerNorm: 3-24              [-1, 256, 512]            1,024\n",
       "|    └─TransformerBlock: 2-5             [-1, 256, 512]            --\n",
       "|    |    └─MultiheadAttention: 3-25     [-1, 256, 512]            1,050,624\n",
       "|    |    └─Dropout: 3-26                [-1, 256, 512]            --\n",
       "|    |    └─LayerNorm: 3-27              [-1, 256, 512]            1,024\n",
       "|    |    └─Sequential: 3-28             [-1, 256, 512]            1,050,112\n",
       "|    |    └─Dropout: 3-29                [-1, 256, 512]            --\n",
       "|    |    └─LayerNorm: 3-30              [-1, 256, 512]            1,024\n",
       "|    └─TransformerBlock: 2-6             [-1, 256, 512]            --\n",
       "|    |    └─MultiheadAttention: 3-31     [-1, 256, 512]            1,050,624\n",
       "|    |    └─Dropout: 3-32                [-1, 256, 512]            --\n",
       "|    |    └─LayerNorm: 3-33              [-1, 256, 512]            1,024\n",
       "|    |    └─Sequential: 3-34             [-1, 256, 512]            1,050,112\n",
       "|    |    └─Dropout: 3-35                [-1, 256, 512]            --\n",
       "|    |    └─LayerNorm: 3-36              [-1, 256, 512]            1,024\n",
       "├─LayerNorm: 1-2                         [-1, 256, 512]            1,024\n",
       "├─Linear: 1-3                            [-1, 256, 768]            393,984\n",
       "==========================================================================================\n",
       "Total params: 13,405,440\n",
       "Trainable params: 13,405,440\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 32.26\n",
       "==========================================================================================\n",
       "Input size (MB): 0.75\n",
       "Forward/backward pass size (MB): 33.50\n",
       "Params size (MB): 51.14\n",
       "Estimated Total Size (MB): 85.39\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100  # número de exemplos fictício\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Gera uma imagem aleatória como entrada e uma versão \"melhorada\" como alvo\n",
    "        input_image = Image.fromarray((np.random.rand(256, 256, 3) * 255).astype(np.uint8))\n",
    "        target_image = Image.fromarray((np.random.rand(256, 256, 3) * 255).astype(np.uint8))\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            target_image = self.transform(target_image)\n",
    "\n",
    "        return input_image, target_image\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "dataset = DummyDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "model = ImageEnhancerTransformer()\n",
    "# Gera um tensor aleatório como entrada\n",
    "input_tensor = torch.rand(1, 3, 256, 256)  # Batch size de 1, 3 canais, 256x256 de resolução\n",
    "\n",
    "# Passa o tensor pelo modelo\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "# Mostra as formas dos tensores de entrada e saída\n",
    "print(\"Forma do tensor de entrada:\", input_tensor.shape)\n",
    "print(\"Forma do tensor de saída:\", output_tensor.shape)\n",
    "\n",
    "summary(model, (3, 256, 256))\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # Loop de treinamento\n",
    "# for epoch in range(10):  # número de épocas fictício\n",
    "#     for inputs, targets in dataloader:\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitetura da U-Net plotada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Plotando a arquitetura da rede\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "\n",
    "    y = model(torch.randn(1, 3, 256, 256).to(device))\n",
    "    tv.make_dot(y, params=dict(list(model.named_parameters()))).render(\"vit_architecture\", format=\"png\")\n",
    "    print(\"Arquitetura da U-Net plotada com sucesso.\")\n",
    "except ImportError:\n",
    "    print(\"Para plotar a arquitetura, instale o pacote torchviz.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória alocada: 0 bytes\n",
      "Memória reservada: 0 bytes\n",
      "Memória alocada: 0 bytes\n",
      "Memória reservada: 0 bytes\n",
      "Memoria limpa\n"
     ]
    }
   ],
   "source": [
    "### Limpa memoria CUDA\n",
    "import gc\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "# Force a coleta de lixo para liberar a memória imediatamente\n",
    "gc.collect()\n",
    "\n",
    "# Libere o cache de memória do PyTorch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "print(f\"Memoria limpa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gan loss\t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, use_lsgan=True):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.use_lsgan = use_lsgan\n",
    "        if use_lsgan:\n",
    "            self.criterion = nn.MSELoss()\n",
    "        else:\n",
    "            self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, output, target_is_real):\n",
    "        target = torch.ones_like(output) if target_is_real else torch.zeros_like(output)\n",
    "        loss = self.criterion(output, target)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color loss\t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorLoss, self).__init__()\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.criterion(input, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SSIM PSNR MS-SSIM\t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49804675579071045 -7.775151252746582 67.11198425292969 0.07674139738082886\n"
     ]
    }
   ],
   "source": [
    "from kornia.losses import ssim_loss as ssim, psnr_loss as psnr, MS_SSIMLoss as ms_ssim, charbonnier_loss as charbonnier\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "input1 = torch.rand(1, 3, 255, 255)\n",
    "input2 = torch.rand(1, 3, 255, 255)\n",
    "# print(np.max(input1.numpy()), np.max(input2.numpy()))\n",
    "\n",
    "loss = ssim(input1, input2, 11)\n",
    "\n",
    "loss2 = psnr(input1,input2, np.max(input1.numpy())) # 10 * log(4/((1.2-1)**2)) / log(10)\n",
    "loss3 = ms_ssim()\n",
    "\n",
    "loss4 = charbonnier(input1, input2,reduction='mean')\n",
    "\n",
    "# class SSIMLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SSIMLoss, self).__init__()\n",
    "\n",
    "#     def forward(self, img1, img2):\n",
    "#         return ssim(img1, img2)\n",
    "# class PSNRLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(PSNRLoss, self).__init__()\n",
    "\n",
    "#     def forward(self, img1, img2):\n",
    "#         return (img1, img2)\n",
    "print(loss.item(), loss2.item(), loss3(input1, input2).item(), loss4.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE\t \t\t \t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAELoss, self).__init__()\n",
    "        self.criterion = nn.L1Loss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.criterion(input, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAELoss, self).__init__()\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.criterion(input, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uranker\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUSIQ\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Loss VGG\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acumlative Superiority\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminative\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIoU\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGan Loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Structure Perceptual\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detail loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGan\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angular\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### margin-ranking loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity mapping\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle consistency loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB loss\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCH loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Losses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
