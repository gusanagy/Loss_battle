{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A notebook for test and debug some python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "Create a conda env \n",
    "    Im having a error\n",
    "Selecionar e colocar arquiteturas\n",
    "Colocar funcoes de perda da tabela\n",
    "Colocar todas as metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the utils libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torchviz as tv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archtectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256]) torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloco de duas convoluções 3x3 seguidas de ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementação da arquitetura U-Net adaptada para geração de imagens.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Encoder\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.down5 = DoubleConv(512, 1024)\n",
    "\n",
    "        # Max-pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.conv_up1 = DoubleConv(1024, 512)\n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv_up2 = DoubleConv(512, 256)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv_up3 = DoubleConv(256, 128)\n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv_up4 = DoubleConv(128, 64)\n",
    "\n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(self.pool(d1))\n",
    "        d3 = self.down3(self.pool(d2))\n",
    "        d4 = self.down4(self.pool(d3))\n",
    "        d5 = self.down5(self.pool(d4))\n",
    "\n",
    "        # Decoder\n",
    "        u1 = self.up1(d5)\n",
    "        u1 = torch.cat((u1, d4), dim=1)\n",
    "        u1 = self.conv_up1(u1)\n",
    "        u2 = self.up2(u1)\n",
    "        u2 = torch.cat((u2, d3), dim=1)\n",
    "        u2 = self.conv_up2(u2)\n",
    "        u3 = self.up3(u2)\n",
    "        u3 = torch.cat((u3, d2), dim=1)\n",
    "        u3 = self.conv_up3(u3)\n",
    "        u4 = self.up4(u3)\n",
    "        u4 = torch.cat((u4, d1), dim=1)\n",
    "        u4 = self.conv_up4(u4)\n",
    "\n",
    "        return self.final_conv(u4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Definindo o dispositivo (GPU se disponível)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Inicializando a U-Net\n",
    "    in_channels = 3  # Por exemplo, RGB\n",
    "    out_channels = 3  # Saída de imagem RGB\n",
    "    model = UNet(in_channels, out_channels).to(device)\n",
    "\n",
    "    # Exemplo de entrada\n",
    "    x = torch.randn(1, in_channels, 256, 256).to(device)  # Batch size 1, 256x256 imagem\n",
    "    output = model(x)\n",
    "\n",
    "    print(x.shape, output.shape)  # Deve ser (1, out_channels, 256, 256)\n",
    "     # Plotando a arquitetura da rede\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitetura da U-Net plotada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "        y = model(torch.randn(1, in_channels, 256, 256).to(device))\n",
    "        tv.make_dot(y, params=dict(list(model.named_parameters()))).render(\"unet_architecture\", format=\"png\")\n",
    "        \n",
    "        #plot.format = 'png'\n",
    "        \n",
    "        print(\"Arquitetura da U-Net plotada com sucesso.\")\n",
    "except ImportError:\n",
    "        print(\"Para plotar a arquitetura, instale o pacote torchviz.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Layer (type:depth-idx)                   Output Shape     Param #\n",
      "========================================================================\n",
      "DoubleConv: 2-1                          [-1, 64, 256, 256] --\n",
      "MaxPool2d: 1-1                           [-1, 64, 128, 128] --\n",
      "DoubleConv: 2-2                          [-1, 128, 128, 128] --\n",
      "MaxPool2d: 1-2                           [-1, 128, 64, 64] --\n",
      "DoubleConv: 2-3                          [-1, 256, 64, 64] --\n",
      "MaxPool2d: 1-3                           [-1, 256, 32, 32] --\n",
      "DoubleConv: 1-4                          [-1, 512, 32, 32] --\n",
      "Sequential: 2-4                          [-1, 512, 32, 32] --\n",
      "ConvTranspose2d: 2-5                     [-1, 256, 64, 64] 524,544\n",
      "DoubleConv: 2-6                          [-1, 256, 64, 64] --\n",
      "ConvTranspose2d: 2-7                     [-1, 128, 128, 128] 131,200\n",
      "DoubleConv: 2-8                          [-1, 128, 128, 128] --\n",
      "ConvTranspose2d: 2-9                     [-1, 64, 256, 256] 32,832\n",
      "DoubleConv: 2-10                         [-1, 64, 256, 256] --\n",
      "Conv2d: 1-5                              [-1, 3, 256, 256] 195\n",
      "========================================================================\n",
      "Total params: 3,542,211\n",
      "Trainable params: 3,542,211\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 6.47\n",
      "========================================================================\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 57.50\n",
      "Params size (MB): 13.51\n",
      "Estimated Total Size (MB): 71.76\n",
      "========================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================\n",
       "Layer (type:depth-idx)                   Output Shape     Param #\n",
       "========================================================================\n",
       "DoubleConv: 2-1                          [-1, 64, 256, 256] --\n",
       "MaxPool2d: 1-1                           [-1, 64, 128, 128] --\n",
       "DoubleConv: 2-2                          [-1, 128, 128, 128] --\n",
       "MaxPool2d: 1-2                           [-1, 128, 64, 64] --\n",
       "DoubleConv: 2-3                          [-1, 256, 64, 64] --\n",
       "MaxPool2d: 1-3                           [-1, 256, 32, 32] --\n",
       "DoubleConv: 1-4                          [-1, 512, 32, 32] --\n",
       "Sequential: 2-4                          [-1, 512, 32, 32] --\n",
       "ConvTranspose2d: 2-5                     [-1, 256, 64, 64] 524,544\n",
       "DoubleConv: 2-6                          [-1, 256, 64, 64] --\n",
       "ConvTranspose2d: 2-7                     [-1, 128, 128, 128] 131,200\n",
       "DoubleConv: 2-8                          [-1, 128, 128, 128] --\n",
       "ConvTranspose2d: 2-9                     [-1, 64, 256, 256] 32,832\n",
       "DoubleConv: 2-10                         [-1, 64, 256, 256] --\n",
       "Conv2d: 1-5                              [-1, 3, 256, 256] 195\n",
       "========================================================================\n",
       "Total params: 3,542,211\n",
       "Trainable params: 3,542,211\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.47\n",
       "========================================================================\n",
       "Input size (MB): 0.75\n",
       "Forward/backward pass size (MB): 57.50\n",
       "Params size (MB): 13.51\n",
       "Estimated Total Size (MB): 71.76\n",
       "========================================================================"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, x,depth=1, dtypes=[torch.long],branching=False,verbose=1, col_width=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "Arquitetura da U-Net plotada com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloco de duas convoluções 3x3 seguidas de ReLU.\n",
    "    Permite a parametrização do uso de BatchNorm.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_batch_norm=True):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        layers.extend([\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ])\n",
    "        if use_batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        self.double_conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementação da arquitetura U-Net parametrizável para geração de imagens.\n",
    "    Permite a parametrização do número de blocos de convolução e o uso de BatchNorm.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, base_filters=64, num_layers=5, use_batch_norm=True):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.base_filters = base_filters\n",
    "        self.num_layers = num_layers\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        filters = base_filters\n",
    "        \n",
    "        for i in range(num_layers-1):\n",
    "            self.encoder_layers.append(DoubleConv(in_channels, filters, use_batch_norm))\n",
    "            in_channels = filters\n",
    "            filters *= 2\n",
    "        \n",
    "        # Max-pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(filters // 2, filters, use_batch_norm)\n",
    "\n",
    "        # Decoder\n",
    "        self.up_layers = nn.ModuleList()\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        for i in range(num_layers-1, 0, -1):#range(num_layers-1, 0, -1)\n",
    "            filters //= 2\n",
    "            self.up_layers.append(nn.ConvTranspose2d(filters * 2, filters, kernel_size=2, stride=2))\n",
    "            self.up_convs.append(DoubleConv(filters * 2, filters , use_batch_norm))\n",
    "            \n",
    "\n",
    "        # Final convolution\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(base_filters, out_channels, kernel_size=1)\n",
    "    def see_model(self):\n",
    "        print(f\"\"\"\n",
    "        encoder: {self.encoder_layers}\n",
    "        pool: {self.pool}\n",
    "        botleneck: {self.bottleneck}\n",
    "        up laeyers: {self.up_layers}\n",
    "        up_convs: {self.up_convs}\n",
    "        final conv: {self.final_conv}\"\"\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        encoder_results = []\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "            encoder_results.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Decoder\n",
    "        for i, (up, conv) in enumerate(zip(self.up_layers, self.up_convs)):\n",
    "            x = up(x)\n",
    "            x = torch.cat((x, encoder_results[-(i+1)]), dim=1)\n",
    "            x = conv(x)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "def standardize_image(image):\n",
    "    \"\"\"\n",
    "    Função para padronizar as imagens de entrada.\n",
    "    \"\"\"\n",
    "    mean = image.mean()\n",
    "    std = image.std()\n",
    "    return (image - mean) / std\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Definindo o dispositivo (GPU se disponível)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Inicializando a U-Net parametrizável\n",
    "    in_channels = 3  # Por exemplo, RGB\n",
    "    out_channels = 3  # Saída de imagem RGB\n",
    "    base_filters = 64\n",
    "    num_layers = 4\n",
    "    use_batch_norm = True\n",
    "    model = UNet(in_channels, out_channels, base_filters, num_layers, use_batch_norm).to(device)\n",
    "    #model.see_model()\n",
    "\n",
    "    # Exibindo a arquitetura da rede\n",
    "    #summary(model, input_size=(in_channels, 256, 256))\n",
    "\n",
    "    # Exemplo de entrada\n",
    "    x = torch.randn(1, in_channels, 256, 256).to(device)  # Batch size 1, 256x256 imagem\n",
    "    # x = standardize_image(x)\n",
    "    output = model(x)\n",
    "\n",
    "    print(output.shape)  # Deve ser (1, out_channels, 256, 256)\n",
    "\n",
    "    # Plotando a arquitetura da rede\n",
    "    try:\n",
    "\n",
    "        y = model(torch.randn(1, in_channels, 256, 256).to(device))\n",
    "        tv.make_dot(y, params=dict(list(model.named_parameters()))).render(\"unet_architecture_2\", format=\"png\")\n",
    "        print(\"Arquitetura da U-Net plotada com sucesso.\")\n",
    "    except ImportError:\n",
    "        print(\"Para plotar a arquitetura, instale o pacote torchviz.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbranching\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcol_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcol_width\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdepth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtypes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorchsummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_statistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelStatistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Summarize the given PyTorch model. Summarized information includes:\n",
      "    1) Layer names,\n",
      "    2) input/output shapes,\n",
      "    3) kernel shape,\n",
      "    4) # of parameters,\n",
      "    5) # of operations (Mult-Adds)\n",
      "\n",
      "Args:\n",
      "    model (nn.Module):\n",
      "            PyTorch model to summarize. The model should be fully in either train()\n",
      "            or eval() mode. If layers are not all in the same mode, running summary\n",
      "            may have side effects on batchnorm or dropout statistics. If you\n",
      "            encounter an issue with this, please open a GitHub issue.\n",
      "\n",
      "    input_data (Sequence of Sizes or Tensors):\n",
      "            Example input tensor of the model (dtypes inferred from model input).\n",
      "            - OR -\n",
      "            Shape of input data as a List/Tuple/torch.Size\n",
      "            (dtypes must match model input, default is FloatTensors).\n",
      "            You should NOT include batch size in the tuple.\n",
      "            - OR -\n",
      "            If input_data is not provided, no forward pass through the network is\n",
      "            performed, and the provided model information is limited to layer names.\n",
      "            Default: None\n",
      "\n",
      "    batch_dim (int):\n",
      "            Batch_dimension of input data. If batch_dim is None, the input data\n",
      "            is assumed to contain the batch dimension.\n",
      "            WARNING: in a future version, the default will change to None.\n",
      "            Default: 0\n",
      "\n",
      "    branching (bool):\n",
      "            Whether to use the branching layout for the printed output.\n",
      "            Default: True\n",
      "\n",
      "    col_names (Iterable[str]):\n",
      "            Specify which columns to show in the output. Currently supported:\n",
      "            (\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\")\n",
      "            If input_data is not provided, only \"num_params\" is used.\n",
      "            Default: (\"output_size\", \"num_params\")\n",
      "\n",
      "    col_width (int):\n",
      "            Width of each column.\n",
      "            Default: 25\n",
      "\n",
      "    depth (int):\n",
      "            Number of nested layers to traverse (e.g. Sequentials).\n",
      "            Default: 3\n",
      "\n",
      "    device (torch.Device):\n",
      "            Uses this torch device for model and input_data.\n",
      "            If not specified, uses result of torch.cuda.is_available().\n",
      "            Default: None\n",
      "\n",
      "    dtypes (List[torch.dtype]):\n",
      "            For multiple inputs, specify the size of both inputs, and\n",
      "            also specify the types of each parameter here.\n",
      "            Default: None\n",
      "\n",
      "    verbose (int):\n",
      "            0 (quiet): No output\n",
      "            1 (default): Print model summary\n",
      "            2 (verbose): Show weight and bias layers in full detail\n",
      "            Default: 1\n",
      "\n",
      "    *args, **kwargs:\n",
      "            Other arguments used in `model.forward` function.\n",
      "\n",
      "Return:\n",
      "    ModelStatistics object\n",
      "            See torchsummary/model_statistics.py for more information.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/Losses/lib/python3.12/site-packages/torchsummary/torchsummary.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "summary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "Layer (type:depth-idx)                   Output Shape     Param #\n",
      "========================================================================\n",
      "DoubleConv: 2-1                          [-1, 64, 256, 256] --\n",
      "MaxPool2d: 1-1                           [-1, 64, 128, 128] --\n",
      "DoubleConv: 2-2                          [-1, 128, 128, 128] --\n",
      "MaxPool2d: 1-2                           [-1, 128, 64, 64] --\n",
      "DoubleConv: 2-3                          [-1, 256, 64, 64] --\n",
      "MaxPool2d: 1-3                           [-1, 256, 32, 32] --\n",
      "DoubleConv: 1-4                          [-1, 512, 32, 32] --\n",
      "Sequential: 2-4                          [-1, 512, 32, 32] --\n",
      "ConvTranspose2d: 2-5                     [-1, 256, 64, 64] 524,544\n",
      "DoubleConv: 2-6                          [-1, 256, 64, 64] --\n",
      "ConvTranspose2d: 2-7                     [-1, 128, 128, 128] 131,200\n",
      "DoubleConv: 2-8                          [-1, 128, 128, 128] --\n",
      "ConvTranspose2d: 2-9                     [-1, 64, 256, 256] 32,832\n",
      "DoubleConv: 2-10                         [-1, 64, 256, 256] --\n",
      "Conv2d: 1-5                              [-1, 3, 256, 256] 195\n",
      "========================================================================\n",
      "Total params: 3,542,211\n",
      "Trainable params: 3,542,211\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 6.47\n",
      "========================================================================\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 57.50\n",
      "Params size (MB): 13.51\n",
      "Estimated Total Size (MB): 71.76\n",
      "========================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================\n",
       "Layer (type:depth-idx)                   Output Shape     Param #\n",
       "========================================================================\n",
       "DoubleConv: 2-1                          [-1, 64, 256, 256] --\n",
       "MaxPool2d: 1-1                           [-1, 64, 128, 128] --\n",
       "DoubleConv: 2-2                          [-1, 128, 128, 128] --\n",
       "MaxPool2d: 1-2                           [-1, 128, 64, 64] --\n",
       "DoubleConv: 2-3                          [-1, 256, 64, 64] --\n",
       "MaxPool2d: 1-3                           [-1, 256, 32, 32] --\n",
       "DoubleConv: 1-4                          [-1, 512, 32, 32] --\n",
       "Sequential: 2-4                          [-1, 512, 32, 32] --\n",
       "ConvTranspose2d: 2-5                     [-1, 256, 64, 64] 524,544\n",
       "DoubleConv: 2-6                          [-1, 256, 64, 64] --\n",
       "ConvTranspose2d: 2-7                     [-1, 128, 128, 128] 131,200\n",
       "DoubleConv: 2-8                          [-1, 128, 128, 128] --\n",
       "ConvTranspose2d: 2-9                     [-1, 64, 256, 256] 32,832\n",
       "DoubleConv: 2-10                         [-1, 64, 256, 256] --\n",
       "Conv2d: 1-5                              [-1, 3, 256, 256] 195\n",
       "========================================================================\n",
       "Total params: 3,542,211\n",
       "Trainable params: 3,542,211\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.47\n",
       "========================================================================\n",
       "Input size (MB): 0.75\n",
       "Forward/backward pass size (MB): 57.50\n",
       "Params size (MB): 13.51\n",
       "Estimated Total Size (MB): 71.76\n",
       "========================================================================"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, x,depth=1, dtypes=[torch.long],branching=False,verbose=1, col_width=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpa memoria CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória alocada: 1007716352 bytes\n",
      "Memória reservada: 1392508928 bytes\n",
      "Memória alocada: 1007716352 bytes\n",
      "Memória reservada: 1270874112 bytes\n",
      "Memoria limpa\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "# Force a coleta de lixo para liberar a memória imediatamente\n",
    "gc.collect()\n",
    "\n",
    "# Libere o cache de memória do PyTorch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Verifique a memória alocada e reservada\n",
    "print(f'Memória alocada: {torch.cuda.memory_allocated()} bytes')\n",
    "print(f'Memória reservada: {torch.cuda.memory_reserved()} bytes')\n",
    "\n",
    "print(f\"Memoria limpa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CycleGAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gan loss\t \t \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### color loss\t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SSIM\t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS-SSIM\t \t\t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE\t \t\t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE\t \t \t \t \t \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uranker\t \t \t \t \t \n",
    "### MUSIQ\t \t \t \t \t \n",
    "### Content Loss VGG\t \t \t \t \t \n",
    "### Acumlative Superiority\t \t \t \t \t \n",
    "### Discriminative\t \t \t \t \t \n",
    "### L2\t \t \t \t \t \n",
    "### L1\t \t \t \t \t \n",
    "### CIoU\t \t \t \t \t \n",
    "### Adversarial Loss\t \t \t \t \t \n",
    "### WGan Loss\t \t \t \t \t \n",
    "### Gradient Loss\t \t \t \t \t \n",
    "### Color Structure Perceptual\t \t \t \t \t \n",
    "### Detail loss\t \t \t \t \t \n",
    "### CGan\t \t \t \t \t \n",
    "### Angular\t \t \t \t \t \n",
    "### margin-ranking loss\t \t \t \t \t \n",
    "### Classifier loss\t \t \t \t \t \n",
    "### Identity mapping\t \t \t \t \t \n",
    "### Cycle consistency loss\t \t \t \t \t \n",
    "### LAB loss\t \t \t \t \t \n",
    "### LCH loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Losses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
