digraph {
	graph [size="54.6,54.6"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140064874114544 [label="
 (1, 3, 256, 256)" fillcolor=darkolivegreen1]
	140065392449168 [label=ConvolutionBackward0]
	140064887590816 -> 140065392449168
	140064887590816 [label=NativeBatchNormBackward0]
	140064874507392 -> 140064887590816
	140064874507392 [label=ReluBackward0]
	140064874506768 -> 140064874507392
	140064874506768 [label=ConvolutionBackward0]
	140064874506672 -> 140064874506768
	140064874506672 [label=NativeBatchNormBackward0]
	140064874506384 -> 140064874506672
	140064874506384 [label=ReluBackward0]
	140064874506048 -> 140064874506384
	140064874506048 [label=ConvolutionBackward0]
	140064874506000 -> 140064874506048
	140064874506000 [label=CatBackward0]
	140064874505328 -> 140064874506000
	140064874505328 [label=ConvolutionBackward0]
	140064874505376 -> 140064874505328
	140064874505376 [label=NativeBatchNormBackward0]
	140064874504848 -> 140064874505376
	140064874504848 [label=ReluBackward0]
	140064874504704 -> 140064874504848
	140064874504704 [label=ConvolutionBackward0]
	140064874504416 -> 140064874504704
	140064874504416 [label=NativeBatchNormBackward0]
	140064874504080 -> 140064874504416
	140064874504080 [label=ReluBackward0]
	140064874503744 -> 140064874504080
	140064874503744 [label=ConvolutionBackward0]
	140064874503408 -> 140064874503744
	140064874503408 [label=CatBackward0]
	140064874502736 -> 140064874503408
	140064874502736 [label=ConvolutionBackward0]
	140064874502784 -> 140064874502736
	140064874502784 [label=NativeBatchNormBackward0]
	140064874502256 -> 140064874502784
	140064874502256 [label=ReluBackward0]
	140064874502112 -> 140064874502256
	140064874502112 [label=ConvolutionBackward0]
	140064874501824 -> 140064874502112
	140064874501824 [label=NativeBatchNormBackward0]
	140064874501488 -> 140064874501824
	140064874501488 [label=ReluBackward0]
	140064874501152 -> 140064874501488
	140064874501152 [label=ConvolutionBackward0]
	140064874500768 -> 140064874501152
	140064874500768 [label=CatBackward0]
	140064874500912 -> 140064874500768
	140064874500912 [label=ConvolutionBackward0]
	140064874507728 -> 140064874500912
	140064874507728 [label=NativeBatchNormBackward0]
	140064874507776 -> 140064874507728
	140064874507776 [label=ReluBackward0]
	140064874500336 -> 140064874507776
	140064874500336 [label=ConvolutionBackward0]
	140064874500240 -> 140064874500336
	140064874500240 [label=NativeBatchNormBackward0]
	140064874499424 -> 140064874500240
	140064874499424 [label=ReluBackward0]
	140064874499568 -> 140064874499424
	140064874499568 [label=ConvolutionBackward0]
	140064874499136 -> 140064874499568
	140064874499136 [label=MaxPool2DWithIndicesBackward0]
	140064874500816 -> 140064874499136
	140064874500816 [label=NativeBatchNormBackward0]
	140064874498800 -> 140064874500816
	140064874498800 [label=ReluBackward0]
	140064874498272 -> 140064874498800
	140064874498272 [label=ConvolutionBackward0]
	140064874498176 -> 140064874498272
	140064874498176 [label=NativeBatchNormBackward0]
	140064874497888 -> 140064874498176
	140064874497888 [label=ReluBackward0]
	140064874497552 -> 140064874497888
	140064874497552 [label=ConvolutionBackward0]
	140064874497504 -> 140064874497552
	140064874497504 [label=MaxPool2DWithIndicesBackward0]
	140064874502928 -> 140064874497504
	140064874502928 [label=NativeBatchNormBackward0]
	140064874496880 -> 140064874502928
	140064874496880 [label=ReluBackward0]
	140064874496496 -> 140064874496880
	140064874496496 [label=ConvolutionBackward0]
	140064874496112 -> 140064874496496
	140064874496112 [label=NativeBatchNormBackward0]
	140064874496256 -> 140064874496112
	140064874496256 [label=ReluBackward0]
	140064881981424 -> 140064874496256
	140064881981424 [label=ConvolutionBackward0]
	140064877786352 -> 140064881981424
	140064877786352 [label=MaxPool2DWithIndicesBackward0]
	140064874505520 -> 140064877786352
	140064874505520 [label=NativeBatchNormBackward0]
	140064874212720 -> 140064874505520
	140064874212720 [label=ReluBackward0]
	140064874210032 -> 140064874212720
	140064874210032 [label=ConvolutionBackward0]
	140064874211184 -> 140064874210032
	140064874211184 [label=NativeBatchNormBackward0]
	140064874210512 -> 140064874211184
	140064874210512 [label=ReluBackward0]
	140064874217184 -> 140064874210512
	140064874217184 [label=ConvolutionBackward0]
	140064874217376 -> 140064874217184
	140064884086400 [label="encoder_layers.0.double_conv.0.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	140064884086400 -> 140064874217376
	140064874217376 [label=AccumulateGrad]
	140064874217328 -> 140064874217184
	140064881766592 [label="encoder_layers.0.double_conv.0.bias
 (64)" fillcolor=lightblue]
	140064881766592 -> 140064874217328
	140064874217328 [label=AccumulateGrad]
	140064874211376 -> 140064874211184
	140064874119104 [label="encoder_layers.0.double_conv.2.weight
 (64)" fillcolor=lightblue]
	140064874119104 -> 140064874211376
	140064874211376 [label=AccumulateGrad]
	140064874212096 -> 140064874211184
	140064875983200 [label="encoder_layers.0.double_conv.2.bias
 (64)" fillcolor=lightblue]
	140064875983200 -> 140064874212096
	140064874212096 [label=AccumulateGrad]
	140064874212048 -> 140064874210032
	140064874486816 [label="encoder_layers.0.double_conv.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140064874486816 -> 140064874212048
	140064874212048 [label=AccumulateGrad]
	140064874209840 -> 140064874210032
	140064874486256 [label="encoder_layers.0.double_conv.3.bias
 (64)" fillcolor=lightblue]
	140064874486256 -> 140064874209840
	140064874209840 [label=AccumulateGrad]
	140064874212672 -> 140064874505520
	140064874486336 [label="encoder_layers.0.double_conv.5.weight
 (64)" fillcolor=lightblue]
	140064874486336 -> 140064874212672
	140064874212672 [label=AccumulateGrad]
	140064874211856 -> 140064874505520
	140064874486496 [label="encoder_layers.0.double_conv.5.bias
 (64)" fillcolor=lightblue]
	140064874486496 -> 140064874211856
	140064874211856 [label=AccumulateGrad]
	140064878162848 -> 140064881981424
	140064874485456 [label="encoder_layers.1.double_conv.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140064874485456 -> 140064878162848
	140064878162848 [label=AccumulateGrad]
	140064878159152 -> 140064881981424
	140064874485536 [label="encoder_layers.1.double_conv.0.bias
 (128)" fillcolor=lightblue]
	140064874485536 -> 140064878159152
	140064878159152 [label=AccumulateGrad]
	140064874496160 -> 140064874496112
	140064874485696 [label="encoder_layers.1.double_conv.2.weight
 (128)" fillcolor=lightblue]
	140064874485696 -> 140064874496160
	140064874496160 [label=AccumulateGrad]
	140064874496352 -> 140064874496112
	140064874485776 [label="encoder_layers.1.double_conv.2.bias
 (128)" fillcolor=lightblue]
	140064874485776 -> 140064874496352
	140064874496352 [label=AccumulateGrad]
	140064874496304 -> 140064874496496
	140064874485376 [label="encoder_layers.1.double_conv.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140064874485376 -> 140064874496304
	140064874496304 [label=AccumulateGrad]
	140064874496832 -> 140064874496496
	140064874485216 [label="encoder_layers.1.double_conv.3.bias
 (128)" fillcolor=lightblue]
	140064874485216 -> 140064874496832
	140064874496832 [label=AccumulateGrad]
	140064874497216 -> 140064874502928
	140064874484656 [label="encoder_layers.1.double_conv.5.weight
 (128)" fillcolor=lightblue]
	140064874484656 -> 140064874497216
	140064874497216 [label=AccumulateGrad]
	140064874496976 -> 140064874502928
	140064874484736 [label="encoder_layers.1.double_conv.5.bias
 (128)" fillcolor=lightblue]
	140064874484736 -> 140064874496976
	140064874496976 [label=AccumulateGrad]
	140064874497696 -> 140064874497552
	140064874484496 [label="encoder_layers.2.double_conv.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140064874484496 -> 140064874497696
	140064874497696 [label=AccumulateGrad]
	140064874497648 -> 140064874497552
	140064874480816 [label="encoder_layers.2.double_conv.0.bias
 (256)" fillcolor=lightblue]
	140064874480816 -> 140064874497648
	140064874497648 [label=AccumulateGrad]
	140064874498224 -> 140064874498176
	140064874486656 [label="encoder_layers.2.double_conv.2.weight
 (256)" fillcolor=lightblue]
	140064874486656 -> 140064874498224
	140064874498224 [label=AccumulateGrad]
	140064874498128 -> 140064874498176
	140064874481056 [label="encoder_layers.2.double_conv.2.bias
 (256)" fillcolor=lightblue]
	140064874481056 -> 140064874498128
	140064874498128 [label=AccumulateGrad]
	140064874498080 -> 140064874498272
	140064874480576 [label="encoder_layers.2.double_conv.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140064874480576 -> 140064874498080
	140064874498080 [label=AccumulateGrad]
	140064874498512 -> 140064874498272
	140064874480016 [label="encoder_layers.2.double_conv.3.bias
 (256)" fillcolor=lightblue]
	140064874480016 -> 140064874498512
	140064874498512 [label=AccumulateGrad]
	140064874498752 -> 140064874500816
	140064874480096 [label="encoder_layers.2.double_conv.5.weight
 (256)" fillcolor=lightblue]
	140064874480096 -> 140064874498752
	140064874498752 [label=AccumulateGrad]
	140064874498848 -> 140064874500816
	140064874480256 [label="encoder_layers.2.double_conv.5.bias
 (256)" fillcolor=lightblue]
	140064874480256 -> 140064874498848
	140064874498848 [label=AccumulateGrad]
	140064874499184 -> 140064874499568
	140064874479776 [label="bottleneck.double_conv.0.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	140064874479776 -> 140064874499184
	140064874499184 [label=AccumulateGrad]
	140064874499664 -> 140064874499568
	140068682265008 [label="bottleneck.double_conv.0.bias
 (512)" fillcolor=lightblue]
	140068682265008 -> 140064874499664
	140064874499664 [label=AccumulateGrad]
	140064874499616 -> 140064874500240
	140065127852992 [label="bottleneck.double_conv.2.weight
 (512)" fillcolor=lightblue]
	140065127852992 -> 140064874499616
	140064874499616 [label=AccumulateGrad]
	140064874499808 -> 140064874500240
	140065127855312 [label="bottleneck.double_conv.2.bias
 (512)" fillcolor=lightblue]
	140065127855312 -> 140064874499808
	140064874499808 [label=AccumulateGrad]
	140064874500144 -> 140064874500336
	140068153370624 [label="bottleneck.double_conv.3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140068153370624 -> 140064874500144
	140064874500144 [label=AccumulateGrad]
	140064874500096 -> 140064874500336
	140068153362864 [label="bottleneck.double_conv.3.bias
 (512)" fillcolor=lightblue]
	140068153362864 -> 140064874500096
	140064874500096 [label=AccumulateGrad]
	140064874506864 -> 140064874507728
	140068153372864 [label="bottleneck.double_conv.5.weight
 (512)" fillcolor=lightblue]
	140068153372864 -> 140064874506864
	140064874506864 [label=AccumulateGrad]
	140064874500288 -> 140064874507728
	140068153373904 [label="bottleneck.double_conv.5.bias
 (512)" fillcolor=lightblue]
	140068153373904 -> 140064874500288
	140064874500288 [label=AccumulateGrad]
	140064874500480 -> 140064874500912
	140064890407024 [label="up_layers.0.weight
 (512, 256, 2, 2)" fillcolor=lightblue]
	140064890407024 -> 140064874500480
	140064874500480 [label=AccumulateGrad]
	140064874500528 -> 140064874500912
	140064890406544 [label="up_layers.0.bias
 (256)" fillcolor=lightblue]
	140064890406544 -> 140064874500528
	140064874500528 [label=AccumulateGrad]
	140064874500816 -> 140064874500768
	140064874500960 -> 140064874501152
	140068703720768 [label="up_convs.0.double_conv.0.weight
 (256, 512, 3, 3)" fillcolor=lightblue]
	140068703720768 -> 140064874500960
	140064874500960 [label=AccumulateGrad]
	140064874501440 -> 140064874501152
	140068703721088 [label="up_convs.0.double_conv.0.bias
 (256)" fillcolor=lightblue]
	140068703721088 -> 140064874501440
	140064874501440 [label=AccumulateGrad]
	140064874501392 -> 140064874501824
	140064874167936 [label="up_convs.0.double_conv.2.weight
 (256)" fillcolor=lightblue]
	140064874167936 -> 140064874501392
	140064874501392 [label=AccumulateGrad]
	140064874501584 -> 140064874501824
	140064874168016 [label="up_convs.0.double_conv.2.bias
 (256)" fillcolor=lightblue]
	140064874168016 -> 140064874501584
	140064874501584 [label=AccumulateGrad]
	140064874502208 -> 140064874502112
	140064874167776 [label="up_convs.0.double_conv.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140064874167776 -> 140064874502208
	140064874502208 [label=AccumulateGrad]
	140064874502160 -> 140064874502112
	140064874167856 [label="up_convs.0.double_conv.3.bias
 (256)" fillcolor=lightblue]
	140064874167856 -> 140064874502160
	140064874502160 [label=AccumulateGrad]
	140064874502448 -> 140064874502784
	140064874167696 [label="up_convs.0.double_conv.5.weight
 (256)" fillcolor=lightblue]
	140064874167696 -> 140064874502448
	140064874502448 [label=AccumulateGrad]
	140064874502496 -> 140064874502784
	140064874167136 [label="up_convs.0.double_conv.5.bias
 (256)" fillcolor=lightblue]
	140064874167136 -> 140064874502496
	140064874502496 [label=AccumulateGrad]
	140064874502976 -> 140064874502736
	140064874166816 [label="up_layers.1.weight
 (256, 128, 2, 2)" fillcolor=lightblue]
	140064874166816 -> 140064874502976
	140064874502976 [label=AccumulateGrad]
	140064874502832 -> 140064874502736
	140064874166976 [label="up_layers.1.bias
 (128)" fillcolor=lightblue]
	140064874166976 -> 140064874502832
	140064874502832 [label=AccumulateGrad]
	140064874502928 -> 140064874503408
	140064874503456 -> 140064874503744
	140064874166896 [label="up_convs.1.double_conv.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	140064874166896 -> 140064874503456
	140064874503456 [label=AccumulateGrad]
	140064874504032 -> 140064874503744
	140064874166496 [label="up_convs.1.double_conv.0.bias
 (128)" fillcolor=lightblue]
	140064874166496 -> 140064874504032
	140064874504032 [label=AccumulateGrad]
	140064874503984 -> 140064874504416
	140064874166576 [label="up_convs.1.double_conv.2.weight
 (128)" fillcolor=lightblue]
	140064874166576 -> 140064874503984
	140064874503984 [label=AccumulateGrad]
	140064874504176 -> 140064874504416
	140064874166656 [label="up_convs.1.double_conv.2.bias
 (128)" fillcolor=lightblue]
	140064874166656 -> 140064874504176
	140064874504176 [label=AccumulateGrad]
	140064874504800 -> 140064874504704
	140064874166176 [label="up_convs.1.double_conv.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140064874166176 -> 140064874504800
	140064874504800 [label=AccumulateGrad]
	140064874504752 -> 140064874504704
	140064874165616 [label="up_convs.1.double_conv.3.bias
 (128)" fillcolor=lightblue]
	140064874165616 -> 140064874504752
	140064874504752 [label=AccumulateGrad]
	140064874505040 -> 140064874505376
	140064874165696 [label="up_convs.1.double_conv.5.weight
 (128)" fillcolor=lightblue]
	140064874165696 -> 140064874505040
	140064874505040 [label=AccumulateGrad]
	140064874505088 -> 140064874505376
	140064874165856 [label="up_convs.1.double_conv.5.bias
 (128)" fillcolor=lightblue]
	140064874165856 -> 140064874505088
	140064874505088 [label=AccumulateGrad]
	140064874505568 -> 140064874505328
	140064874165536 [label="up_layers.2.weight
 (128, 64, 2, 2)" fillcolor=lightblue]
	140064874165536 -> 140064874505568
	140064874505568 [label=AccumulateGrad]
	140064874505424 -> 140064874505328
	140064874165376 [label="up_layers.2.bias
 (64)" fillcolor=lightblue]
	140064874165376 -> 140064874505424
	140064874505424 [label=AccumulateGrad]
	140064874505520 -> 140064874506000
	140064874506192 -> 140064874506048
	140064874165056 [label="up_convs.2.double_conv.0.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	140064874165056 -> 140064874506192
	140064874506192 [label=AccumulateGrad]
	140064874506144 -> 140064874506048
	140064874165136 [label="up_convs.2.double_conv.0.bias
 (64)" fillcolor=lightblue]
	140064874165136 -> 140064874506144
	140064874506144 [label=AccumulateGrad]
	140064874506720 -> 140064874506672
	140064874164976 [label="up_convs.2.double_conv.2.weight
 (64)" fillcolor=lightblue]
	140064874164976 -> 140064874506720
	140064874506720 [label=AccumulateGrad]
	140064874506624 -> 140064874506672
	140064874164496 [label="up_convs.2.double_conv.2.bias
 (64)" fillcolor=lightblue]
	140064874164496 -> 140064874506624
	140064874506624 [label=AccumulateGrad]
	140064874506576 -> 140064874506768
	140064874164096 [label="up_convs.2.double_conv.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140064874164096 -> 140064874506576
	140064874506576 [label=AccumulateGrad]
	140064874507008 -> 140064874506768
	140064874164176 [label="up_convs.2.double_conv.3.bias
 (64)" fillcolor=lightblue]
	140064874164176 -> 140064874507008
	140064874507008 [label=AccumulateGrad]
	140064874507632 -> 140064887590816
	140064874164336 [label="up_convs.2.double_conv.5.weight
 (64)" fillcolor=lightblue]
	140064874164336 -> 140064874507632
	140064874507632 [label=AccumulateGrad]
	140064874507248 -> 140064887590816
	140064874164416 [label="up_convs.2.double_conv.5.bias
 (64)" fillcolor=lightblue]
	140064874164416 -> 140064874507248
	140064874507248 [label=AccumulateGrad]
	140064883206048 -> 140065392449168
	140064874163856 [label="final_conv.weight
 (3, 64, 1, 1)" fillcolor=lightblue]
	140064874163856 -> 140064883206048
	140064883206048 [label=AccumulateGrad]
	140064874507344 -> 140065392449168
	140064874163456 [label="final_conv.bias
 (3)" fillcolor=lightblue]
	140064874163456 -> 140064874507344
	140064874507344 [label=AccumulateGrad]
	140065392449168 -> 140064874114544
}
